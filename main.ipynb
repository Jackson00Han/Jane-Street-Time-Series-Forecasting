{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6e401b",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c935a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 环境与依赖\n",
    "\n",
    "# 基础包\n",
    "import tempfile\n",
    "\n",
    "import os, gc, glob, json, yaml, time\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "import lightgbm as lgb\n",
    "from dataclasses import dataclass\n",
    "import pyarrow.parquet as pq\n",
    "from typing import Sequence, Optional, Union, List, Tuple, Iterable, Mapping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Azure & 文件系统\n",
    "import fsspec\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # 默认会加载当前目录下的 .env 文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f494621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接云空间\n",
    "\n",
    "ACC = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "KEY = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "if not ACC or not KEY:\n",
    "    raise RuntimeError(\"Azure credentials not found. Please set them in .env\")\n",
    "storage_options = {\"account_name\": ACC, \"account_key\": KEY}\n",
    "fs = fsspec.filesystem(\"az\", **storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcd4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义路径辅助函数\n",
    "\n",
    "import os, yaml, fsspec\n",
    "from pathlib import Path\n",
    "\n",
    "# 读取配置（唯一来源）\n",
    "cfg = yaml.safe_load(open(\"exp/v1/config/data.yaml\"))\n",
    "\n",
    "# 路径辅助函数\n",
    "def P(kind: str, *parts) -> str:\n",
    "    \"\"\"\n",
    "    kind: 'az' | 'np' | 'local'\n",
    "      az    -> 'az://<container>/<prefix>/...'\n",
    "      np    -> '<container>/<prefix>/...'\n",
    "      local -> '<local_root>/...'\n",
    "    parts:  额外路径片段（可传 'a/b/c' 或 ('a','b','c')）\n",
    "    \"\"\"\n",
    "    container  = str(cfg[\"blob\"][\"container\"]).strip(\"/\")\n",
    "    prefix     = str(cfg[\"blob\"][\"prefix\"]).strip(\"/\")\n",
    "    local_root = Path(cfg[\"local\"][\"root\"])\n",
    "\n",
    "    segs = [str(p).strip(\"/\") for p in parts if p]\n",
    "    tail = \"/\".join(segs) if segs else \"\"\n",
    "\n",
    "    if kind == \"az\":\n",
    "        base = \"az://\" + f\"{container}/{prefix}\" \n",
    "        return f\"{base}/{tail}\" if tail else base\n",
    "    if kind == \"np\":\n",
    "        base = f\"{container}/{prefix}\"\n",
    "        return f\"{base}/{tail}\" if tail else base\n",
    "    if kind == \"local\":\n",
    "        return str(Path(local_root, tail))\n",
    "    raise ValueError(\"kind must be 'az', 'np', or 'local'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca767b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_ALL = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "\n",
    "lb = pl.scan_parquet(P(\"az\", cfg['paths']['raw']) + \"/train.parquet/partition_id=*[4-8]/*.parquet\", storage_options=storage_options)\n",
    "lt = pl.scan_parquet(P(\"az\", cfg['paths']['raw']) + \"/train.parquet/partition_id=*[8-9]/*.parquet\", storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10ad82",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA：每个交易日的symbol覆盖情况,是否覆盖全程\n",
    "lf_s_d = lb.select(['date_id', 'symbol_id'])\n",
    "\n",
    "per_date = (\n",
    "    lf_s_d.group_by(\"date_id\")\n",
    "      .agg(pl.col(\"symbol_id\").n_unique().alias(\"n_symbols\"))\n",
    "      .sort(\"date_id\")\n",
    ")\n",
    "\n",
    "max_n = per_date.select(pl.max(\"n_symbols\")).collect().item()\n",
    "summary = per_date.with_columns([\n",
    "    pl.lit(max_n).alias(\"max_n\"),\n",
    "    (pl.col(\"n_symbols\") == max_n).alias(\"is_full_universe\")\n",
    "])\n",
    "\n",
    "dates_missing = summary.filter(pl.col(\"is_full_universe\") == False).select(\"date_id\")\n",
    "# summary.collect(); dates_missing.collect()\n",
    "\n",
    "dates_missing.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先选一个横向，纵向都比较小的样本,按照data_id来选一小块快速试验\n",
    "\n",
    "ls = lb.filter(pl.col('symbol_id').is_in([1,2,3,4,5]) & pl.col('date_id').is_in([1400,1420]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120e6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a362c607",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dc1b49",
   "metadata": {},
   "source": [
    "Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_sigma_clip(\n",
    "    lf: pl.LazyFrame,\n",
    "    features: Sequence[str],\n",
    "    window: int = 50, # 可以一列一窗口\n",
    "    k: float = 3.0,\n",
    "    ddof: int = 0,\n",
    "    min_valid: int = 10,\n",
    "    cast_float32: bool = True,\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    对指定 features 做按 symbol_id 的滚动 kσ 裁剪。\n",
    "    假设 lf 已经按 ['symbol_id','date_id','time_id'] 排好序。\n",
    "    \"\"\"\n",
    "    lf = lf.select(pl.col(features + ['symbol_id', 'date_id', 'time_id']))\n",
    "    over_cols = [\"symbol_id\"]\n",
    "    # 确保标准差有定义：ddof=0 时至少 1 个样本，ddof=1 时至少 2 个样本\n",
    "    min_need = max(min_valid, ddof + 1)\n",
    "\n",
    "    exprs = []\n",
    "    for c in features:\n",
    "        x   = pl.col(c)\n",
    "        cnt = x.is_not_null().cast(pl.Int32).rolling_sum(window_size=window, min_samples=1).over(over_cols)\n",
    "        mu  = x.rolling_mean(window_size=window, min_samples=1).over(over_cols)\n",
    "        sd  = x.rolling_std(window_size=window, ddof=ddof, min_samples=1).over(over_cols)\n",
    "\n",
    "        lo, hi = mu - k * sd, mu + k * sd\n",
    "        if cast_float32:\n",
    "            x  = x.cast(pl.Float32)\n",
    "            lo = lo.cast(pl.Float32)\n",
    "            hi = hi.cast(pl.Float32)\n",
    "\n",
    "        exprs.append(\n",
    "            pl.when(cnt >= min_need).then(x.clip(lo, hi)).otherwise(x).alias(c)\n",
    "        )\n",
    "\n",
    "    return lf.with_columns(exprs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip features - train\n",
    "\n",
    "\n",
    "lf_clip = rolling_sigma_clip(\n",
    "    lf=lb,\n",
    "    features=FEATURE_ALL\n",
    ")\n",
    "lf_clip = lf_clip.sort(cfg['keys'])\n",
    "\n",
    "lf_clip.collect(streaming=True).write_parquet(f\"{P('local', cfg['paths']['cache'])}/clip.parquet\", compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d510d",
   "metadata": {},
   "source": [
    "Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e616658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_impute(\n",
    "    lf: pl.LazyFrame,\n",
    "    features: Sequence[str],\n",
    "    open_ticks: Tuple[int, int] = (0, 10),\n",
    "    carry_days: int = 5,\n",
    "    intraday_limit: int = 100,\n",
    "    same_tick_days: int = 5,\n",
    "    ensure_sorted: bool = False,\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"严格因果的极简填补：\n",
    "       1) 开盘段：按 symbol 跨日 ffill（TTL）\n",
    "       2) 全日：按 (symbol,date) 日内 ffill（可限步数）\n",
    "       3) 可选：同一 time_id 跨日 ffill（TTL）\n",
    "       不做任何均值/中位数/全局兜底。\n",
    "    \"\"\"\n",
    "    if ensure_sorted:\n",
    "        lf = lf.sort([\"symbol_id\", \"date_id\", \"time_id\"])\n",
    "\n",
    "    t0, t1 = open_ticks\n",
    "    is_open = pl.col(\"time_id\").is_between(t0, t1)\n",
    "\n",
    "    # --- 1) 开盘：跨日承接（带 TTL） ---\n",
    "    open_exprs = []\n",
    "    for c in features:\n",
    "        # “上一次非空值所在的 date_id”（按 symbol）\n",
    "        last_date = (\n",
    "            pl.when(pl.col(c).is_not_null()).then(pl.col(\"date_id\")).otherwise(None)\n",
    "            .forward_fill().over(\"symbol_id\")\n",
    "        )\n",
    "        cand = pl.col(c).forward_fill().over(\"symbol_id\")\n",
    "        gap  = (pl.col(\"date_id\") - last_date).cast(pl.Int32)\n",
    "        open_exprs.append(\n",
    "            pl.when(is_open & pl.col(c).is_null()\n",
    "                    & cand.is_not_null()\n",
    "                    & last_date.is_not_null()\n",
    "                    & (gap <= carry_days)\n",
    "            ).then(cand).otherwise(pl.col(c)).alias(c)\n",
    "        )\n",
    "    lf1 = lf.with_columns(open_exprs)\n",
    "\n",
    "    # --- 2) 日内 ffill（按 (symbol,date)）---\n",
    "    if intraday_limit is None:\n",
    "        lf2 = lf1.with_columns([\n",
    "            pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"]).alias(c)\n",
    "            for c in features\n",
    "        ])\n",
    "    else:\n",
    "        intra_exprs = []\n",
    "        for c in features:\n",
    "            cand_intra = pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            last_t = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"time_id\")).otherwise(None)\n",
    "                .forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            )\n",
    "            step_gap = (pl.col(\"time_id\") - last_t).cast(pl.Int32)\n",
    "            intra_exprs.append(\n",
    "                pl.when(pl.col(c).is_null()\n",
    "                        & cand_intra.is_not_null()\n",
    "                        & last_t.is_not_null()\n",
    "                        & (step_gap <= intraday_limit)\n",
    "                ).then(cand_intra).otherwise(pl.col(c)).alias(c)\n",
    "            )\n",
    "        lf2 = lf1.with_columns(intra_exprs)\n",
    "\n",
    "    # --- 3) 同一 time_id 跨日承接（可选，带 TTL）---\n",
    "    lf3 = lf2\n",
    "    if same_tick_days is not None:\n",
    "        tick_exprs = []\n",
    "        for c in features:\n",
    "            last_date_same = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"date_id\")).otherwise(None)\n",
    "                .forward_fill().over([\"symbol_id\",\"time_id\"])\n",
    "            )\n",
    "            cand_same = pl.col(c).forward_fill().over([\"symbol_id\",\"time_id\"])\n",
    "            gap2 = (pl.col(\"date_id\") - last_date_same).cast(pl.Int32)\n",
    "            tick_exprs.append(\n",
    "                pl.when(pl.col(c).is_null()\n",
    "                        & cand_same.is_not_null()\n",
    "                        & last_date_same.is_not_null()\n",
    "                        & (gap2 <= same_tick_days)\n",
    "                ).then(cand_same).otherwise(pl.col(c)).alias(c)\n",
    "            )\n",
    "        lf3 = lf2.with_columns(tick_exprs)\n",
    "\n",
    "    # --- 4) 再做一次日内 ffill（把 step 3 新填出的值向后传播） ---\n",
    "    if intraday_limit is None:\n",
    "        lf4 = lf3.with_columns([\n",
    "            pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"]).alias(c)\n",
    "            for c in features\n",
    "        ])\n",
    "    else:\n",
    "        post_exprs = []\n",
    "        for c in features:\n",
    "            cand = pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            last_t = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"time_id\")).otherwise(None)\n",
    "                .forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            )\n",
    "            gap = (pl.col(\"time_id\") - last_t).cast(pl.Int32)\n",
    "            post_exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & cand.is_not_null()\n",
    "                        & last_t.is_not_null() & (gap <= intraday_limit)\n",
    "                ).then(cand).otherwise(pl.col(c)).alias(c)\n",
    "            )\n",
    "        lf4 = lf3.with_columns(post_exprs)\n",
    "\n",
    "    return lf4.select([\"symbol_id\",\"date_id\",\"time_id\", *features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41829327",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_clip = pl.scan_parquet(f\"{P('local', cfg['paths']['cache'])}/clip.parquet\")\n",
    "lf_imp = causal_impute(\n",
    "    lf=lf_clip,\n",
    "    features=FEATURE_ALL,\n",
    "    ensure_sorted=True\n",
    ")\n",
    "\n",
    "lf_imp.collect(streaming=True).write_parquet(f\"{P('local', cfg['paths']['cache'])}/imp.parquet\", compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a628d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 右边\n",
    "rhs = lb.select(['symbol_id','date_id','time_id','weight', *RESP_COLS]).unique(subset=['symbol_id','date_id','time_id'])\n",
    "\n",
    "# 左边\n",
    "lf_imp = pl.scan_parquet(f\"{P('local', cfg['paths']['cache'])}/imp.parquet\")\n",
    "\n",
    "dmin, dmax = (\n",
    "    lf_imp.select(\n",
    "        pl.col('date_id').min().alias('dmin'),\n",
    "        pl.col('date_id').max().alias('dmax'),\n",
    "    )\n",
    "    .collect()\n",
    "    .row(0)\n",
    ")\n",
    "path = P('az', 'exp/v1', cfg['paths']['clean'])\n",
    "fs.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"Processing date range: {dmin} to {dmax}\")\n",
    "\n",
    "for lo in range(dmin, dmax + 1, BUCKET):\n",
    "    hi = min(lo + BUCKET - 1, dmax)\n",
    "    left  = lf_imp.filter(pl.col('date_id').is_between(lo, hi))\n",
    "    right = rhs.filter(pl.col('date_id').is_between(lo, hi))\n",
    "    part  = left.join(right, on=['symbol_id','date_id','time_id'], how='left')\n",
    "    (\n",
    "        part.collect(streaming=True)\n",
    "            .write_parquet(f\"{path}/imp_{lo}_{hi}.parquet\",\n",
    "                           compression='zstd',\n",
    "                           storage_options=storage_options)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文件排序\n",
    "\n",
    "# 先提取文件列表\n",
    "paths = fs.glob(f\"{P('az', 'exp/v1', cfg['paths']['clean'])}/*.parquet\")\n",
    "\n",
    "# 按文件名中的起点时间排序\n",
    "\n",
    "start_points = []\n",
    "\n",
    "for p in paths:\n",
    "    bn = os.path.basename(p)\n",
    "    # \"_\"分割\n",
    "    parts = bn.split(\"_\")\n",
    "    start = int(parts[1])\n",
    "    start_points.append((start, p))\n",
    "    \n",
    "start_points.sort()  # 按起点时间排序\n",
    "\n",
    "sorted_paths = [p for _, p in start_points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9eb33a",
   "metadata": {},
   "source": [
    "\n",
    "# 特征工程函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85363ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "\n",
    "# -----------------------------\n",
    "# A. prev-day tails + daily summaries\n",
    "# -----------------------------\n",
    "import polars as pl\n",
    "from typing import Sequence, Optional, Tuple\n",
    "\n",
    "def _resolve_prev_for_daily(\n",
    "    daily: pl.LazyFrame,\n",
    "    *,\n",
    "    keys: Tuple[str,str] = (\"symbol_id\",\"date_id\"),\n",
    "    cols: Sequence[str],\n",
    "    prev_soft_days: Optional[int],\n",
    "    cast_f32: bool = True,\n",
    ") -> pl.LazyFrame:\n",
    "    g_symbol, g_date = keys\n",
    "    exprs = []\n",
    "    for c in cols:\n",
    "        prev_row_val = pl.col(c).shift(1).over(g_symbol)\n",
    "        prev_row_day = pl.col(g_date).shift(1).over(g_symbol)\n",
    "\n",
    "        # strict d-1\n",
    "        prev_strict = pl.when(pl.col(g_date) == (prev_row_day + 1)).then(prev_row_val).otherwise(None)\n",
    "\n",
    "        if prev_soft_days is None:\n",
    "            resolved = prev_strict\n",
    "        else:\n",
    "            # last non-null day/value before d, computed per symbol\n",
    "            last_non_null_day = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(g_date))\n",
    "                  .otherwise(None)\n",
    "                  .forward_fill().over(g_symbol)\n",
    "                  .shift(1)\n",
    "            )\n",
    "            last_non_null_val = (\n",
    "                pl.col(c).forward_fill().over(g_symbol).shift(1)\n",
    "            )\n",
    "            gap = (pl.col(g_date) - last_non_null_day).cast(pl.Int32)\n",
    "\n",
    "            resolved = pl.coalesce([\n",
    "                prev_strict,\n",
    "                pl.when((gap <= int(prev_soft_days)) & last_non_null_val.is_not_null())\n",
    "                  .then(last_non_null_val),\n",
    "            ])\n",
    "\n",
    "        exprs.append((resolved.cast(pl.Float32) if cast_f32 else resolved).alias(c))\n",
    "\n",
    "    return daily.with_columns(exprs)\n",
    "\n",
    "\n",
    "def fe_prevday_tail_and_summaries(\n",
    "    lf: pl.LazyFrame,\n",
    "    *,\n",
    "    rep_cols: Sequence[str],                           # responders，用于“昨日尾部”和“日级摘要”\n",
    "    tail_lags: Sequence[int] = (1,),                   # 倒数第 L（会自动补 diffs 所需的 L+1）\n",
    "    tail_diffs: Sequence[int] = (1,),                 # dK = lag1 - lag(K+1)\n",
    "    rolling_windows: Sequence[int] | None = (3,5,20), # 对 prevday_close 做滚动\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    assume_sorted: bool = True,\n",
    "    cast_f32: bool = True,\n",
    "    prev_soft_days: Optional[int] = None,             # 严格 d-1 + ≤K 天回退（None=只严格 d-1）\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"一次日频聚合得到昨日尾部与日级摘要 → 统一上一日解析 → 回拼到 tick 级。\"\"\"\n",
    "    g_symbol, g_date, g_time = keys\n",
    "    if not assume_sorted:\n",
    "        lf = lf.sort(list(keys))\n",
    "\n",
    "    # --- 一次性日频聚合 ---\n",
    "    need_L = sorted(set(tail_lags) | {k+1 for k in tail_diffs} | {1})\n",
    "    agg_exprs = []\n",
    "    for r in rep_cols:\n",
    "        # 尾部（倒数第 L）\n",
    "        for L in need_L:\n",
    "            agg_exprs.append(\n",
    "                pl.col(r).sort_by(pl.col(g_time)).slice(-L, 1).first().alias(f\"{r}_prev_tail_lag{L}\")\n",
    "            )\n",
    "        # 当日统计\n",
    "        agg_exprs += [\n",
    "            pl.col(r).sort_by(pl.col(g_time)).last().alias(f\"{r}_prevday_close\"),\n",
    "            pl.col(r).mean().alias(f\"{r}_prevday_mean\"),\n",
    "            pl.col(r).std(ddof=0).alias(f\"{r}_prevday_std\"),\n",
    "        ]\n",
    "\n",
    "    daily = (lf.group_by([g_symbol, g_date]).agg(agg_exprs).sort([g_symbol, g_date]))\n",
    "\n",
    "    # 派生 dK\n",
    "    daily = daily.with_columns([\n",
    "        (pl.col(f\"{r}_prev_tail_lag1\") - pl.col(f\"{r}_prev_tail_lag{K+1}\")).alias(f\"{r}_prev_tail_d{K}\")\n",
    "        for r in rep_cols for K in tail_diffs\n",
    "    ])\n",
    "\n",
    "    # prev2day/overnight/rolling\n",
    "    daily = daily.with_columns([\n",
    "        pl.col(f\"{r}_prevday_close\").shift(1).over(g_symbol).alias(f\"{r}_prev2day_close\")\n",
    "        for r in rep_cols\n",
    "    ]).with_columns(\n",
    "        [\n",
    "            (pl.col(f\"{r}_prevday_close\") - pl.col(f\"{r}_prevday_mean\")).alias(f\"{r}_prevday_close_minus_mean\")\n",
    "            for r in rep_cols\n",
    "        ] + [\n",
    "            (pl.col(f\"{r}_prevday_close\") - pl.col(f\"{r}_prev2day_close\")).alias(f\"{r}_overnight_gap\")\n",
    "            for r in rep_cols\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if rolling_windows:\n",
    "        wins = sorted({int(w) for w in rolling_windows if int(w) > 1})\n",
    "        roll_exprs = []\n",
    "        for r in rep_cols:\n",
    "            for w in wins:\n",
    "                roll_exprs += [\n",
    "                    pl.col(f\"{r}_prevday_close\").rolling_mean(window_size=w, min_samples=1).over(g_symbol).alias(f\"{r}_close_roll{w}_mean\"),\n",
    "                    pl.col(f\"{r}_prevday_close\").rolling_std(window_size=w, ddof=0, min_samples=1).over(g_symbol).alias(f\"{r}_close_roll{w}_std\"),\n",
    "                ]\n",
    "        daily = daily.with_columns(roll_exprs)\n",
    "\n",
    "\n",
    "    # === 关键：把所有“非键列”转为“对当日 d 生效的上一日值”（严格 d-1 + ≤K 回退） ===\n",
    "    prev_cols = [c for c in daily.collect_schema().names() if c not in (g_symbol, g_date)]\n",
    "    daily_prev = _resolve_prev_for_daily(\n",
    "        daily, keys=(g_symbol, g_date), cols=prev_cols, prev_soft_days=prev_soft_days, cast_f32=cast_f32\n",
    "    )\n",
    "\n",
    "    # 回拼到 tick 级\n",
    "    return lf.join(daily_prev, on=[g_symbol, g_date], how=\"left\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# B: same time_id cross-day\n",
    "# -----------------------------\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Sequence, Iterable, Optional, Tuple\n",
    "\n",
    "def fe_same_timeid_crossday(\n",
    "    lf: pl.LazyFrame,\n",
    "    rep_cols: Sequence[str],\n",
    "    ndays: int = 3,\n",
    "    stats_rep_cols: Optional[Sequence[str]] = None,\n",
    "    add_prev1_multirep: bool = True,\n",
    "    batch_size: int = 5,\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    assume_sorted: bool = True,\n",
    "    cast_f32: bool = True,\n",
    "    # <<< 新增：控制“严格 d-k / 宽松 K 天”\n",
    "    prev_soft_days: Optional[int] = None,   # None=不限制；比如 5 表示仅接受 gap<=5 天内的 prev{k}\n",
    "    strict_k: bool = False,                 # True=严格 d-k（gap==k）；忽略 prev_soft_days\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    同一 time_id 跨日 prev{k} + 统计（均值/方差/斜率）。\n",
    "\n",
    "    新增：\n",
    "    - prev_soft_days: 若设定，则对每个 prev{k} 仅保留 (date - date.shift(k)) <= K 的值，过旧置空。\n",
    "    - strict_k: 若 True，则严格 d-k（gap==k）；此时 prev_soft_days 被忽略。\n",
    "    默认维持原语义（纯 shift），仅在传参时启用过滤/严格。\n",
    "    \"\"\"\n",
    "    g_symbol, g_date, g_time = keys\n",
    "    if not assume_sorted:\n",
    "        # 保证 (symbol, time) 维度上 date 递增，再做 shift(k).over([symbol,time]) 是因果安全的\n",
    "        lf = lf.sort([g_symbol, g_time, g_date])\n",
    "\n",
    "    if stats_rep_cols is None:\n",
    "        stats_rep_cols = list(rep_cols)\n",
    "\n",
    "    def _chunks(lst, k):\n",
    "        for i in range(0, len(lst), k):\n",
    "            yield lst[i:i+k]\n",
    "\n",
    "    lf_cur = lf\n",
    "\n",
    "    # 1) prev{k}（可选：严格/TTL 过滤）\n",
    "    for batch in _chunks(list(rep_cols), batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            for k in range(1, ndays+1):\n",
    "                val_k  = pl.col(r).shift(k).over([g_symbol, g_time])\n",
    "                day_k  = pl.col(g_date).shift(k).over([g_symbol, g_time])\n",
    "                gap_k  = (pl.col(g_date) - day_k).cast(pl.Int32)\n",
    "\n",
    "                if strict_k:\n",
    "                    # 严格 d-k：仅 gap==k 保留\n",
    "                    val_k = pl.when(gap_k == k).then(val_k).otherwise(None)\n",
    "                elif prev_soft_days is not None:\n",
    "                    # 宽松 K 天：仅 gap<=K 保留（最近 k 个“可见历史”仍可能跨缺口，但限制不超过 K 天）\n",
    "                    val_k = pl.when(gap_k <= int(prev_soft_days)).then(val_k).otherwise(None)\n",
    "                # else: 保持原逻辑（纯 shift）\n",
    "\n",
    "                if cast_f32:\n",
    "                    val_k = val_k.cast(pl.Float32)\n",
    "                exprs.append(val_k.alias(f\"{r}_same_t_prev{k}\"))\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 2) mean/std（忽略 null）\n",
    "    for batch in _chunks([r for r in stats_rep_cols if r in rep_cols], batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            cols = [f\"{r}_same_t_prev{k}\" for k in range(1, ndays+1)]\n",
    "            vals = pl.concat_list([pl.col(c) for c in cols]).list.drop_nulls()\n",
    "            m = vals.list.mean()\n",
    "            s = vals.list.std(ddof=0)\n",
    "            if cast_f32:\n",
    "                m = m.cast(pl.Float32); s = s.cast(pl.Float32)\n",
    "            exprs += [\n",
    "                m.alias(f\"{r}_same_t_last{ndays}_mean\"),\n",
    "                s.alias(f\"{r}_same_t_last{ndays}_std\"),\n",
    "            ]\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 3) slope（标准化后与标准化时间权重点积 / 有效样本数）\n",
    "    x = np.arange(1, ndays+1, dtype=np.float64)\n",
    "    x = (x - x.mean()) / (x.std() + 1e-9)\n",
    "    x_lits = [pl.lit(float(v)) for v in x]\n",
    "\n",
    "    for batch in _chunks([r for r in stats_rep_cols if r in rep_cols], batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            cols = [f\"{r}_same_t_prev{k}\" for k in range(1, ndays+1)]\n",
    "            mean_ref = pl.col(f\"{r}_same_t_last{ndays}_mean\")\n",
    "            std_ref  = pl.col(f\"{r}_same_t_last{ndays}_std\")\n",
    "            terms = [((pl.col(c) - mean_ref) / (std_ref + 1e-9)) * x_lits[i]\n",
    "                    for i,c in enumerate(cols)]\n",
    "            n_eff = pl.sum_horizontal([pl.col(c).is_not_null().cast(pl.Int32) for c in cols]).cast(pl.Float32)\n",
    "            den   = pl.when(n_eff > 0).then(n_eff).otherwise(pl.lit(1.0))\n",
    "            slope = (pl.sum_horizontal(terms) / den)\n",
    "            if cast_f32: slope = slope.cast(pl.Float32)\n",
    "            exprs.append(slope.alias(f\"{r}_same_t_last{ndays}_slope\"))\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 4) 跨 responder 的 prev1 行内统计\n",
    "    if add_prev1_multirep and len(rep_cols) > 0:\n",
    "        n_rep = len(rep_cols)\n",
    "        prev1_cols = [f\"{r}_same_t_prev1\" for r in rep_cols]\n",
    "        prev1_list = pl.concat_list([pl.col(c) for c in prev1_cols]).list.drop_nulls()\n",
    "        m1 = prev1_list.list.mean()\n",
    "        s1 = prev1_list.list.std(ddof=0)\n",
    "        if cast_f32: m1 = m1.cast(pl.Float32); s1 = s1.cast(pl.Float32)\n",
    "        lf_cur = lf_cur.with_columns([\n",
    "            m1.alias(f\"prev1_same_t_mean_{n_rep}rep\"),\n",
    "            s1.alias(f\"prev1_same_t_std_{n_rep}rep\"),\n",
    "        ])\n",
    "\n",
    "    return lf_cur\n",
    "\n",
    "\n",
    "\n",
    "# C 系列：历史值、收益率、差分、极值归一化、指数加权均值等\n",
    "\n",
    "def build_history_features_polars(\n",
    "    *,\n",
    "    lf: Optional[pl.LazyFrame] = None,\n",
    "    paths: Optional[Sequence[str]] = None,\n",
    "    feature_cols: Sequence[str],\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    group_cols: Sequence[str] = (\"symbol_id\",),\n",
    "    assume_sorted: bool = False,\n",
    "    cast_f32: bool = True,\n",
    "    batch_size: int = 10,\n",
    "    lags: Iterable[int] = (1, 3),\n",
    "    ret_periods: Iterable[int] = (1,),\n",
    "    diff_periods: Iterable[int] = (1,),\n",
    "    rz_windows: Iterable[int] = (5,),\n",
    "    keep_rmean_rstd: bool = True,\n",
    "    ewm_spans: Iterable[int] = (10,),\n",
    "    cs_cols: Optional[Sequence[str]] = None,\n",
    "    cs_by: Sequence[str] = (\"date_id\",\"time_id\"),\n",
    "    prev_soft_days: Optional[int] = None,\n",
    ") -> pl.LazyFrame:\n",
    "    assert lf is not None or paths is not None, \"provide either lf or paths\"\n",
    "    if lf is None:\n",
    "        lf = pl.scan_parquet(list(paths))\n",
    "\n",
    "    g_sym, g_date, g_time = keys\n",
    "    by_grp = list(group_cols)\n",
    "    by_cs  = list(cs_by)\n",
    "    feature_cols = list(feature_cols)\n",
    "\n",
    "    need_cols = [*keys, *feature_cols]\n",
    "    schema = lf.collect_schema()\n",
    "    miss = [c for c in need_cols if c not in schema]\n",
    "    if miss:\n",
    "        raise KeyError(f\"Columns not found: {miss}\")\n",
    "\n",
    "    lf_out = lf.select(need_cols)\n",
    "    if not assume_sorted:\n",
    "        lf_out = lf_out.sort(list(keys))\n",
    "\n",
    "    def _chunks(lst, k):\n",
    "        for i in range(0, len(lst), k):\n",
    "            yield lst[i:i+k]\n",
    "\n",
    "    # ---- 规范化参数：None/[] -> 空元组；并去重/转 int/保正数 ----\n",
    "    def _clean_iter(x):\n",
    "        if not x:\n",
    "            return tuple()\n",
    "        return tuple(int(v) for v in x)\n",
    "    def _clean_pos_sorted_unique(x):\n",
    "        if not x:\n",
    "            return tuple()\n",
    "        return tuple(sorted({int(v) for v in x if int(v) >= 1}))\n",
    "\n",
    "    LAGS   = _clean_pos_sorted_unique(lags)\n",
    "    K_RET  = _clean_pos_sorted_unique(ret_periods)\n",
    "    K_DIFF = _clean_pos_sorted_unique(diff_periods)\n",
    "    RZW    = _clean_pos_sorted_unique(rz_windows)\n",
    "    SPANS  = _clean_pos_sorted_unique(ewm_spans)\n",
    "\n",
    "    # ---- TTL helpers ----\n",
    "    def _ttl_mask(k: int) -> pl.Expr:\n",
    "        if prev_soft_days is None:\n",
    "            return pl.lit(True)\n",
    "        return (pl.col(g_date) - pl.col(g_date).shift(k).over(by_grp)) <= int(prev_soft_days)\n",
    "\n",
    "    def _gate(expr: pl.Expr, k: int) -> pl.Expr:\n",
    "        if prev_soft_days is None:\n",
    "            return expr\n",
    "        return pl.when(_ttl_mask(k)).then(expr).otherwise(None)\n",
    "\n",
    "    # C1 lags（可选）\n",
    "    if LAGS:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for L in LAGS:\n",
    "                for c in batch:\n",
    "                    e = pl.col(c).shift(L).over(by_grp)\n",
    "                    e = _gate(e, L)\n",
    "                    if cast_f32: e = e.cast(pl.Float32)\n",
    "                    exprs.append(e.alias(f\"{c}__lag{L}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "    # C2 returns（可选）\n",
    "    if K_RET:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for c in batch:\n",
    "                cur = pl.col(c)\n",
    "                for k in K_RET:\n",
    "                    prev = (pl.col(f\"{c}__lag{k}\") if k in LAGS\n",
    "                            else pl.col(c).shift(k).over(by_grp))\n",
    "                    mask = _ttl_mask(k)\n",
    "                    ret = pl.when(mask & prev.is_not_null() & (prev != 0)).then(cur / prev - 1.0).otherwise(None)\n",
    "                    if cast_f32: ret = ret.cast(pl.Float32)\n",
    "                    exprs.append(ret.alias(f\"{c}__ret{k}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "    # C3 diffs（可选）\n",
    "    if K_DIFF:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for c in batch:\n",
    "                cur = pl.col(c)\n",
    "                for k in K_DIFF:\n",
    "                    prevk = pl.col(c).shift(k).over(by_grp)\n",
    "                    d = pl.when(_ttl_mask(k)).then(cur - prevk).otherwise(None)\n",
    "                    if cast_f32: d = d.cast(pl.Float32)\n",
    "                    exprs.append(d.alias(f\"{c}__diff{k}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "    # 仅当 RZ/EWM 需要、且没有 lag1 时，才建临时 lag1\n",
    "    need_tmp_lag1 = (bool(RZW) or bool(SPANS)) and (1 not in LAGS)\n",
    "    if need_tmp_lag1:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            lf_out = lf_out.with_columns([\n",
    "                pl.col(c).shift(1).over(by_grp).alias(f\"{c}__lag1_tmp\") for c in batch\n",
    "            ])\n",
    "\n",
    "    def _lag1_col(c: str) -> pl.Expr:\n",
    "        return pl.col(f\"{c}__lag1\") if 1 in LAGS else pl.col(f\"{c}__lag1_tmp\")\n",
    "\n",
    "    # C4 rolling r-z（可选）\n",
    "    if RZW:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            base_cols = []\n",
    "            for c in batch:\n",
    "                base_alias = f\"{c}__tminus1_base\"\n",
    "                base_cols.append(_gate(_lag1_col(c), 1).alias(base_alias))\n",
    "            lf_out = lf_out.with_columns(base_cols)\n",
    "\n",
    "            roll_exprs = []\n",
    "            for c in batch:\n",
    "                base = pl.col(f\"{c}__tminus1_base\")\n",
    "                for w in RZW:\n",
    "                    m  = base.rolling_mean(window_size=w, min_samples=1).over(by_grp)\n",
    "                    s  = base.rolling_std(window_size=w, ddof=0, min_samples=2).over(by_grp)\n",
    "                    rz = (base - m) / (s + 1e-9)\n",
    "                    if cast_f32:\n",
    "                        m = m.cast(pl.Float32); s = s.cast(pl.Float32); rz = rz.cast(pl.Float32)\n",
    "                    if keep_rmean_rstd:\n",
    "                        roll_exprs += [\n",
    "                            m.alias(f\"{c}__rmean{w}\"),\n",
    "                            s.alias(f\"{c}__rstd{w}\"),\n",
    "                            rz.alias(f\"{c}__rz{w}\"),\n",
    "                        ]\n",
    "                    else:\n",
    "                        roll_exprs.append(rz.alias(f\"{c}__rz{w}\"))\n",
    "            lf_out = lf_out.with_columns(roll_exprs)\n",
    "            lf_out = lf_out.drop([f\"{c}__tminus1_base\" for c in batch])\n",
    "\n",
    "    # C5 EWM（可选）\n",
    "    if SPANS:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            base_cols = []\n",
    "            for c in batch:\n",
    "                base_alias = f\"{c}__tminus1_base\"\n",
    "                base_cols.append(_gate(_lag1_col(c), 1).alias(base_alias))\n",
    "            lf_out = lf_out.with_columns(base_cols)\n",
    "\n",
    "            ewm_exprs = []\n",
    "            for c in batch:\n",
    "                base = pl.col(f\"{c}__tminus1_base\")\n",
    "                for s in SPANS:\n",
    "                    ema = base.ewm_mean(span=int(s), adjust=False, ignore_nulls=True).over(by_grp)\n",
    "                    if cast_f32: ema = ema.cast(pl.Float32)\n",
    "                    ewm_exprs.append(ema.alias(f\"{c}__ewm{s}\"))\n",
    "            lf_out = lf_out.with_columns(ewm_exprs)\n",
    "            lf_out = lf_out.drop([f\"{c}__tminus1_base\" for c in batch])\n",
    "\n",
    "    if need_tmp_lag1:\n",
    "        lf_out = lf_out.drop([f\"{c}__lag1_tmp\" for c in feature_cols])\n",
    "\n",
    "    # C6 cross-section rank（可选）\n",
    "    if cs_cols:\n",
    "        cs_cols = [c for c in cs_cols if c in feature_cols]\n",
    "        if cs_cols:\n",
    "            lf_out = lf_out.with_columns([\n",
    "                _gate(pl.col(c).shift(1).over(by_grp), 1).alias(f\"{c}__tminus1_base\")\n",
    "                for c in cs_cols\n",
    "            ])\n",
    "            cs_n_tbl = (\n",
    "                lf_out.select(list(by_cs))\n",
    "                      .group_by(list(by_cs))\n",
    "                      .len()\n",
    "                      .rename({\"len\": \"__cs_n\"})\n",
    "            )\n",
    "            lf_out = lf_out.join(cs_n_tbl, on=list(by_cs), how=\"left\")\n",
    "            lf_out = lf_out.with_columns([\n",
    "                pl.when(pl.col(f\"{c}__tminus1_base\").is_null())\n",
    "                  .then(None)\n",
    "                  .otherwise(pl.col(f\"{c}__tminus1_base\").rank(method=\"average\"))\n",
    "                  .over(list(by_cs))\n",
    "                  .alias(f\"{c}__cs_rank_raw\")\n",
    "                for c in cs_cols\n",
    "            ])\n",
    "            lf_out = lf_out.with_columns([\n",
    "                pl.when(pl.col(f\"{c}__tminus1_base\").is_null())\n",
    "                  .then(None)\n",
    "                  .otherwise(pl.col(f\"{c}__cs_rank_raw\") / pl.col(\"__cs_n\"))\n",
    "                  .cast(pl.Float32 if cast_f32 else pl.Float64)\n",
    "                  .alias(f\"{c}__csrank\")\n",
    "                for c in cs_cols\n",
    "            ])\n",
    "            lf_out = lf_out.drop(\n",
    "                [f\"{c}__tminus1_base\" for c in cs_cols] +\n",
    "                [f\"{c}__cs_rank_raw\"  for c in cs_cols]\n",
    "            )\n",
    "\n",
    "    return lf_out\n",
    "\n",
    "def collect_write(lf: pl.LazyFrame, path: str, compression: str = \"zstd\"):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        df = lf.collect(streaming=True)\n",
    "    except TypeError:\n",
    "        df = lf.collect()\n",
    "    df.write_parquet(path, compression=compression)\n",
    "    del df\n",
    "    \n",
    "@dataclass\n",
    "class StageA:\n",
    "    tail_lags: Sequence[int]\n",
    "    tail_diffs: Sequence[int]\n",
    "    rolling_windows: Optional[Sequence[int]]\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    assume_sorted: bool = True\n",
    "    cast_f32: bool = True\n",
    "\n",
    "@dataclass\n",
    "class StageB:\n",
    "    ndays: int\n",
    "    stats_rep_cols: Optional[Sequence[str]] = None\n",
    "    add_prev1_multirep: bool = True\n",
    "    batch_size: int = 5\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    strict_k: bool = False\n",
    "    assume_sorted: bool = True\n",
    "    cast_f32: bool = True\n",
    "\n",
    "# C 的每个操作可选；None / [] 表示跳过该操作\n",
    "@dataclass\n",
    "class StageC:\n",
    "    lags: Optional[Iterable[int]] = None\n",
    "    ret_periods: Optional[Iterable[int]] = None\n",
    "    diff_periods: Optional[Iterable[int]] = None\n",
    "    rz_windows: Optional[Iterable[int]] = None\n",
    "    ewm_spans: Optional[Iterable[int]] = None\n",
    "    keep_rmean_rstd: bool = True\n",
    "    cs_cols: Optional[Sequence[str]] = None\n",
    "    cs_by: Sequence[str] = (\"date_id\",\"time_id\")\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    batch_size: Optional[int] = 10\n",
    "    assume_sorted: bool = True\n",
    "    cast_f32: bool = True\n",
    "\n",
    "\n",
    "def run_staged_engineering(\n",
    "    lf_base: pl.LazyFrame,\n",
    "    *,\n",
    "    keys: Sequence[str],\n",
    "    rep_cols: Sequence[str],\n",
    "    feature_cols: Sequence[str],\n",
    "    out_dir: str,\n",
    "    A: StageA | None = None,\n",
    "    B: StageB | None = None,\n",
    "    C: StageC | None = None,\n",
    "):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    paths = {}\n",
    "\n",
    "    # ---------- A ----------\n",
    "    if A is not None:\n",
    "        lf_resp = lf_base.select([*keys, *rep_cols])\n",
    "        lf_a_full = fe_prevday_tail_and_summaries(\n",
    "            lf_resp,\n",
    "            rep_cols=rep_cols,\n",
    "            tail_lags=A.tail_lags,\n",
    "            tail_diffs=A.tail_diffs,\n",
    "            rolling_windows=A.rolling_windows,\n",
    "            keys=tuple(keys),\n",
    "            assume_sorted=A.assume_sorted,\n",
    "            cast_f32=A.cast_f32,\n",
    "            prev_soft_days=A.prev_soft_days,\n",
    "        )\n",
    "        drop = set(keys) | set(rep_cols)\n",
    "        a_cols = [c for c in lf_a_full.collect_schema().names() if c not in drop]\n",
    "        pA = f\"{out_dir}/stage_a.parquet\"\n",
    "        collect_write(lf_a_full.select([*keys, *a_cols]), pA)\n",
    "        paths[\"A\"] = pA\n",
    "\n",
    "    # ---------- B ----------\n",
    "    if B is not None:\n",
    "        lf_resp = lf_base.select([*keys, *rep_cols])\n",
    "        lf_b_full = fe_same_timeid_crossday(\n",
    "            lf_resp,\n",
    "            rep_cols=rep_cols,\n",
    "            ndays=B.ndays,\n",
    "            stats_rep_cols=B.stats_rep_cols,\n",
    "            add_prev1_multirep=B.add_prev1_multirep,\n",
    "            batch_size=B.batch_size,\n",
    "            keys=tuple(keys),\n",
    "            assume_sorted=B.assume_sorted,\n",
    "            cast_f32=B.cast_f32,\n",
    "            prev_soft_days=B.prev_soft_days,\n",
    "            strict_k=B.strict_k,\n",
    "        )\n",
    "        drop = set(keys) | set(rep_cols)\n",
    "        b_cols = [c for c in lf_b_full.collect_schema().names() if c not in drop]\n",
    "        pB = f\"{out_dir}/stage_b.parquet\"\n",
    "        collect_write(lf_b_full.select([*keys, *b_cols]), pB)\n",
    "        paths[\"B\"] = pB\n",
    "\n",
    "    # ---------- C：按“操作”分别输出单文件（不分窗口、不分 batch） ----------\n",
    "    if C is not None:\n",
    "        paths[\"C\"] = {}\n",
    "\n",
    "        def _do_op(op_name: str, **op_flags):\n",
    "            lf_src = lf_base.select([*keys, *feature_cols])\n",
    "            lf_d = build_history_features_polars(\n",
    "                lf=lf_src,\n",
    "                feature_cols=feature_cols,\n",
    "                keys=tuple(keys),\n",
    "                group_cols=(\"symbol_id\",),\n",
    "                assume_sorted=C.assume_sorted,\n",
    "                cast_f32=C.cast_f32,\n",
    "                batch_size=C.batch_size,\n",
    "                lags=op_flags.get(\"lags\"),\n",
    "                ret_periods=op_flags.get(\"ret_periods\"),\n",
    "                diff_periods=op_flags.get(\"diff_periods\"),\n",
    "                rz_windows=op_flags.get(\"rz_windows\"),\n",
    "                keep_rmean_rstd=C.keep_rmean_rstd,\n",
    "                ewm_spans=op_flags.get(\"ewm_spans\"),\n",
    "                cs_cols=op_flags.get(\"cs_cols\"),\n",
    "                cs_by=C.cs_by,\n",
    "                prev_soft_days=C.prev_soft_days\n",
    "            ).drop(feature_cols)\n",
    "            p = f\"{out_dir}/stage_c_{op_name}.parquet\"\n",
    "            collect_write(lf_d, p)\n",
    "            paths[\"C\"][op_name] = p\n",
    "\n",
    "        if C.lags:         _do_op(\"lags\",   lags=C.lags)\n",
    "        if C.ret_periods:  _do_op(\"ret\",    ret_periods=C.ret_periods)\n",
    "        if C.diff_periods: _do_op(\"diff\",   diff_periods=C.diff_periods)\n",
    "        if C.rz_windows:   _do_op(\"rz\",     rz_windows=C.rz_windows)\n",
    "        if C.ewm_spans:    _do_op(\"ewm\",    ewm_spans=C.ewm_spans)\n",
    "        if C.cs_cols:      _do_op(\"csrank\", cs_cols=C.cs_cols)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def _join_from(lf_left: pl.LazyFrame, path: str, lo: int, hi: int) -> pl.LazyFrame:\n",
    "    cols = FILE2COLS.get(path, [])\n",
    "    if not cols: return lf_left\n",
    "    lf_right = (pl.scan_parquet(path)\n",
    "                  .select([*KEYS, *cols])\n",
    "                  .filter(pl.col(\"date_id\").is_between(lo, hi))\n",
    "                  .sort(KEYS))\n",
    "    return lf_left.join(lf_right, on=KEYS, how=\"left\")\n",
    "\n",
    "def build_slice(lo: int, hi: int) -> pd.DataFrame:\n",
    "    lf = (pl.scan_parquet(PARQUET_PATHS)\n",
    "            .select([*KEYS, TARGET])\n",
    "            .filter(pl.col(\"date_id\").is_between(lo, hi))\n",
    "            .sort(KEYS))\n",
    "    for p in FILE2COLS:\n",
    "        lf = _join_from(lf, p, lo, hi)\n",
    "    return lf.collect(streaming=True).to_pandas()\n",
    "\n",
    "def weighted_r2_zero_mean(y_true, y_pred, weight) -> float:\n",
    "    \"\"\"\n",
    "    Sample-weighted zero-mean R^2 used in Jane Street:\n",
    "        R^2 = 1 - sum_i w_i (y_i - yhat_i)^2 / sum_i w_i y_i^2\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "    weight = np.asarray(weight, dtype=np.float64).ravel()\n",
    "    assert y_true.shape == y_pred.shape == weight.shape\n",
    "\n",
    "    num = np.sum(weight * (y_true - y_pred) ** 2)\n",
    "    den = np.sum(weight * (y_true ** 2))\n",
    "    if den <= 0:\n",
    "        return 0.0  # safe fallback (shouldn't happen on the full JS eval)\n",
    "    return 1.0 - (num / den)\n",
    "\n",
    "def lgb_wr2_eval(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    w = train_data.get_weight()\n",
    "    if w is None:\n",
    "        w = np.ones_like(y)\n",
    "    score = weighted_r2_zero_mean(y, preds, w)\n",
    "    return ('wr2', score, True)  # higher is better\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c19477",
   "metadata": {},
   "source": [
    "# 特征选择- 初选 (选特征，省略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b61d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, glob\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = [\"/mnt/data/js/clean/final_clean.parquet\"]\n",
    "KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "TARGET = \"responder_6\"\n",
    "WEIGHT = 'weight'\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)]\n",
    "\n",
    "OUT_DIR = \"/mnt/data/js/cache/first_selection\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- A: prev-day tails + daily summaries ---\n",
    "A = StageA(\n",
    "    tail_lags=(1,),\n",
    "    tail_diffs=(1,),\n",
    "    rolling_windows=(5,),\n",
    "    prev_soft_days=3,          # allow fallback up to 3 calendar days\n",
    ")\n",
    "\n",
    "# --- B: same time_id cross-day ---\n",
    "B = StageB(\n",
    "    ndays=3,                   # prev{1..3} at same time_id\n",
    "    stats_rep_cols=None,       # default: use rep_cols\n",
    "    add_prev1_multirep=True,\n",
    "    batch_size=5,\n",
    "    prev_soft_days=3,          # TTL for gaps\n",
    "    strict_k=False,            # allow ≤K-day gaps instead of strict d-k\n",
    ")\n",
    "\n",
    "# --- C: history features (keep it tiny) ---\n",
    "C = StageC(\n",
    "    lags=(1, ),\n",
    "    ret_periods=(1,),\n",
    "    diff_periods=(1,),\n",
    "    rz_windows=(5,),\n",
    "    ewm_spans=(10,),\n",
    "    keep_rmean_rstd=True,\n",
    "    cs_cols=None,        # must be subset of feature_cols\n",
    "    cs_by=(\"date_id\",\"time_id\"),\n",
    "    prev_soft_days=3,\n",
    ")\n",
    "\n",
    "# example call\n",
    "paths = run_staged_engineering(\n",
    "    lf_base=lf_base,                # your base LazyFrame\n",
    "    keys=KEYS,\n",
    "    rep_cols=REP_COLS,         # updated to use REP_COLS\n",
    "    feature_cols=FEATURE_COLS, # updated to use FEATURE_COLS\n",
    "    out_dir=OUT_DIR,\n",
    "    A=A, B=B, C=C,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e1720",
   "metadata": {},
   "source": [
    "0. 准备与切分天数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4df6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STAGE_PATHS = [\n",
    "    \"/mnt/data/js/cache/first_selection/stage_a.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_b.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_lags.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_ret.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_diff.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_rz.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_ewm.parquet\",\n",
    "]\n",
    "\n",
    "DATE_LO, DATE_HI = 1200, 1400\n",
    "OUT_DIR = \"/mnt/data/js/cache/first_selection/run_full\"\n",
    "SHARD_DIR = f\"{OUT_DIR}/shards_all\"\n",
    "Path(SHARD_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lf_base = pl.scan_parquet(BASE_PATH)\n",
    "# 仅拿目标区间的base\n",
    "lf_range = lf_base.filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "\n",
    "# days & split\n",
    "days = (lf_range.select(pl.col(\"date_id\").unique().sort())\n",
    "                .collect(streaming=True)[\"date_id\"].to_list())\n",
    "cut = int(len(days) * 0.8)\n",
    "train_days, val_days = days[:cut], days[cut:]\n",
    "print(f\"[split] train {len(train_days)} days, val {len(val_days)} days, range={days[0]}..{days[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eddc42",
   "metadata": {},
   "source": [
    "1.收集全量特征列名（并集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 来自 base 的特征列\n",
    "feat_cols = set(FEATURE_COLS)\n",
    "\n",
    "# 各 stage 全部列（除 KEYS/TARGET/WEIGHT）\n",
    "for p in STAGE_PATHS:\n",
    "    if not os.path.exists(p):\n",
    "        print(f\"[skip] missing: {p}\")\n",
    "        continue\n",
    "    names = pl.scan_parquet(p).collect_schema().names()\n",
    "    for c in names:\n",
    "        if c not in KEYS and c not in (TARGET, WEIGHT):\n",
    "            feat_cols.add(c)\n",
    "\n",
    "feat_cols = sorted(feat_cols)\n",
    "print(f\"[cols] total feature columns = {len(feat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278b8fe",
   "metadata": {},
   "source": [
    "2. 写“天片”—把所有列拼上并立刻落盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01289383",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_PER_SHARD = 16\n",
    "\n",
    "# 左表（含 base 的 FEATURE_COLS）\n",
    "lf_left_base = (\n",
    "    lf_range\n",
    "    .select([*KEYS, TARGET, WEIGHT, *[pl.col(c) for c in FEATURE_COLS]])\n",
    ")\n",
    "\n",
    "# 为每个 stage 准备元信息（列名 + 文件大小，先拼小文件更省内存）\n",
    "stage_meta = []\n",
    "for p in STAGE_PATHS:\n",
    "    if not os.path.exists(p): \n",
    "        continue\n",
    "    scan = pl.scan_parquet(p).filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "    cols = [c for c in scan.collect_schema().names() if c not in KEYS]\n",
    "    if cols:\n",
    "        stage_meta.append({\"path\": p, \"cols\": cols, \"size\": os.path.getsize(p)})\n",
    "stage_meta.sort(key=lambda d: d[\"size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637955db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_shards(tag, days_list):\n",
    "    ds = sorted(days_list)\n",
    "    for i in range(0, len(ds), DAYS_PER_SHARD):\n",
    "        batch = set(ds[i:i+DAYS_PER_SHARD])\n",
    "\n",
    "        # 当前片的左表\n",
    "        lf_chunk = lf_left_base.filter(pl.col(\"date_id\").is_in(batch))\n",
    "        already = set(lf_chunk.collect_schema().names())\n",
    "\n",
    "        # 逐 stage 拼接（右表只取该片天数 + 只取未存在列）\n",
    "        for m in stage_meta:\n",
    "            need = [c for c in m[\"cols\"] if c not in already]\n",
    "            if not need:\n",
    "                continue\n",
    "            lf_add = (pl.scan_parquet(m[\"path\"])\n",
    "                        .filter(pl.col(\"date_id\").is_in(batch))\n",
    "                        .select([*KEYS, *need]))\n",
    "            lf_chunk = lf_chunk.join(lf_add, on=KEYS, how=\"left\")\n",
    "            already.update(need)\n",
    "\n",
    "        # 统一 float32 并落盘（列按 feat_cols 顺序对齐；片内缺失的列自然是 null）\n",
    "        present = [c for c in feat_cols if c in already]\n",
    "        cast_feats = [pl.col(c).cast(pl.Float32).alias(c) for c in present]\n",
    "        lf_out = lf_chunk.select([\n",
    "            *KEYS,\n",
    "            pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "            pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "            *cast_feats,\n",
    "        ])\n",
    "        out_path = f\"{SHARD_DIR}/{tag}_shard_{i//DAYS_PER_SHARD:04d}.parquet\"\n",
    "        lf_out.sink_parquet(out_path, compression=\"zstd\")\n",
    "        print(f\"[{tag}] wrote {out_path}\")\n",
    "        gc.collect()\n",
    "\n",
    "write_shards(\"train\", train_days)\n",
    "write_shards(\"val\",   val_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173b9e",
   "metadata": {},
   "source": [
    "3. 从 shards 构建 memmap 数组 （恒定内存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memmap_from_shards(glob_pat, feat_cols, prefix):\n",
    "    paths = sorted(glob.glob(glob_pat))\n",
    "    counts = [pl.scan_parquet(p).select(pl.len()).collect(streaming=True).item() for p in paths]\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    print(f\"[memmap] {glob_pat}: {len(paths)} files, {n_rows} rows, {n_feat} features\")\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    i = 0\n",
    "    for p, k in zip(paths, counts):\n",
    "        lf = pl.scan_parquet(p)\n",
    "        names = set(lf.collect_schema().names())\n",
    "        exprs = [\n",
    "            (pl.col(c).cast(pl.Float32).alias(c) if c in names\n",
    "             else pl.lit(None, dtype=pl.Float32).alias(c))\n",
    "            for c in feat_cols\n",
    "        ]\n",
    "        df = lf.select([\n",
    "            pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "            pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "            *exprs\n",
    "        ]).collect(streaming=True)\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(TARGET)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(WEIGHT)).to_numpy().ravel()\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush()\n",
    "    return X, y, w\n",
    "\n",
    "train_X, train_y, train_w = memmap_from_shards(f\"{SHARD_DIR}/train_shard_*.parquet\", feat_cols, f\"{OUT_DIR}/train\")\n",
    "val_X,   val_y,   val_w   = memmap_from_shards(f\"{SHARD_DIR}/val_shard_*.parquet\",   feat_cols, f\"{OUT_DIR}/val\")\n",
    "\n",
    "print(\"train shapes:\", train_X.shape, train_y.shape, train_w.shape)\n",
    "print(\"val   shapes:\", val_X.shape,   val_y.shape,   val_w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101d7bd",
   "metadata": {},
   "source": [
    "4. LightGBM 训练 + 重要性 （一次性全列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76652e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(train_X, label=train_y, weight=train_w,\n",
    "                     feature_name=feat_cols, free_raw_data=True)\n",
    "dval   = lgb.Dataset(val_X,   label=val_y,   weight=val_w,\n",
    "                     feature_name=feat_cols, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\", metric=\"None\",\n",
    "    num_threads=16, seed=42, deterministic=True, first_metric_only=True,\n",
    "    learning_rate=0.05, num_leaves=31, max_depth=-1, min_data_in_leaf=20,\n",
    "    # 内存友好\n",
    "    max_bin=63, bin_construct_sample_cnt=100_000, min_data_in_bin=3,\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params, dtrain,\n",
    "    valid_sets=[dval, dtrain], valid_names=[\"val\",\"train\"],\n",
    "    num_boost_round=1000, callbacks=[lgb.early_stopping(50)],\n",
    "    feval=lgb_wr2_eval,   # 你的评估函数\n",
    ")\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": model.feature_name(),\n",
    "    \"gain\": model.feature_importance(\"gain\"),\n",
    "    \"split\": model.feature_importance(\"split\"),\n",
    "}).sort_values(\"gain\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(imp.head(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.to_csv(f\"{OUT_DIR}/imp_1r.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7305fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.read_csv(f\"{OUT_DIR}/imp_1r.csv\")\n",
    "top_feats = imp.loc[imp.gain > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864927ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = top_feats['feature'].str.extract(r'^(feature_\\d{2}|responder_\\d)', expand=False)\n",
    "top_feats['family'] = fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f31e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_feats = top_feats.groupby('family').agg(\n",
    "    n = ('feature', 'count'),\n",
    "    gain = ('gain', 'sum'),\n",
    "    split = ('split', 'sum'),\n",
    ").reset_index().sort_values('gain', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03415ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fam_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_feat = fam_feats['family'].str.startswith('feature_', na=False)\n",
    "mask_resp = fam_feats[\"family\"].str.startswith(\"responder_\", na=False)\n",
    "features_only   = fam_feats[mask_feat].sort_values(\"gain\", ascending=False)\n",
    "responders_only = fam_feats[mask_resp].sort_values(\"gain\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = features_only['family'][:79] # select all\n",
    "selected_resps = responders_only['family'][:9] # select all\n",
    "\n",
    "# save the Series (no index)\n",
    "selected_features.to_csv(f\"{OUT_DIR}/selected_features_1r.csv\", index=False, header=False)\n",
    "selected_resps.to_csv(f\"{OUT_DIR}/selected_responders_1r.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230fcef",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec3fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集参数\n",
    "\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)] #FEATURE_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_features_1r.csv\", header=None).squeeze().tolist()\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)] #REP_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_responders_1r.csv\", header=None).squeeze().tolist()\n",
    "\n",
    "paths = fs.glob(f\"{P('az', 'exp/v1', cfg['paths']['clean'])}/*.parquet\")\n",
    "clean_files = []\n",
    "for p in paths:\n",
    "    bn = os.path.basename(p)\n",
    "    parts = bn.split('_')\n",
    "    start = int(parts[1])\n",
    "    clean_files.append((start, p))\n",
    "    \n",
    "clean_sorted_file_list = [f\"az://{f}\" for _, f in sorted(clean_files)]\n",
    "\n",
    "lc = pl.scan_parquet(clean_sorted_file_list, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3304b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_DAYS = 30                 # 每片保留的核心天数\n",
    "# 例：A 的最大 rolling=60 天；C 的最大 tick 窗口=968 → 1 天\n",
    "PAD_DAYS = 60   # 自行按实际 A/B/C 调整\n",
    "\n",
    "# 所有交易日\n",
    "days = (lc.select(pl.col(\"date_id\").unique().sort())\n",
    "            .collect(streaming=True)[\"date_id\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be658d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分片并写出训练集\n",
    "for start_idx in range(PAD_DAYS, len(days), CORE_DAYS):\n",
    "    core_lo_idx = start_idx\n",
    "    core_hi_idx = min(start_idx + CORE_DAYS - 1, len(days) - 1)\n",
    "    pad_lo_idx  = core_lo_idx - PAD_DAYS\n",
    "\n",
    "    core_lo, core_hi = days[core_lo_idx], days[core_hi_idx]\n",
    "    pad_lo           = days[pad_lo_idx]\n",
    "    path = P(\"np\", \"exp/v1\", cfg[\"paths\"][\"clean_shards\"])\n",
    "    fs.mkdir(path, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    out_path = f\"{P('az', 'exp/v1', cfg['paths']['clean_shards'])}/clean_{core_lo}_{core_hi}_pad_{PAD_DAYS}.parquet\"\n",
    "\n",
    "    (lc\n",
    "    .filter(pl.col(\"date_id\").is_between(pad_lo, core_hi))\n",
    "    .select([*cfg['keys'], cfg['weight'], *FEATURE_COLS, *REP_COLS])\n",
    "    .sink_parquet(\n",
    "        out_path,\n",
    "        compression=\"zstd\",\n",
    "        storage_options=storage_options,   # ← 用你构造 FS 的那个 dict\n",
    "        statistics=True,                   # 可选：写入列统计\n",
    "        maintain_order=True,             # 可选：严格保持当前行序\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_shard_paths = fs.glob(f\"{P('az', 'exp/v1', cfg['paths']['clean_shards'])}/*.parquet\")\n",
    "\n",
    "start_points = []\n",
    "\n",
    "for p in clean_shard_paths:\n",
    "    bn = os.path.basename(p)\n",
    "    parts = bn.split('_')\n",
    "    start = int(parts[1])\n",
    "    start_points.append((start, p))\n",
    "    \n",
    "clean_shard_sorted_file_list = [f\"az://{f}\" for _, f in sorted(start_points)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- step 2: FE per clean shard (A+B once, C batched internally via C.batch_size) -------\n",
    "\n",
    "# params (tweak as needed)\n",
    "A = StageA(\n",
    "    tail_lags=(1,2,3,5),\n",
    "    tail_diffs=(1,2),\n",
    "    rolling_windows=(5,20,60),\n",
    "    prev_soft_days=7,\n",
    ")\n",
    "B = StageB(\n",
    "    ndays=5, stats_rep_cols=None, add_prev1_multirep=True,\n",
    "    batch_size=8, prev_soft_days=7, strict_k=False,\n",
    ")\n",
    "C = StageC(\n",
    "    lags=(1,16,64,100,256,968),\n",
    "    ret_periods=(1,16,64,100,256),\n",
    "    diff_periods=(1,16,64,100,256,968),\n",
    "    rz_windows=(16,64,100,256,968),\n",
    "    ewm_spans=(16,64,100,256,968),\n",
    "    keep_rmean_rstd=False,                 # 列多时建议关；需要时再开\n",
    "    cs_cols=None, cs_by=(\"date_id\",\"time_id\"),\n",
    "    prev_soft_days=7,\n",
    "    batch_size=4,                         # ← 内部分批规模；OOM 就降到 8/6\n",
    "    assume_sorted=True, cast_f32=True,\n",
    ")\n",
    "\n",
    "\n",
    "# ---- 写出到 az ----\n",
    "def trim_core(in_path: str, out_path_blob: str, lo: int, hi: int):\n",
    "    lf = (pl.scan_parquet(in_path)  # 本地读\n",
    "            .filter(pl.col(\"date_id\").is_between(lo, hi)))\n",
    "\n",
    "    lf.sink_parquet(out_path_blob, storage_options=storage_options)\n",
    "\n",
    "# 创建 FE 输出目录\n",
    "fe_dir = P(\"az\", \"exp/v1\", cfg[\"paths\"][\"fe_shards\"])\n",
    "fs.mkdir(fe_dir, exist_ok=True)\n",
    "\n",
    "# 创建本地临时工作区\n",
    "tmp_root = P('local', cfg['paths']['tmp'])\n",
    "os.makedirs(tmp_root, exist_ok=True)\n",
    "\n",
    "\n",
    "def fe_on_clean_shard(clean_shard_path: str, \n",
    "                    *, \n",
    "                    core_lo: int, \n",
    "                    core_hi: int,\n",
    "                    A=None, B=None, C=None,\n",
    "                    keys=cfg['keys'], \n",
    "                    rep_cols=REP_COLS, \n",
    "                    feature_cols=FEATURE_COLS,\n",
    "                    fe_dir=P(\"az\", \"exp/v1\", cfg[\"paths\"][\"fe_shards\"]),\n",
    "                    tmp_root=P('local', cfg['paths']['tmp'])):\n",
    "    \n",
    "    # clean_shards_path 是 az://... 的路径\n",
    "    lf_slice = pl.scan_parquet(clean_shard_path, storage_options=storage_options)\n",
    "\n",
    "    # 临时工作区仍用本地，跑完删除\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_root) as tmp_dir:\n",
    "        res = run_staged_engineering(\n",
    "            lf_base=lf_slice,\n",
    "            keys=keys, \n",
    "            rep_cols=rep_cols, \n",
    "            feature_cols=feature_cols,\n",
    "            out_dir=tmp_dir, \n",
    "            A=A, \n",
    "            B=B, \n",
    "            C=C\n",
    "        )\n",
    "\n",
    "        if \"A\" in res:\n",
    "            trim_core(res[\"A\"], f\"{fe_dir}/A_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "        if \"B\" in res:\n",
    "            trim_core(res[\"B\"], f\"{fe_dir}/B_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "        for op, p in res.get(\"C\", {}).items():\n",
    "            trim_core(p, f\"{fe_dir}/C_{op}_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "for raw_np in clean_shard_sorted_file_list:\n",
    "    bn = os.path.basename(raw_np)\n",
    "    parts = bn.split('_')\n",
    "    lo, hi = int(parts[1]), int(parts[2])\n",
    "    fe_on_clean_shard(raw_np, core_lo=lo, core_hi=hi, A=A, B=B, C=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117700e2",
   "metadata": {},
   "source": [
    "# 特征选择 二选（选参数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b84f17",
   "metadata": {},
   "source": [
    "至此，我们已经获得了全量数据的满参数下的各片特征量，接下来选小量样本 进行特征参数选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92abcb1",
   "metadata": {},
   "source": [
    "样本集_特征选择\n",
    "\n",
    "- 选择目标日期的数据集并合并\n",
    "- 划分训练集，验证集\n",
    "- 简单参数lgb训练\n",
    "- 特征重要性排序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a2a43",
   "metadata": {},
   "source": [
    "1. 基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2add24db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "# 列名与区间\n",
    "#FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)]  #pd.read_csv(\"/mnt/data/js/config/selected_features_1r.csv\",\n",
    "DATE_LO, DATE_HI = 1300, 1500 # 指定训练/验证的 date_id 范围, 后期转为全部训练集\n",
    "\n",
    "# 基本量\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)] #FEATURE_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_features_1r.csv\", header=None).squeeze().tolist()\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)] #REP_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_responders_1r.csv\", header=None).squeeze().tolist()\n",
    "\n",
    "paths = fs.glob(f\"{P('az', 'exp/v1', cfg['paths']['clean'])}/*.parquet\")\n",
    "clean_files = []\n",
    "for p in paths:\n",
    "    bn = os.path.basename(p)\n",
    "    parts = bn.split('_')\n",
    "    start = int(parts[1])\n",
    "    clean_files.append((start, p))\n",
    "    \n",
    "clean_sorted_file_list = [f\"az://{f}\" for _, f in sorted(clean_files)]\n",
    "\n",
    "lc = pl.scan_parquet(clean_sorted_file_list, storage_options=storage_options)\n",
    "\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1788951",
   "metadata": {},
   "source": [
    "2.枚举窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f692f97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows in range: [(1280, 1309), (1310, 1339), (1340, 1369), (1370, 1399), (1400, 1429)] ... (total 8)\n"
     ]
    }
   ],
   "source": [
    "# 从 Blob 列出全部 fe_shards 分片（返回不带协议的路径，要手动加 az://）\n",
    "\n",
    "fe_all = fs.glob(f\"{P('np', 'exp/v1',cfg['paths']['fe_shards'])}/*.parquet\")\n",
    "fe_all = [f\"az://{p}\" for p in fe_all]\n",
    "\n",
    "# 按日期范围筛选\n",
    "wins = set()\n",
    "for p in fe_all:\n",
    "    base = p.split(\"/\")[-1]  # e.g. C_lags_1220_1249.parquet\n",
    "    lo = int(base.split(\"_\")[-2]); hi = int(base.split(\"_\")[-1].split(\".\")[0])\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "# 取得该区间实际天\n",
    "days = [d for d in range(DATE_LO, DATE_HI+1)]\n",
    "#cut = int(len(days)*0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643f6de",
   "metadata": {},
   "source": [
    "3. 按窗口拼接 (A + B + 所有 C_*) → 直接写数据分片（无大表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f8976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import fsspec, gc\n",
    "\n",
    "T = int(cfg['ticks'])                 # 例如 968 或 969 —— 请以数据为准\n",
    "K = int(cfg['bucket_size'])           # 例如 6\n",
    "open_n  = int(cfg.get('open_auction_ticks', 5))\n",
    "close_n = int(cfg.get('close_auction_ticks', 5))\n",
    "\n",
    "# 安全的“上界截断”工具（兼容旧版 Polars 无 clip_max）\n",
    "def clip_upper(expr: pl.Expr, ub: int) -> pl.Expr:\n",
    "    return pl.when(expr > pl.lit(ub)).then(pl.lit(ub)).otherwise(expr)\n",
    "\n",
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "\n",
    "    # 基表（先筛行，再一次性加时间特征）\n",
    "    base = (\n",
    "        lc.filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "          .select([*cfg['keys'], cfg['target'], cfg['weight'], *FEATURE_COLS])\n",
    "        .with_columns([\n",
    "            # 复制一份 time_id 作为“位置特征”，避免与 key 列冲突\n",
    "            pl.col(\"time_id\").cast(pl.Float32).alias(\"time_pos\"),\n",
    "\n",
    "              # 周期相位：phase = 2π * time_id / T\n",
    "              (2*np.pi * pl.col(\"time_id\") / pl.lit(T, dtype=pl.Float32)).alias(\"_phase_\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # 兼容旧版：对表达式调用 .sin() / .cos()\n",
    "            pl.col(\"_phase_\").sin().cast(pl.Float32).alias(\"time_sin\"),\n",
    "            pl.col(\"_phase_\").cos().cast(pl.Float32).alias(\"time_cos\"),\n",
    "        ])\n",
    "        .drop([\"_phase_\"])\n",
    "        .with_columns([\n",
    "            # 开盘/收盘指示器（恰好 open_n / close_n 个 tick）\n",
    "            (pl.col(\"time_id\") <  pl.lit(open_n)).cast(pl.Int8).alias(\"is_open_auction\"),\n",
    "            (pl.col(\"time_id\") >= pl.lit(T - close_n)).cast(pl.Int8).alias(\"is_close_auction\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # 分桶：bucket = floor(time_id * K / T)，并防御性截到 [0..K-1]\n",
    "    bucket_raw = ( (pl.col('time_id') * pl.lit(K)) // pl.lit(T) )\n",
    "    bucket_capped = clip_upper(bucket_raw, K - 1)\n",
    "    base = base.with_columns([\n",
    "        bucket_capped.cast(pl.Int8).alias(f\"time_bucket_{K}\")\n",
    "    ])\n",
    "\n",
    "    lf = base  # 后面继续你的 join 逻辑\n",
    "\n",
    "    fe_files = []\n",
    "    for name in (f\"A_{lo}_{hi}.parquet\", f\"B_{lo}_{hi}.parquet\"):\n",
    "        p = f\"{P('az', 'exp/v1', cfg['paths']['fe_shards'])}/{name}\"\n",
    "        fe_files.append(p)\n",
    "\n",
    "    # 同窗口所有 C_* 分片\n",
    "    c_files = fs.glob(f\"{P('np', 'exp/v1', cfg['paths']['fe_shards'])}/C_*_{lo}_{hi}.parquet\")\n",
    "    c_files = [f\"az://{p}\" for p in c_files]\n",
    "    fe_files.extend(c_files)\n",
    "\n",
    "    # 逐个左连接\n",
    "    already = set(lf.collect_schema().names())\n",
    "    for fp in fe_files:\n",
    "        ds = pl.scan_parquet(fp, storage_options=storage_options)\n",
    "        names = ds.collect_schema().names()\n",
    "        add_cols = [c for c in names if c not in already]\n",
    "        if add_cols:\n",
    "            lf = lf.join(ds.select([*cfg['keys'], *add_cols]), on=cfg['keys'], how=\"left\")\n",
    "            already.update(add_cols)\n",
    "\n",
    "    # 选出特征并统一类型\n",
    "    feat_present = [c for c in already if c not in (*cfg['keys'], cfg['target'], cfg['weight'])]\n",
    "    select_exprs = [\n",
    "        *cfg['keys'],\n",
    "        pl.col(cfg['target']).cast(pl.Float32).alias(cfg['target']),\n",
    "        pl.col(cfg['weight']).cast(pl.Float32).alias(cfg['weight']),\n",
    "        *[pl.col(c).cast(pl.Float32).alias(c) for c in feat_present],\n",
    "    ]\n",
    "    lf_win = lf.select(select_exprs)\n",
    "\n",
    "    # 直接流式写分片\n",
    "    sample_path = P(\"az\", \"exp/v1\", cfg[\"paths\"][\"sample_fe\"])\n",
    "    fs.mkdir(sample_path, exist_ok=True)\n",
    "    out_path = f\"{sample_path}/sample_{w_lo}_{w_hi}.parquet\"\n",
    "    (\n",
    "        lf_win.sink_parquet(\n",
    "            out_path,\n",
    "            compression=\"zstd\",\n",
    "            storage_options=storage_options,\n",
    "            statistics=True,\n",
    "            maintain_order=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d7d31",
   "metadata": {},
   "source": [
    "4.生成最终特征清单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0780efb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final feature list size = 2454\n"
     ]
    }
   ],
   "source": [
    "import glob, polars as pl\n",
    "\n",
    "# 任选一个训练分片当“列模板”\n",
    "ref = f\"{P('az', 'exp/v1', cfg['paths']['sample_fe'])}/sample_1300_1309.parquet\"\n",
    "names = pl.scan_parquet(ref, storage_options=storage_options).collect_schema().names()\n",
    "\n",
    "# 直接从这个分片拿特征列（已包含 base + engineered）\n",
    "feat_cols = [c for c in names if c not in (*cfg['keys'], cfg['target'], cfg['weight'])]\n",
    "print(f\"final feature list size = {len(feat_cols)}\")\n",
    "\n",
    "df_feat = pd.DataFrame({'feature': feat_cols})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34bb65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.to_csv(f\"{P('local', cfg['paths']['tmp'])}/sample_feature_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95fc78",
   "metadata": {},
   "source": [
    "5.构建memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ae6e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mm_dir = P(\"local\", \"exp/v1\", cfg[\"paths\"][\"sample_mm\"])\n",
    "os.makedirs(mm_dir, exist_ok=True)\n",
    "\n",
    "def full_shard_key(p: str):\n",
    "    bn = os.path.basename(p)          # e.g. sample_1200_1219.parquet\n",
    "    m = re.match(r\"sample_(\\d+)_(\\d+)\\.parquet$\", bn)\n",
    "    if not m:\n",
    "        return (10**12, 10**12, bn)\n",
    "    lo, hi = map(int, m.groups())\n",
    "    return (lo, hi)\n",
    "\n",
    "\n",
    "def shard2memmap(glob_paths: list[str], feat_cols: list[str], prefix: str):\n",
    "    date_col   = cfg[\"keys\"][1]\n",
    "    target_col = cfg[\"target\"]\n",
    "    weight_col = cfg[\"weight\"]\n",
    "\n",
    "    paths = sorted(glob_paths, key=full_shard_key)\n",
    "\n",
    "    counts = []\n",
    "    for p in paths:\n",
    "        k = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "               .select(pl.len()).collect(streaming=True).item())\n",
    "        counts.append(int(k))\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "\n",
    "    os.makedirs(os.path.dirname(prefix), exist_ok=True)\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    d = np.memmap(f\"{prefix}.date.i32.mmap\",  dtype=\"int32\",   mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    i = 0\n",
    "    need_cols = [date_col, target_col, weight_col, *feat_cols]\n",
    "    for p, k in zip(paths, counts):\n",
    "        df = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "                .select(need_cols).collect(streaming=True))\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(target_col)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(weight_col)).to_numpy().ravel()\n",
    "        d[i:i+k]    = df.select(pl.col(date_col)).to_numpy().ravel().astype(\"int32\")\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush(); d.flush()\n",
    "\n",
    "    meta = {\n",
    "        \"n_rows\": int(n_rows), \"n_feat\": int(n_feat), \"dtype\": \"float32\", \"ts\": time.time(),\n",
    "        \"features\": list(feat_cols), \"target\": target_col, \"weight\": weight_col,\n",
    "        \"date_col\": date_col, \"files\": paths\n",
    "    }\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "    return X, y, w\n",
    "\n",
    "np_paths = fs.glob(f\"{P('np', 'exp/v1', cfg['paths']['sample_fe'])}/sample_*_*.parquet\")\n",
    "glob_paths = []\n",
    "for p in np_paths:\n",
    "    glob_paths.append(f\"az://{p}\")\n",
    "    \n",
    "X, y, w = shard2memmap(glob_paths= glob_paths, feat_cols=feat_cols, prefix=f\"{mm_dir}/full_sample_v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6a9c1",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6195e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑动式来分割训练集/验证集\n",
    "\n",
    "d = np.memmap(f\"/mnt/data/js/exp/v1/sample_mm/full_sample_v1.date.i32.mmap\", dtype=\"int32\", mode=\"r\")\n",
    "\n",
    "def make_sliding_cv(days:List, *, n_splits: int, gap_days: int = 5, train_to_val: int = 9):\n",
    "    \"\"\"\n",
    "    滑动式 walk-forward（固定 train:val = R:1，步长 = 当前折的验证长度 V_i）。\n",
    "    给定折数 K、gap G、比例 R:\n",
    "        (R + K) * V = N - G\n",
    "    其中 N 为 unique(date_id) 的天数。\n",
    "    做法：\n",
    "      1) V_base = floor((N - G) / (R + K))，余数 rem 均匀分配到前 rem 个验证窗口（各 +1）。\n",
    "      2) 训练窗口长度 T = R * V_base（固定）。\n",
    "      3) 第 1 折验证起点 v_lo = T + G；每折验证长度依次为 v_lens[i]，下一折起点递增 v_lo += V_i。\n",
    "      4) 每折：\n",
    "            train_days = days[tr_lo:tr_hi]，其中 tr_hi = v_lo - G，tr_lo = tr_hi - T\n",
    "            valid_days = days[v_lo:v_hi]，其中 v_hi = v_lo + V_i\n",
    "    返回：folds = [(train_idx, val_idx), ...]；若不可行返回 []。\n",
    "    \"\"\"\n",
    "    # 读取并排序唯一日期\n",
    "    N = len(days)\n",
    "    R = int(train_to_val)\n",
    "    K = int(n_splits)\n",
    "    usable = N - int(gap_days)\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "\n",
    "    # 基准验证窗口与余数\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "\n",
    "    # 固定训练窗口；验证窗口长度列表（前 rem 折 +1 天）\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "\n",
    "    # 第一折验证起点\n",
    "    v_lo = T + gap_days\n",
    "    folds = []\n",
    "\n",
    "    for V_i in v_lens:\n",
    "        v_hi  = v_lo + V_i\n",
    "        tr_hi = v_lo - gap_days\n",
    "        tr_lo = tr_hi - T\n",
    "\n",
    "        # 边界不可行就停止\n",
    "        if tr_lo < 0 or v_hi > N:\n",
    "            break\n",
    "\n",
    "        # 映射到样本行索引\n",
    "        tr_days = days[tr_lo:tr_hi]\n",
    "        va_days = days[v_lo:v_hi]\n",
    "        tr_idx  = np.flatnonzero(np.isin(d, tr_days))\n",
    "        va_idx  = np.flatnonzero(np.isin(d, va_days))\n",
    "        if len(tr_idx) == 0 or len(va_idx) == 0:\n",
    "            break\n",
    "\n",
    "        folds.append((tr_idx, va_idx))\n",
    "        v_lo = v_hi  # 下一折从这里开始\n",
    "\n",
    "    return folds\n",
    "\n",
    "folds = make_sliding_cv(days=d, n_splits=3, gap_days=5, train_to_val=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4287d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) 加载 memmap ----------\n",
    "import json, numpy as np, lightgbm as lgb\n",
    "prefix = f\"/mnt/data/js/exp/v1/sample_mm/full_sample_v1\"\n",
    "with open(f\"{prefix}.meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "n_rows, n_feat = meta[\"n_rows\"], meta[\"n_feat\"]\n",
    "feat_names = meta[\"features\"]\n",
    "\n",
    "X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows, n_feat))\n",
    "y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows,))\n",
    "w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows,))\n",
    "# 你之前已定义：weighted_r2_zero_mean、lgb_wr2_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bf999",
   "metadata": {},
   "source": [
    "6.训练LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a5a15c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows≈7,307,432, dense_groups≈2454, est GPU load≈13.36 GiB\n"
     ]
    }
   ],
   "source": [
    "# 1) 统计 200 天窗口的行数（按你真实筛选逻辑来）\n",
    "n_rows = (\n",
    "    lc.filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))  # 你选的 200天区间\n",
    "      .select(pl.len())\n",
    "      .collect()\n",
    "      .item()  # -> int\n",
    ")\n",
    "\n",
    "# 2) 估算 GPU “transfer to GPU” 的大头（经验值）\n",
    "n_feat = len(feat_names)\n",
    "dense_groups = int(n_feat)  # 按之前比例估\n",
    "bytes_est = n_rows * 0.8* dense_groups         \n",
    "gb_est = bytes_est / (1024**3)\n",
    "\n",
    "print(f\"rows≈{n_rows:,}, dense_groups≈{dense_groups}, est GPU load≈{gb_est:.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff74677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 74246\n",
      "[LightGBM] [Info] Number of data points in the train set: 5837040, number of used features: 2454\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2274 dense feature groups (12669.66 MB) transferred to GPU in 2.183158 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.000123\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's wr2: 0.0506093\tval's wr2: 0.0426513\n",
      "[200]\ttrain's wr2: 0.0804119\tval's wr2: 0.059281\n",
      "[300]\ttrain's wr2: 0.103504\tval's wr2: 0.0716149\n",
      "[400]\ttrain's wr2: 0.124374\tval's wr2: 0.0838898\n",
      "[500]\ttrain's wr2: 0.142423\tval's wr2: 0.0941556\n",
      "[600]\ttrain's wr2: 0.159163\tval's wr2: 0.103175\n",
      "[700]\ttrain's wr2: 0.174633\tval's wr2: 0.110798\n",
      "[800]\ttrain's wr2: 0.188808\tval's wr2: 0.117868\n",
      "[900]\ttrain's wr2: 0.202375\tval's wr2: 0.125044\n",
      "[1000]\ttrain's wr2: 0.215042\tval's wr2: 0.131487\n",
      "[1100]\ttrain's wr2: 0.22713\tval's wr2: 0.137805\n",
      "[1200]\ttrain's wr2: 0.238536\tval's wr2: 0.14355\n",
      "[1300]\ttrain's wr2: 0.249663\tval's wr2: 0.149415\n",
      "[1400]\ttrain's wr2: 0.260527\tval's wr2: 0.154868\n",
      "[1500]\ttrain's wr2: 0.270682\tval's wr2: 0.159871\n",
      "[1600]\ttrain's wr2: 0.280164\tval's wr2: 0.164284\n",
      "[1700]\ttrain's wr2: 0.289392\tval's wr2: 0.168794\n",
      "[1800]\ttrain's wr2: 0.298478\tval's wr2: 0.172976\n",
      "[1900]\ttrain's wr2: 0.307105\tval's wr2: 0.17697\n",
      "[2000]\ttrain's wr2: 0.315288\tval's wr2: 0.180927\n",
      "[2100]\ttrain's wr2: 0.322983\tval's wr2: 0.18441\n",
      "[2200]\ttrain's wr2: 0.330883\tval's wr2: 0.187863\n",
      "[2300]\ttrain's wr2: 0.338221\tval's wr2: 0.191132\n",
      "[2400]\ttrain's wr2: 0.345244\tval's wr2: 0.194026\n",
      "[2500]\ttrain's wr2: 0.352405\tval's wr2: 0.19705\n",
      "[2600]\ttrain's wr2: 0.359277\tval's wr2: 0.200291\n",
      "[2700]\ttrain's wr2: 0.365587\tval's wr2: 0.202982\n",
      "[2800]\ttrain's wr2: 0.372145\tval's wr2: 0.205226\n",
      "[2900]\ttrain's wr2: 0.378637\tval's wr2: 0.207442\n",
      "[3000]\ttrain's wr2: 0.384818\tval's wr2: 0.210057\n",
      "[3100]\ttrain's wr2: 0.39083\tval's wr2: 0.212301\n",
      "[3200]\ttrain's wr2: 0.396419\tval's wr2: 0.214576\n",
      "[3300]\ttrain's wr2: 0.402016\tval's wr2: 0.217103\n",
      "[3400]\ttrain's wr2: 0.40736\tval's wr2: 0.218874\n",
      "[3500]\ttrain's wr2: 0.413014\tval's wr2: 0.220775\n",
      "[3600]\ttrain's wr2: 0.418128\tval's wr2: 0.22276\n",
      "[3700]\ttrain's wr2: 0.423025\tval's wr2: 0.224905\n",
      "[3800]\ttrain's wr2: 0.428181\tval's wr2: 0.226816\n",
      "[3900]\ttrain's wr2: 0.433265\tval's wr2: 0.228987\n",
      "[4000]\ttrain's wr2: 0.43794\tval's wr2: 0.230724\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3997]\ttrain's wr2: 0.437795\tval's wr2: 0.230747\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 74246\n",
      "[LightGBM] [Info] Number of data points in the train set: 6535936, number of used features: 2454\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2274 dense feature groups (14186.66 MB) transferred to GPU in 2.270011 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.000219\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's wr2: 0.05583\tval's wr2: 0.112569\n",
      "[200]\ttrain's wr2: 0.084143\tval's wr2: 0.153463\n",
      "[300]\ttrain's wr2: 0.106666\tval's wr2: 0.181474\n",
      "[400]\ttrain's wr2: 0.126828\tval's wr2: 0.202237\n",
      "[500]\ttrain's wr2: 0.144503\tval's wr2: 0.220677\n",
      "[600]\ttrain's wr2: 0.160101\tval's wr2: 0.236638\n",
      "[700]\ttrain's wr2: 0.174053\tval's wr2: 0.251421\n",
      "[800]\ttrain's wr2: 0.187576\tval's wr2: 0.263812\n",
      "[900]\ttrain's wr2: 0.20048\tval's wr2: 0.276439\n",
      "[1000]\ttrain's wr2: 0.212346\tval's wr2: 0.287656\n",
      "[1100]\ttrain's wr2: 0.223972\tval's wr2: 0.298868\n",
      "[1200]\ttrain's wr2: 0.234828\tval's wr2: 0.308938\n",
      "[1300]\ttrain's wr2: 0.244926\tval's wr2: 0.318149\n",
      "[1400]\ttrain's wr2: 0.254582\tval's wr2: 0.327371\n",
      "[1500]\ttrain's wr2: 0.264426\tval's wr2: 0.337019\n",
      "[1600]\ttrain's wr2: 0.273706\tval's wr2: 0.346043\n",
      "[1700]\ttrain's wr2: 0.282626\tval's wr2: 0.354051\n",
      "[1800]\ttrain's wr2: 0.290803\tval's wr2: 0.361925\n",
      "[1900]\ttrain's wr2: 0.29883\tval's wr2: 0.369987\n",
      "[2000]\ttrain's wr2: 0.30662\tval's wr2: 0.376612\n",
      "[2100]\ttrain's wr2: 0.314455\tval's wr2: 0.383727\n",
      "[2200]\ttrain's wr2: 0.321777\tval's wr2: 0.390429\n",
      "[2300]\ttrain's wr2: 0.329203\tval's wr2: 0.396712\n",
      "[2400]\ttrain's wr2: 0.336209\tval's wr2: 0.402999\n",
      "[2500]\ttrain's wr2: 0.342622\tval's wr2: 0.409165\n",
      "[2600]\ttrain's wr2: 0.349104\tval's wr2: 0.415075\n",
      "[2700]\ttrain's wr2: 0.355455\tval's wr2: 0.420887\n",
      "[2800]\ttrain's wr2: 0.361859\tval's wr2: 0.42702\n",
      "[2900]\ttrain's wr2: 0.367744\tval's wr2: 0.4324\n",
      "[3000]\ttrain's wr2: 0.373569\tval's wr2: 0.438094\n",
      "[3100]\ttrain's wr2: 0.379195\tval's wr2: 0.443225\n",
      "[3200]\ttrain's wr2: 0.384785\tval's wr2: 0.448192\n",
      "[3300]\ttrain's wr2: 0.390248\tval's wr2: 0.45317\n",
      "[3400]\ttrain's wr2: 0.395394\tval's wr2: 0.45796\n",
      "[3500]\ttrain's wr2: 0.400483\tval's wr2: 0.462257\n",
      "[3600]\ttrain's wr2: 0.405481\tval's wr2: 0.466798\n",
      "[3700]\ttrain's wr2: 0.410231\tval's wr2: 0.471323\n",
      "[3800]\ttrain's wr2: 0.414993\tval's wr2: 0.475723\n",
      "[3900]\ttrain's wr2: 0.419896\tval's wr2: 0.480808\n",
      "[4000]\ttrain's wr2: 0.424545\tval's wr2: 0.485172\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4000]\ttrain's wr2: 0.424545\tval's wr2: 0.485172\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 74246\n",
      "[LightGBM] [Info] Number of data points in the train set: 6535936, number of used features: 2454\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2274 dense feature groups (14186.66 MB) transferred to GPU in 2.218195 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.000219\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's wr2: 0.05583\tval's wr2: 0.0957289\n",
      "[200]\ttrain's wr2: 0.084143\tval's wr2: 0.129939\n",
      "[300]\ttrain's wr2: 0.106666\tval's wr2: 0.153185\n",
      "[400]\ttrain's wr2: 0.126324\tval's wr2: 0.169562\n",
      "[500]\ttrain's wr2: 0.142939\tval's wr2: 0.184719\n",
      "[600]\ttrain's wr2: 0.15885\tval's wr2: 0.19646\n",
      "[700]\ttrain's wr2: 0.173238\tval's wr2: 0.208309\n",
      "[800]\ttrain's wr2: 0.186805\tval's wr2: 0.21952\n",
      "[900]\ttrain's wr2: 0.199385\tval's wr2: 0.22958\n",
      "[1000]\ttrain's wr2: 0.211357\tval's wr2: 0.2388\n",
      "[1100]\ttrain's wr2: 0.222754\tval's wr2: 0.247297\n",
      "[1200]\ttrain's wr2: 0.23372\tval's wr2: 0.256116\n",
      "[1300]\ttrain's wr2: 0.243912\tval's wr2: 0.264278\n",
      "[1400]\ttrain's wr2: 0.253949\tval's wr2: 0.27177\n",
      "[1500]\ttrain's wr2: 0.263947\tval's wr2: 0.279923\n",
      "[1600]\ttrain's wr2: 0.273115\tval's wr2: 0.286319\n",
      "[1700]\ttrain's wr2: 0.282354\tval's wr2: 0.293269\n",
      "[1800]\ttrain's wr2: 0.290923\tval's wr2: 0.299246\n",
      "[1900]\ttrain's wr2: 0.299051\tval's wr2: 0.3056\n",
      "[2000]\ttrain's wr2: 0.306667\tval's wr2: 0.311183\n",
      "[2100]\ttrain's wr2: 0.314475\tval's wr2: 0.316695\n",
      "[2200]\ttrain's wr2: 0.322144\tval's wr2: 0.322193\n",
      "[2300]\ttrain's wr2: 0.329533\tval's wr2: 0.327575\n",
      "[2400]\ttrain's wr2: 0.336439\tval's wr2: 0.332657\n",
      "[2500]\ttrain's wr2: 0.343024\tval's wr2: 0.337591\n",
      "[2600]\ttrain's wr2: 0.349684\tval's wr2: 0.342332\n",
      "[2700]\ttrain's wr2: 0.356142\tval's wr2: 0.347498\n",
      "[2800]\ttrain's wr2: 0.362438\tval's wr2: 0.351793\n",
      "[2900]\ttrain's wr2: 0.368633\tval's wr2: 0.356221\n",
      "[3000]\ttrain's wr2: 0.374264\tval's wr2: 0.360405\n",
      "[3100]\ttrain's wr2: 0.379851\tval's wr2: 0.364835\n",
      "[3200]\ttrain's wr2: 0.385471\tval's wr2: 0.368967\n",
      "[3300]\ttrain's wr2: 0.39073\tval's wr2: 0.373089\n",
      "[3400]\ttrain's wr2: 0.395921\tval's wr2: 0.377129\n",
      "[3500]\ttrain's wr2: 0.401151\tval's wr2: 0.380894\n",
      "[3600]\ttrain's wr2: 0.406195\tval's wr2: 0.384468\n",
      "[3700]\ttrain's wr2: 0.410982\tval's wr2: 0.388118\n",
      "[3800]\ttrain's wr2: 0.415899\tval's wr2: 0.391481\n",
      "[3900]\ttrain's wr2: 0.420582\tval's wr2: 0.394775\n",
      "[4000]\ttrain's wr2: 0.425319\tval's wr2: 0.398025\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4000]\ttrain's wr2: 0.425319\tval's wr2: 0.398025\n"
     ]
    }
   ],
   "source": [
    "ds_params = dict(\n",
    "    max_bin=31,                    # 降低直方图桶数，省显存/内存\n",
    "    bin_construct_sample_cnt=100000,# 构桶采样行数（默认是20万）\n",
    "    min_data_in_bin=3,\n",
    "    data_random_seed=42,\n",
    ")\n",
    "\n",
    "# 1) 全集 Dataset\n",
    "d_all = lgb.Dataset(\n",
    "    X, label=y, weight=w,\n",
    "    feature_name=feat_names,\n",
    "    free_raw_data=True,\n",
    "    params=ds_params,               # 让子集也继承这些设置\n",
    ")\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"None\",\n",
    "    device_type=\"gpu\",\n",
    "    learning_rate=0.08,\n",
    "    num_leaves=31,\n",
    "    max_depth=8,\n",
    "    feature_fraction=0.60,\n",
    "    bagging_fraction=0.60,\n",
    "    bagging_freq=1,\n",
    "    min_data_in_leaf=200,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 2) 多折训练 + 每折 wr2 + 汇总 gain_share（仅一张表）\n",
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "fi = pd.DataFrame({\"feature\": feat_names})\n",
    "scores = []\n",
    "\n",
    "for k, (tr, va) in enumerate(folds, 1):\n",
    "    dtrain = d_all.subset(tr, params=ds_params)    # 只构建本折的子集\n",
    "    dvalid = d_all.subset(va, params=ds_params)\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params, dtrain,\n",
    "        valid_sets=[dvalid, dtrain],\n",
    "        valid_names=[\"val\", \"train\"],\n",
    "        feval=lgb_wr2_eval,\n",
    "        num_boost_round=4000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "            lgb.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 每折分数\n",
    "    scores.append(bst.best_score[\"val\"][\"wr2\"])   # or bst.best_score[\"val\"][\"wr2\"]\n",
    "\n",
    "    # 每折 gain_share → 作为一列加入\n",
    "    g = bst.feature_importance(importance_type=\"gain\", iteration=bst.best_iteration).astype(float)\n",
    "    denom = g.sum()\n",
    "    fi[f\"fold{k}_gain_share\"] = (g / denom) if denom > 0 else np.zeros_like(g, dtype=float)\n",
    "    bst.free_dataset()                 # 释放 booster 里持有的 Dataset\n",
    "    del dtrain, dvalid, bst; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "078611a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总均值 + 排序 + 保存\n",
    "fold_cols = [c for c in fi.columns if c.startswith(\"fold\")]\n",
    "fi[\"mean_gain_share\"] = fi[fold_cols].mean(axis=1)\n",
    "fi = fi.sort_values(\"mean_gain_share\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33c083fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 汇总均值\n",
    "fi.to_csv(f\"/mnt/data/js/exp/v1/sample_mm//fe_v1_gain_share.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99fc9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.read_csv(f\"/mnt/data/js/exp/v1/sample_mm//fe_v1_gain_share.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1f195dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>fold1_gain_share</th>\n",
       "      <th>fold2_gain_share</th>\n",
       "      <th>fold3_gain_share</th>\n",
       "      <th>mean_gain_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>time_pos</td>\n",
       "      <td>0.004678</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>0.004574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>time_cos</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.003869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_36</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature_06</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.003192</td>\n",
       "      <td>0.003021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time_sin</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.002860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  fold1_gain_share  fold2_gain_share  fold3_gain_share  \\\n",
       "0    time_pos          0.004678          0.004576          0.004468   \n",
       "1    time_cos          0.004250          0.003616          0.003741   \n",
       "2  feature_36          0.002577          0.003578          0.003678   \n",
       "3  feature_06          0.002684          0.003187          0.003192   \n",
       "4    time_sin          0.002528          0.003014          0.003039   \n",
       "\n",
       "   mean_gain_share  \n",
       "0         0.004574  \n",
       "1         0.003869  \n",
       "2         0.003278  \n",
       "3         0.003021  \n",
       "4         0.002860  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "542ea4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = cfg.get(\"white_list\", [])\n",
    "fi_normal = fi[~fi[\"feature\"].isin(whitelist)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823becc8",
   "metadata": {},
   "source": [
    "展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6579d7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfi = fi_normal[[\"feature\", \"mean_gain_share\"]].copy()\n",
    "dfi.reset_index(drop=True, inplace=True)\n",
    "dfi['rank'] = dfi.index + 1\n",
    "\n",
    "cum_share = dfi[\"mean_gain_share\"].cumsum()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "ax1.plot(dfi[\"rank\"], dfi[\"mean_gain_share\"], color=\"tab:blue\")\n",
    "ax1.set_xlabel(\"Feature rank (desc)\")\n",
    "ax1.set_ylabel(\"Mean gain share\", color=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(dfi[\"rank\"], cum_share, color=\"tab:orange\")\n",
    "ax2.set_ylabel(\"Cumulative share\", color=\"tab:orange\")\n",
    "\n",
    "plt.title(\"Feature importance (gain share)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "865fe42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80% → Top-811\n",
      "90% → Top-1074\n",
      "95% → Top-1287\n"
     ]
    }
   ],
   "source": [
    "cum_share = dfi[\"mean_gain_share\"].cumsum()\n",
    "tot = cum_share.iloc[-1]\n",
    "for th in [0.8, 0.9, 0.95]:\n",
    "    k = (cum_share <= th*tot).sum()\n",
    "    print(f\"{th*100:.0f}% → Top-{k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a1d29bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time_pos',\n",
       " 'time_sin',\n",
       " 'time_cos',\n",
       " 'time_bucket_6',\n",
       " 'is_open_auction',\n",
       " 'is_close_auction']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cf27467",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feats = list(dict.fromkeys(whitelist + dfi['feature'][:800].tolist()))  # 保持顺序且不重复\n",
    "final_feats = pd.Series(final_feats)\n",
    "\n",
    "final_feats.to_csv(\"/mnt/data/js/exp/v1/sample_mm/top_fi_0910.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10f5f4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                time_pos\n",
       "1                time_sin\n",
       "2                time_cos\n",
       "3           time_bucket_6\n",
       "4         is_open_auction\n",
       "              ...        \n",
       "801    feature_28__lag100\n",
       "802    feature_23__ewm968\n",
       "803    feature_78__ewm256\n",
       "804            feature_56\n",
       "805    feature_42__lag968\n",
       "Length: 806, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0cf82",
   "metadata": {},
   "source": [
    "# 去共线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATHS = [\"/mnt/data/js/final_clean.parquet\"]\n",
    "KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "TARGET = \"responder_6\"\n",
    "FEATURE_COLS = pd.read_csv('/home/admin_ml/Jackson/projects/selected_features.csv')['family'].tolist()\n",
    "REP_COLS = pd.read_csv('/home/admin_ml/Jackson/projects/selected_resps.csv')['family'].tolist()\n",
    "\n",
    "lf_base = pl.scan_parquet(PARQUET_PATHS).select(KEYS+FEATURE_COLS+REP_COLS)\n",
    "\n",
    "\n",
    "lf_slice = lf_base.filter((pl.col(\"date_id\") >= 1200) & (pl.col(\"date_id\") <= 1400))\n",
    "\n",
    "PARAMS = dict(\n",
    "        prev_soft_days=7,\n",
    "        tail_lags=(2, 5, 20, 40, 100),\n",
    "        tail_diffs=(2, 5,),\n",
    "        rolling_windows=(5, 20),\n",
    "        same_time_ndays=5,\n",
    "        strict_k=False,\n",
    "        hist_lags=(1,2,5,10,20,100),\n",
    "        ret_periods=(1,5,20),\n",
    "        diff_periods=(1,5),\n",
    "        rz_windows=(5,20),\n",
    "        ewm_spans=(5,40,100),\n",
    "        cs_cols=None,       # <- keep this small to avoid blow-up\n",
    "    )\n",
    "\n",
    "lf_eng = run_engineering_on_slice(lf_slice, **PARAMS)\n",
    "\n",
    "feats = pd.read_csv(\"/home/admin_ml/Jackson/projects/final_fi_mean.csv\")[\"feature\"].tolist()\n",
    "\n",
    "lf_small = lf_eng.select(feats[:500])\n",
    "lf_small.collect(streaming=True).write_parquet(\"/mnt/data/js/X_ready.parquet\", compression=\"zstd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7dbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lf = pl.scan_parquet(\"/mnt/data/js/X_ready.parquet\")\n",
    "\n",
    "df = lf.collect(streaming=True).to_pandas()\n",
    "\n",
    "# Correlation (pairwise complete obs) + guard on min observations\n",
    "min_obs = max(50, int(0.3 * len(df)))  # tweak as you like\n",
    "C = df.corr(method=\"pearson\", min_periods=min_obs).abs().fillna(0.0)\n",
    "\n",
    "# Ensure to align rows & cols to the same (priority) order, fill any NaNs with 0\n",
    "order = feats\n",
    "C = C.reindex(index=order, columns=order).fillna(0.0).copy()\n",
    "\n",
    "\n",
    "# --- Prepare NumPy array for the greedy loop ---\n",
    "A = C.values\n",
    "np.fill_diagonal(A, 0.0)  # ensure the value is smaller than thresh, so the feature won't be dropped by value'1'\n",
    "m = len(order)\n",
    "\n",
    "# --- Greedy de-correlation (keep-by-priority, drop neighbors) ---\n",
    "THRESH = 0.97\n",
    "keep_mask = np.ones(m, dtype=bool)\n",
    "\n",
    "for i in range(m):\n",
    "    if not keep_mask[i]:\n",
    "        continue  # already removed by a higher-priority pick\n",
    "    # only check j > i (upper triangle) among still-active features\n",
    "    active_slice = keep_mask[i+1:]\n",
    "    drop = (A[i, i+1:] >= THRESH) & active_slice\n",
    "    active_slice[drop] = False  # marks into keep_mask[i+1:] via view\n",
    "keep = [order[i] for i in range(m) if keep_mask[i]]\n",
    "\n",
    "\n",
    "pd.DataFrame({'feature': keep}).to_csv(\"final_selected_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4ba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fca06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93ac3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5d57402",
   "metadata": {},
   "source": [
    "# 全数据训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67727de1",
   "metadata": {},
   "source": [
    "基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_LO, DATE_HI = 680, 1530 # 指定训练/验证的 date_id 范围, 后期转为全部训练集\n",
    "# 基本量\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)] #FEATURE_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_features_1r.csv\", header=None).squeeze().tolist()\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)] #REP_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_responders_1r.csv\", header=None).squeeze().tolist()\n",
    "\n",
    "paths = fs.glob(f\"{P('az', 'exp/v1', cfg['paths']['clean'])}/*.parquet\")\n",
    "clean_files = []\n",
    "for p in paths:\n",
    "    bn = os.path.basename(p)\n",
    "    parts = bn.split('_')\n",
    "    start = int(parts[1])\n",
    "    clean_files.append((start, p))\n",
    "    \n",
    "clean_sorted_file_list = [f\"az://{f}\" for _, f in sorted(clean_files)]\n",
    "\n",
    "lc = pl.scan_parquet(clean_sorted_file_list, storage_options=storage_options)\n",
    "\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433ce72",
   "metadata": {},
   "source": [
    "枚举窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdca44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 Blob 列出全部 fe_shards 分片（返回不带协议的路径，要手动加 az://）\n",
    "\n",
    "fe_all = fs.glob(f\"{P('np', 'exp/v1',cfg['paths']['fe_shards'])}/*.parquet\")\n",
    "fe_all = [f\"az://{p}\" for p in fe_all]\n",
    "\n",
    "# 按日期范围筛选\n",
    "wins = set()\n",
    "for p in fe_all:\n",
    "    base = p.split(\"/\")[-1]  # e.g. C_lags_1220_1249.parquet\n",
    "    lo = int(base.split(\"_\")[-2]); hi = int(base.split(\"_\")[-1].split(\".\")[0])\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "# 取得该区间实际天\n",
    "days = [d for d in range(DATE_LO, DATE_HI+1)]\n",
    "#cut = int(len(days)*0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecf2eb",
   "metadata": {},
   "source": [
    "3. 按窗口拼接 (A + B + 所有 C_*) → 直接写数据分片（无大表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826625ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import gc\n",
    "\n",
    "T = int(cfg['ticks'])                 # 例如 968 \n",
    "K = int(cfg['bucket_size'])           # 例如 6\n",
    "open_n  = int(cfg.get('open_auction_ticks', 5))\n",
    "close_n = int(cfg.get('close_auction_ticks', 5))\n",
    "\n",
    "# 安全的“上界截断”工具（兼容旧版 Polars 无 clip_max）\n",
    "def clip_upper(expr: pl.Expr, ub: int) -> pl.Expr:\n",
    "    return pl.when(expr > pl.lit(ub)).then(pl.lit(ub)).otherwise(expr)\n",
    "\n",
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "\n",
    "    # 基表（先筛行，再一次性加时间特征）\n",
    "    base = (\n",
    "        lc.filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "          .select([*cfg['keys'], cfg['target'], cfg['weight'], *FEATURE_COLS])\n",
    "        .with_columns([\n",
    "            # 复制一份 time_id 作为“位置特征”，避免与 key 列冲突\n",
    "            pl.col(\"time_id\").cast(pl.Float32).alias(\"time_pos\"),\n",
    "\n",
    "              # 周期相位：phase = 2π * time_id / T\n",
    "              (2*np.pi * pl.col(\"time_id\") / pl.lit(T, dtype=pl.Float32)).alias(\"_phase_\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # 兼容旧版：对表达式调用 .sin() / .cos()\n",
    "            pl.col(\"_phase_\").sin().cast(pl.Float32).alias(\"time_sin\"),\n",
    "            pl.col(\"_phase_\").cos().cast(pl.Float32).alias(\"time_cos\"),\n",
    "        ])\n",
    "        .drop([\"_phase_\"])\n",
    "        .with_columns([\n",
    "            # 开盘/收盘指示器（恰好 open_n / close_n 个 tick）\n",
    "            (pl.col(\"time_id\") <  pl.lit(open_n)).cast(pl.Int8).alias(\"is_open_auction\"),\n",
    "            (pl.col(\"time_id\") >= pl.lit(T - close_n)).cast(pl.Int8).alias(\"is_close_auction\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # 分桶：bucket = floor(time_id * K / T)，并防御性截到 [0..K-1]\n",
    "    bucket_raw = ( (pl.col('time_id') * pl.lit(K)) // pl.lit(T) )\n",
    "    bucket_capped = clip_upper(bucket_raw, K - 1)\n",
    "    base = base.with_columns([\n",
    "        bucket_capped.cast(pl.Int8).alias(f\"time_bucket_{K}\")\n",
    "    ])\n",
    "\n",
    "    lf = base  # 后面继续你的 join 逻辑\n",
    "\n",
    "    fe_files = []\n",
    "    for name in (f\"A_{lo}_{hi}.parquet\", f\"B_{lo}_{hi}.parquet\"):\n",
    "        p = f\"{P('az', 'exp/v1', cfg['paths']['fe_shards'])}/{name}\"\n",
    "        fe_files.append(p)\n",
    "\n",
    "    # 同窗口所有 C_* 分片\n",
    "    c_files = fs.glob(f\"{P('np', 'exp/v1', cfg['paths']['fe_shards'])}/C_*_{lo}_{hi}.parquet\")\n",
    "    c_files = [f\"az://{p}\" for p in c_files]\n",
    "    fe_files.extend(c_files)\n",
    "\n",
    "    # 逐个左连接\n",
    "    already = set(lf.collect_schema().names())\n",
    "    for fp in fe_files:\n",
    "        ds = pl.scan_parquet(fp, storage_options=storage_options)\n",
    "        names = ds.collect_schema().names()\n",
    "        add_cols = [c for c in names if c not in already]\n",
    "        if add_cols:\n",
    "            lf = lf.join(ds.select([*cfg['keys'], *add_cols]), on=cfg['keys'], how=\"left\")\n",
    "            already.update(add_cols)\n",
    "\n",
    "    # 选出特征并统一类型\n",
    "    feat_present = [c for c in already if c not in (*cfg['keys'], cfg['target'], cfg['weight'])]\n",
    "    select_exprs = [\n",
    "        *cfg['keys'],\n",
    "        pl.col(cfg['target']).cast(pl.Float32).alias(cfg['target']),\n",
    "        pl.col(cfg['weight']).cast(pl.Float32).alias(cfg['weight']),\n",
    "        *[pl.col(c).cast(pl.Float32).alias(c) for c in feat_present],\n",
    "    ]\n",
    "    lf_win = lf.select(select_exprs)\n",
    "\n",
    "    # 直接流式写分片\n",
    "    panel_path = P(\"az\", \"exp/v1\", cfg[\"paths\"][\"panel_shards\"])\n",
    "    fs.mkdir(panel_path, exist_ok=True)\n",
    "    out_path = f\"{panel_path}/panel_{w_lo}_{w_hi}.parquet\"\n",
    "    (\n",
    "        lf_win.sink_parquet(\n",
    "            out_path,\n",
    "            compression=\"zstd\",\n",
    "            storage_options=storage_options,\n",
    "            statistics=True,\n",
    "            maintain_order=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a744bb",
   "metadata": {},
   "source": [
    "4.导入最终特征清单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27270503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_cols = pd.read_csv(\"/mnt/data/js/exp/v1/sample_mm//fe_v1_gain_share.csv\")[\"feature\"].tolist()\n",
    "feat_cols[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e103cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118550e",
   "metadata": {},
   "source": [
    "5.构建memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mm_dir = P(\"local\", \"exp/v1\", cfg[\"paths\"][\"panel_mm\"])\n",
    "os.makedirs(mm_dir, exist_ok=True)\n",
    "\n",
    "def full_shard_key(p: str):\n",
    "    bn = os.path.basename(p)          # e.g. panel_1200_1219.parquet\n",
    "    m = re.match(r\"panel_(\\d+)_(\\d+)\\.parquet$\", bn)\n",
    "    if not m:\n",
    "        return (10**12, 10**12, bn)\n",
    "    lo, hi = map(int, m.groups())\n",
    "    return (lo, hi)\n",
    "\n",
    "\n",
    "def shard2memmap(glob_paths: list[str], feat_cols: list[str], prefix: str):\n",
    "    date_col   = cfg[\"keys\"][1]\n",
    "    target_col = cfg[\"target\"]\n",
    "    weight_col = cfg[\"weight\"]\n",
    "\n",
    "    paths = sorted(glob_paths, key=full_shard_key)\n",
    "\n",
    "    counts = []\n",
    "    for p in paths:\n",
    "        k = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "               .select(pl.len()).collect(streaming=True).item())\n",
    "        counts.append(int(k))\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "\n",
    "    os.makedirs(os.path.dirname(prefix), exist_ok=True)\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    d = np.memmap(f\"{prefix}.date.i32.mmap\",  dtype=\"int32\",   mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    i = 0\n",
    "    need_cols = [date_col, target_col, weight_col, *feat_cols]\n",
    "    for p, k in zip(paths, counts):\n",
    "        df = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "                .select(need_cols).collect(streaming=True))\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(target_col)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(weight_col)).to_numpy().ravel()\n",
    "        d[i:i+k]    = df.select(pl.col(date_col)).to_numpy().ravel().astype(\"int32\")\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush(); d.flush()\n",
    "\n",
    "    meta = {\n",
    "        \"n_rows\": int(n_rows), \"n_feat\": int(n_feat), \"dtype\": \"float32\", \"ts\": time.time(),\n",
    "        \"features\": list(feat_cols), \"target\": target_col, \"weight\": weight_col,\n",
    "        \"date_col\": date_col, \"files\": paths\n",
    "    }\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "    return X, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_paths = fs.glob(f\"{P('np', 'exp/v1', cfg['paths']['panel_shards'])}/panel_*_*.parquet\")\n",
    "glob_paths = []\n",
    "for p in np_paths:\n",
    "    glob_paths.append(f\"az://{p}\")\n",
    "    \n",
    "X, y, w = shard2memmap(glob_paths= glob_paths, feat_cols=feat_cols, prefix=f\"{mm_dir}/full_panel_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05800ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑动式来分割训练集/验证集\n",
    "\n",
    "d = np.memmap(f\"/mnt/data/js/exp/v1/panel_mm/full_panel_v1.date.i32.mmap\", dtype=\"int32\", mode=\"r\")\n",
    "\n",
    "def make_sliding_cv(days:List, *, n_splits: int, gap_days: int = 5, train_to_val: int = 9):\n",
    "    \"\"\"\n",
    "    滑动式 walk-forward（固定 train:val = R:1，步长 = 当前折的验证长度 V_i）。\n",
    "    给定折数 K、gap G、比例 R:\n",
    "        (R + K) * V = N - G\n",
    "    其中 N 为 unique(date_id) 的天数。\n",
    "    做法：\n",
    "      1) V_base = floor((N - G) / (R + K))，余数 rem 均匀分配到前 rem 个验证窗口（各 +1）。\n",
    "      2) 训练窗口长度 T = R * V_base（固定）。\n",
    "      3) 第 1 折验证起点 v_lo = T + G；每折验证长度依次为 v_lens[i]，下一折起点递增 v_lo += V_i。\n",
    "      4) 每折：\n",
    "            train_days = days[tr_lo:tr_hi]，其中 tr_hi = v_lo - G，tr_lo = tr_hi - T\n",
    "            valid_days = days[v_lo:v_hi]，其中 v_hi = v_lo + V_i\n",
    "    返回：folds = [(train_idx, val_idx), ...]；若不可行返回 []。\n",
    "    \"\"\"\n",
    "    # 读取并排序唯一日期\n",
    "    N = len(days)\n",
    "    R = int(train_to_val)\n",
    "    K = int(n_splits)\n",
    "    usable = N - int(gap_days)\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "\n",
    "    # 基准验证窗口与余数\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "\n",
    "    # 固定训练窗口；验证窗口长度列表（前 rem 折 +1 天）\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "\n",
    "    # 第一折验证起点\n",
    "    v_lo = T + gap_days\n",
    "    folds = []\n",
    "\n",
    "    for V_i in v_lens:\n",
    "        v_hi  = v_lo + V_i\n",
    "        tr_hi = v_lo - gap_days\n",
    "        tr_lo = tr_hi - T\n",
    "\n",
    "        # 边界不可行就停止\n",
    "        if tr_lo < 0 or v_hi > N:\n",
    "            break\n",
    "\n",
    "        # 映射到样本行索引\n",
    "        tr_days = days[tr_lo:tr_hi]\n",
    "        va_days = days[v_lo:v_hi]\n",
    "        tr_idx  = np.flatnonzero(np.isin(d, tr_days))\n",
    "        va_idx  = np.flatnonzero(np.isin(d, va_days))\n",
    "        if len(tr_idx) == 0 or len(va_idx) == 0:\n",
    "            break\n",
    "\n",
    "        folds.append((tr_idx, va_idx))\n",
    "        v_lo = v_hi  # 下一折从这里开始\n",
    "\n",
    "    return folds\n",
    "\n",
    "folds = make_sliding_cv(days=d, n_splits=3, gap_days=5, train_to_val=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e48a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) 加载 memmap ----------\n",
    "import json, numpy as np, lightgbm as lgb\n",
    "prefix = f\"/mnt/data/js/exp/v1/panel_mm/full_panel_v1\"\n",
    "with open(f\"{prefix}.meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "n_rows, n_feat = meta[\"n_rows\"], meta[\"n_feat\"]\n",
    "feat_names = meta[\"features\"]\n",
    "\n",
    "X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows, n_feat))\n",
    "y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows,))\n",
    "w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows,))\n",
    "# 你之前已定义：weighted_r2_zero_mean、lgb_wr2_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003eae7",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6bb5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 统计 200 天窗口的行数（按你真实筛选逻辑来）\n",
    "n_rows = (\n",
    "    lc.filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))  # 你选的 200天区间\n",
    "      .select(pl.len())\n",
    "      .collect()\n",
    "      .item()  # -> int\n",
    ")\n",
    "\n",
    "#  估算 GPU “transfer to GPU” 的大头（经验值）\n",
    "n_feat = len(feat_names)\n",
    "dense_groups = int(n_feat)  # 按之前比例估\n",
    "bytes_est = n_rows * 0.8* dense_groups         \n",
    "gb_est = bytes_est / (1024**3)\n",
    "\n",
    "print(f\"rows≈{n_rows:,}, dense_groups≈{dense_groups}, est GPU load≈{gb_est:.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_LO, DATE_HI = 680, 1529 # 指定训练/验证的 date_id 范围, 后期转为全部训练集\n",
    "\n",
    "BASE_PATH = f\"{BASE_DIR}/final_clean.parquet\"\n",
    "\n",
    "# 从 Blob 列出全部 fe_shards 分片（返回不带协议的路径，要手动加 az://）\n",
    "fe_all = fs.glob(f\"{FE_SHA_DIR_B}/*_*.parquet\")\n",
    "fe_all = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in fe_all]\n",
    "\n",
    "# 按日期范围筛选\n",
    "wins = set()\n",
    "for p in fe_all:\n",
    "    base = p.split(\"/\")[-1]  # e.g. C_lags_1220_1249.parquet\n",
    "    lo = int(base.split(\"_\")[-2]); hi = int(base.split(\"_\")[-1].split(\".\")[0])\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "\n",
    "# 取得该区间实际天，并按 80/20 切分（天为单位，避免泄露）\n",
    "days = (pl.scan_parquet(BASE_PATH, storage_options=storage_options)\n",
    "        .filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "        .select(pl.col(\"date_id\").unique().sort())\n",
    "        .collect(streaming=True)[\"date_id\"].to_list())\n",
    "assert days, \"no days found in range\"\n",
    "cut = int(len(days)*0.8)\n",
    "train_days, val_days = set(days[:cut]), set(days[cut:])\n",
    "print(f\"days total={len(days)}  train={len(train_days)}  val={len(val_days)}\")\n",
    "\n",
    "\n",
    "\n",
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "\n",
    "    # 基表 (带 TARGET/WEIGHT + 基础特征)\n",
    "    lf = (pl.scan_parquet(BASE_PATH, storage_options=storage_options)\n",
    "            .filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "            .select([*KEYS, TARGET, WEIGHT, *[pl.col(c) for c in FEATURE_COLS]]))\n",
    "\n",
    "    protocol = FE_SHA_DIR_B.split(\"://\", 1)[0] + \"://\"\n",
    "\n",
    "    # 若 fe_all 可能是无协议，先归一化\n",
    "    fe_set = {p if p.startswith(protocol) else protocol + p for p in fe_all}\n",
    "\n",
    "    fe_files = []\n",
    "    for name in (f\"A_{lo}_{hi}.parquet\", f\"B_{lo}_{hi}.parquet\"):\n",
    "        p = f\"{FE_SHA_DIR_B}/{name}\"  # 带协议\n",
    "        if p in fe_set:\n",
    "            fe_files.append(p)\n",
    "\n",
    "    # 同窗口所有 C_* 分片\n",
    "    c_files = fs.glob(f\"{FE_SHA_DIR_NP}/C_*_{lo}_{hi}.parquet\")  # 无协议\n",
    "    fe_files += [p if p.startswith(protocol) else protocol + p for p in sorted(c_files)]\n",
    "\n",
    "\n",
    "    # 逐个左连接（scan 到 Blob 时一定加 storage_options）\n",
    "    already = set(lf.collect_schema().names())\n",
    "    for fp in fe_files:\n",
    "        ds = pl.scan_parquet(fp, storage_options=storage_options)\n",
    "        names = ds.collect_schema().names()\n",
    "        add_cols = [c for c in names if c not in already]\n",
    "        if add_cols:\n",
    "            lf = lf.join(ds.select([*KEYS, *add_cols]), on=KEYS, how=\"left\")\n",
    "            already.update(add_cols)\n",
    "\n",
    "\n",
    "    # 统一 float32；注意：此窗口缺失的特征列**不写入**→ 之后 memmap 会自动补 None\n",
    "    feat_present = [c for c in already if c not in (*KEYS, TARGET, WEIGHT)]\n",
    "    select_exprs = [\n",
    "        *KEYS,\n",
    "        pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "        pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "        *[pl.col(c).cast(pl.Float32).alias(c) for c in feat_present],\n",
    "    ]\n",
    "    lf_win = lf.select(select_exprs)\n",
    "\n",
    "    # 直接流式写 train/val 分片（不 materialize 整个窗口）\n",
    "    out_train = f\"{FE_SHA_DIR_B}/train_{lo}_{hi}.parquet\"\n",
    "    out_val   = f\"{FE_SHA_DIR_B}/val_{lo}_{hi}.parquet\"\n",
    "    (lf_win.filter(pl.col(\"date_id\").is_in(list(train_days)))\n",
    "           .sink_parquet(out_train, compression=\"zstd\", storage_options=storage_options))\n",
    "    (lf_win.filter(pl.col(\"date_id\").is_in(list(val_days)))\n",
    "           .sink_parquet(out_val, compression=\"zstd\", storage_options=storage_options))\n",
    "\n",
    "    print(f\"[{lo}_{hi}] write: {os.path.basename(out_train)}, {os.path.basename(out_val)}\")\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "\n",
    "feat_cols = pd.read_csv('exp/v1/config/input_sets/top_imp.csv')\n",
    "feat_cols = feat_cols['feature'].tolist()\n",
    "\n",
    "feat_cols = [*feat_cols, 'time_id']      # 或 feat_cols + ['time_id']\n",
    "# Check the length of feature columns\n",
    "print(len(feat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shard2memmap(glob_pat, feat_cols, prefix, fs=None, storage_options=None,\n",
    "                 target_col=TARGET, weight_col=WEIGHT):\n",
    "    \"\"\"\n",
    "    将若干 parquet 分片顺序拼接为三份 memmap: X(float32), y(float32), w(float32)\n",
    "    - 支持本地路径和 az:// 路径（自动选择 glob 方式）\n",
    "    - 缺失的特征列用 None (=> NaN) 补齐，列顺序以 feat_cols 为准\n",
    "    - 会在同目录写出 {prefix}_{X,y,w}.float32.mmap 以及 {prefix}.meta.json\n",
    "    \"\"\"\n",
    "\n",
    "    def _is_az(p: str) -> bool:\n",
    "        return isinstance(p, str) and p.startswith(\"az://\")\n",
    "\n",
    "    def _glob_paths(pattern: str):\n",
    "        # 远程：用 fsspec.filesystem(\"az\").glob(无协议) → 再补上 az://\n",
    "        if _is_az(pattern):\n",
    "            assert fs is not None, \"fs must be provided for az:// glob\"\n",
    "            nopro = pattern[len(\"az://\"):]               # 去协议\n",
    "            listed = fs.glob(nopro)                       # 无协议\n",
    "            return [\"az://\" + p for p in sorted(listed)]  # 归一化为带协议\n",
    "        else:\n",
    "            import glob as _glob\n",
    "            return sorted(_glob.glob(pattern))            # 本地\n",
    "\n",
    "    def _scan(p: str):\n",
    "        if _is_az(p):\n",
    "            return pl.scan_parquet(p, storage_options=storage_options)\n",
    "        else:\n",
    "            return pl.scan_parquet(p)\n",
    "\n",
    "    # 1) 列出分片\n",
    "    paths = _glob_paths(glob_pat)\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No shards matched: {glob_pat}\")\n",
    "\n",
    "    # 2) 统计每个分片的行数（流式）\n",
    "    counts = []\n",
    "    for p in paths:\n",
    "        k = _scan(p).select(pl.len()).collect(streaming=True).item()\n",
    "        counts.append(int(k))\n",
    "\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    print(f\"[memmap] {glob_pat}: {len(paths)} files, {n_rows} rows, {n_feat} features\")\n",
    "\n",
    "    # 3) 分配 memmap（本地）\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    # 4) 逐分片流式写入\n",
    "    i = 0\n",
    "    for p, k in zip(paths, counts):\n",
    "        lf = _scan(p)\n",
    "        names = set(lf.collect_schema().names())\n",
    "        exprs = [\n",
    "            (pl.col(c).cast(pl.Float32).alias(c) if c in names\n",
    "             else pl.lit(None, dtype=pl.Float32).alias(c))\n",
    "            for c in feat_cols\n",
    "        ]\n",
    "        df = (lf.select([\n",
    "                pl.col(target_col).cast(pl.Float32).alias(target_col),\n",
    "                pl.col(weight_col).cast(pl.Float32).alias(weight_col),\n",
    "                *exprs\n",
    "             ])\n",
    "             .collect(streaming=True))\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(target_col)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(weight_col)).to_numpy().ravel()\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush()\n",
    "\n",
    "    # 5) 保存 meta\n",
    "    meta = {\"n_rows\": int(n_rows), \"n_feat\": int(n_feat), \"dtype\": \"float32\", \"ts\": time.time(),\n",
    "            \"features\": list(feat_cols), \"target\": target_col, \"weight\": weight_col}\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "    return X, y, w\n",
    "\n",
    "\n",
    "train_X, train_y, train_w = shard2memmap(\n",
    "    f\"{FE_SHA_DIR_B}/train_*.parquet\", feat_cols, f\"{MM_DIR}/train\",\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "val_X,   val_y,   val_w   = shard2memmap(\n",
    "    f\"{FE_SHA_DIR_B}/val_*.parquet\",   feat_cols, f\"{MM_DIR}/val\",\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "print(\"train shapes:\", train_X.shape, train_y.shape, train_w.shape)\n",
    "print(\"val   shapes:\", val_X.shape,   val_y.shape,   val_w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np\n",
    "\n",
    "def load_memmap(prefix, readonly=True):\n",
    "    meta_path = f\"{prefix}.meta.json\"\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"missing {meta_path}\")\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    dtype = np.dtype(meta.get(\"dtype\", \"float32\"))\n",
    "    n_rows, n_feat = int(meta[\"n_rows\"]), int(meta[\"n_feat\"])\n",
    "    mode = \"r\" if readonly else \"r+\"\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows,))\n",
    "    feat_cols = meta.get(\"features\")  # 用保存的列顺序，避免不一致\n",
    "    return X, y, w, feat_cols, meta\n",
    "\n",
    "def ensure_memmap(prefix, glob_pat, feat_cols, *, fs=None, storage_options=None,\n",
    "                  target_col=\"responder_6\", weight_col=\"weight\"):\n",
    "    try:\n",
    "        return load_memmap(prefix)\n",
    "    except FileNotFoundError:\n",
    "        X, y, w = shard2memmap(glob_pat, feat_cols, prefix, fs=fs, storage_options=storage_options,\n",
    "                               target_col=target_col, weight_col=weight_col)\n",
    "        # 重新加载一次，拿回 meta 和规范的 feat_cols\n",
    "        return load_memmap(prefix)\n",
    "\n",
    "# —— 使用：\n",
    "train_X, train_y, train_w, feat_cols, _ = ensure_memmap(\n",
    "    f\"{MM_DIR}/train\", f\"{FE_SHA_DIR_B}/train_*.parquet\", feat_cols,\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "val_X, val_y, val_w, _, _ = ensure_memmap(\n",
    "    f\"{MM_DIR}/val\", f\"{FE_SHA_DIR_B}/val_*.parquet\", feat_cols,\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def weighted_r2_zero_mean(y_true, y_pred, weight):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "    weight = np.asarray(weight, dtype=np.float64).ravel()\n",
    "    num = np.sum(weight * (y_true - y_pred)**2)\n",
    "    den = np.sum(weight * (y_true**2))\n",
    "    return 0.0 if den <= 0 else 1.0 - (num/den)\n",
    "\n",
    "def lgb_wr2_eval(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    w = train_data.get_weight()\n",
    "    if w is None:\n",
    "        w = np.ones_like(y)\n",
    "    return (\"wr2\", weighted_r2_zero_mean(y, preds, w), True)\n",
    "\n",
    "dtrain = lgb.Dataset(train_X, label=train_y, weight=train_w, feature_name=feat_cols, free_raw_data=True)\n",
    "dval   = lgb.Dataset(val_X,   label=val_y,   weight=val_w,   feature_name=feat_cols, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"None\",\n",
    "    learning_rate=0.03,              # ↓ 更小学习率，配合更多轮数\n",
    "    num_leaves=127,                  # ↑ 增强模型容量\n",
    "    max_depth=10,\n",
    "    min_data_in_leaf=200,            # ↑ 稳定、抑制过拟合\n",
    "    num_threads=16,\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    first_metric_only=True,\n",
    "\n",
    "    # 直方图/分箱（GPU 友好）\n",
    "    max_bin=63,\n",
    "    bin_construct_sample_cnt=400_000,# ↑ 分箱更准（全量训练）\n",
    "    min_data_in_bin=3,\n",
    "\n",
    "    # GPU\n",
    "    device_type=\"gpu\",\n",
    "    gpu_device_id=0,\n",
    "\n",
    "    # 采样 + 正则\n",
    "    feature_fraction=0.65,           # 列采样\n",
    "    feature_fraction_bynode=0.8,     # 结点级列采样\n",
    "    bagging_fraction=0.70,           # 行采样\n",
    "    bagging_freq=1,\n",
    "    lambda_l2=5.0,                   # L2 正则更稳\n",
    "    extra_trees=True,                # 更稳更快（常见于大数据）\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params, dtrain,\n",
    "    valid_sets=[dval], valid_names=[\"val\"],   # 只监控验证集，收敛更快\n",
    "    num_boost_round=4000,                     # ↑ 配合小 lr\n",
    "    callbacks=[lgb.early_stopping(200), lgb.log_evaluation(50)],\n",
    "    feval=lgb_wr2_eval,\n",
    ")\n",
    "\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": model.feature_name(),\n",
    "    \"gain\": model.feature_importance(\"gain\"),\n",
    "    \"split\": model.feature_importance(\"split\"),\n",
    "}).sort_values(\"gain\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(imp.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7dfde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果不存在则创建目录\n",
    "os.makedirs(\"exp/v1/models\", exist_ok=True)\n",
    "\n",
    "# save model\n",
    "model.save_model(\"exp/v1/models/lgbm_tv_model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并验证集和训练集，重新训练最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28575246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341fe6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c531e6",
   "metadata": {},
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bf46f",
   "metadata": {},
   "source": [
    "首先对测试集做特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b47d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 参数（与训练期基本一致，仅改为 test 路径/前缀） ======\n",
    "import os, gc\n",
    "import polars as pl\n",
    "\n",
    "DATE_LO, DATE_HI = 1530, 1698  # 可按需调整或覆盖\n",
    "\n",
    "# 从 Blob 列出全部 test fe_shards 分片（无协议 → 统一加协议）\n",
    "fe_all_np = fs.glob(f\"{FE_SHA_DIR_NP}/test_*_*.parquet\")\n",
    "protocol = FE_SHA_DIR_B.split(\"://\", 1)[0] + \"://\"\n",
    "fe_all = [p if p.startswith(protocol) else protocol + p for p in fe_all_np]\n",
    "\n",
    "# ---- 解析所有窗口，并按日期范围筛选（与训练期相同逻辑）----\n",
    "wins = set()\n",
    "for p in fe_all_np:\n",
    "    base = os.path.basename(p)                         # e.g. test_C_lags_1220_1249.parquet\n",
    "    stem = base[:-8]                                   # 去掉 .parquet\n",
    "    parts = stem.split(\"_\")\n",
    "    lo, hi = int(parts[-2]), int(parts[-1])            # 最后两段是 lo/hi\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "# ---- 获取该区间内实际存在的 date_id（仅用于 sanity/log；不做 train/val 切分）----\n",
    "days = (pl.scan_parquet(TEST_BASE_PATH, storage_options=storage_options)\n",
    "          .filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "          .select(pl.col(\"date_id\").unique().sort())\n",
    "          .collect(streaming=True)[\"date_id\"].to_list())\n",
    "assert days, \"no days found in range\"\n",
    "print(f\"days total={len(days)}\")\n",
    "\n",
    "# ---- 主循环：对每个 (lo,hi) 窗口横向拼接后，写出 test_{lo}_{hi}.parquet ----\n",
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "\n",
    "    # 基表（仅 KEYS + 可选 WEIGHT + 基础特征），并固定排序保证因果/可复现\n",
    "    # 注意：WEIGHT 在 test 里可能不存在，做个存在性判断\n",
    "    base_scan = (pl.scan_parquet(TEST_BASE_PATH, storage_options=storage_options)\n",
    "                   .filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "                   .sort(KEYS))\n",
    "\n",
    "    lf = base_scan.select([*KEYS, WEIGHT, *[pl.col(c) for c in FEATURE_COLS]])\n",
    "\n",
    "    # 归一化分片路径集合，方便存在性判断\n",
    "    fe_set = {p if p.startswith(protocol) else protocol + p for p in fe_all_np}\n",
    "\n",
    "    # A/B 文件（test_ 前缀）\n",
    "    fe_files = []\n",
    "    for name in (f\"test_A_{lo}_{hi}.parquet\", f\"test_B_{lo}_{hi}.parquet\"):\n",
    "        p = f\"{FE_SHA_DIR_B}/{name}\"  # 带协议\n",
    "        if p in fe_set or fs.exists(p):\n",
    "            fe_files.append(p)\n",
    "\n",
    "    # 同窗口所有 C_* 分片（无协议 → 加协议）\n",
    "    c_files_np = fs.glob(f\"{FE_SHA_DIR_NP}/test_C_*_{lo}_{hi}.parquet\")\n",
    "    fe_files += [p if p.startswith(protocol) else protocol + p for p in sorted(c_files_np)]\n",
    "\n",
    "    # 逐个左连接（严格只补充新列，避免重复；scan 到 Blob 时加 storage_options）\n",
    "    already = set(lf.collect_schema().names())\n",
    "    for fp in fe_files:\n",
    "        ds = pl.scan_parquet(fp, storage_options=storage_options)\n",
    "        names = ds.collect_schema().names()\n",
    "        add_cols = [c for c in names if c not in already]\n",
    "        if add_cols:\n",
    "            lf = lf.join(ds.select([*KEYS, *add_cols]), on=KEYS, how=\"left\")\n",
    "            already.update(add_cols)\n",
    "\n",
    "    # 统一为 Float32；test 无 TARGET，所以只保留 KEYS + 可选 WEIGHT + 所有特征\n",
    "    keep_non_key = [c for c in already if c not in set(KEYS)]\n",
    "    # 确保 WEIGHT 不在特征转换列表里被重复处理\n",
    "    feat_present = [c for c in keep_non_key if c not in (*KEYS, WEIGHT)]\n",
    "\n",
    "    select_exprs = [\n",
    "        *KEYS, \n",
    "        pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT), \n",
    "        *[pl.col(c).cast(pl.Float32).alias(c) for c in feat_present], ]\n",
    "    lf_win = lf.select(select_exprs)\n",
    "\n",
    "    # 写出单文件 test_{lo}_{hi}.parquet（流式，不 materialize 全窗口）\n",
    "    out_test = f\"{FE_SHA_DIR_B}/test_{lo}_{hi}.parquet\"\n",
    "    # 覆盖旧文件（若存在）\n",
    "    if fs.exists(out_test):\n",
    "        fs.rm(out_test)\n",
    "\n",
    "    lf_win.sink_parquet(\n",
    "        out_test,\n",
    "        compression=\"zstd\",\n",
    "        statistics=True,\n",
    "        storage_options=storage_options,\n",
    "        maintain_order=True,  # 上面已 sort(KEYS)，这里维持写出顺序\n",
    "    )\n",
    "\n",
    "    print(f\"[{lo}_{hi}] write: {os.path.basename(out_test)}  (cols={len(select_exprs)})\")\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00571c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, json, time, re\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "feat_cols = pd.read_csv('exp/v1/config/input_sets/top_imp.csv')['feature'].tolist()\n",
    "feat_cols = [*feat_cols, 'time_id']      # 或 feat_cols + ['time_id']\n",
    "\n",
    "def shard2memmap(fe_paths, feat_cols, prefix, storage_options=None):\n",
    "    \"\"\"\n",
    "    将若干 parquet 分片顺序拼接为一份 memmap: X(float32)\n",
    "    - 仅提取 feat_cols（缺失列用 NaN 补齐），列顺序以 feat_cols 为准\n",
    "    - 输出：{prefix}_X.float32.mmap 与 {prefix}.meta.json\n",
    "    - fe_paths: 已经挑选/过滤好的最终 test_{lo}_{hi}.parquet 列表（可含 az:// 或本地路径）\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) 规范化 + 数值排序\n",
    "    def _numeric_key(p: str):\n",
    "        b = os.path.basename(p)\n",
    "        stem = b[:-8]  # 去掉 .parquet\n",
    "        parts = stem.split(\"_\")\n",
    "        try:\n",
    "            lo, hi = int(parts[-2]), int(parts[-1])\n",
    "            return (lo, hi)\n",
    "        except Exception:\n",
    "            return (b, b)\n",
    "    fe_paths = sorted(fe_paths, key=_numeric_key)\n",
    "\n",
    "    # 1) 兜底：确保有文件、确保输出目录存在\n",
    "    if not fe_paths:\n",
    "        raise FileNotFoundError(\"fe_paths is empty.\")\n",
    "    os.makedirs(os.path.dirname(prefix), exist_ok=True)\n",
    "\n",
    "    # 2) 统计每个分片的行数（流式）\n",
    "    counts = []\n",
    "    for p in fe_paths:\n",
    "        lf = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "              if p.startswith(\"az://\") else pl.scan_parquet(p))\n",
    "        k = lf.select(pl.len()).collect(streaming=True).item()\n",
    "        counts.append(int(k))\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    print(f\"[memmap]: {len(fe_paths)} files, {n_rows} rows, {n_feat} features\")\n",
    "\n",
    "    # 3) 分配 memmap（本地）\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "\n",
    "    # 4) 逐分片流式写入\n",
    "    i = 0\n",
    "    for p, k in zip(fe_paths, counts):\n",
    "        lf = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "              if p.startswith(\"az://\") else pl.scan_parquet(p))\n",
    "        names = set(lf.collect_schema().names())\n",
    "        exprs = [\n",
    "            (pl.col(c).cast(pl.Float32).alias(c) if c in names\n",
    "             else pl.lit(None, dtype=pl.Float32).alias(c))\n",
    "            for c in feat_cols\n",
    "        ]\n",
    "        df = lf.select(exprs).collect(streaming=True)\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush()\n",
    "\n",
    "    # 5) 保存 meta（含来源文件）\n",
    "    meta = {\n",
    "        \"n_rows\": int(n_rows),\n",
    "        \"n_feat\": int(n_feat),\n",
    "        \"dtype\": \"float32\",\n",
    "        \"ts\": time.time(),\n",
    "        \"features\": list(feat_cols),\n",
    "        \"paths\": list(fe_paths),\n",
    "    }\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ---- 选取最终 test_{lo}_{hi}.parquet 文件 ----\n",
    "regex = re.compile(r\"^test_\\d+_\\d+\\.parquet$\")\n",
    "fe_all_np = fs.glob(f\"{FE_SHA_DIR_NP}/test_*_*.parquet\")\n",
    "fe_all_np = [p for p in fe_all_np if regex.match(os.path.basename(p))]\n",
    "fe_paths = [(\"az://\" + p) if not p.startswith(\"az://\") else p for p in fe_all_np]\n",
    "\n",
    "# ---- 生成测试 memmap ----\n",
    "test_X = shard2memmap(\n",
    "    fe_paths, feat_cols, prefix=f\"{MM_DIR}/test\", storage_options=storage_options\n",
    ")\n",
    "print(\"test shape:\", test_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 载入模型， 用memmap 批量处理\n",
    "\n",
    "MODEL_PATH = \"exp/v1/models/lgbm_tv_model.txt\"\n",
    "\n",
    "\n",
    "PRED_DIR = \"exp/v1/predictions\"\n",
    "os.makedirs(PRED_DIR, exist_ok=True)\n",
    "\n",
    "# 读取 memmap\n",
    "with open(f\"{MM_DIR}/test.meta.json\", \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "n_rows, n_feat = meta[\"n_rows\"], meta[\"n_feat\"]\n",
    "test_X = np.memmap(f\"{MM_DIR}/test_X.float32.mmap\", dtype=\"float32\", mode=\"r\",\n",
    "                   shape=(n_rows, n_feat))\n",
    "\n",
    "\n",
    "# 载入模型\n",
    "model = lgb.Booster(model_file=MODEL_PATH)\n",
    "\n",
    "\n",
    "\n",
    "BATCH = 50_000  # 按你机器内存调整\n",
    "preds = np.memmap(f\"{PRED_DIR}/test_pred.float32.mmap\", dtype=\"float32\", mode=\"w+\",\n",
    "                  shape=(n_rows,))\n",
    "for s in range(0, n_rows, BATCH):\n",
    "    e = min(s + BATCH, n_rows)\n",
    "    Xb = test_X[s:e]\n",
    "    # scikit/LightGBM/XGBoost 都支持 .predict(np.ndarray)\n",
    "    pb = model.predict(Xb)\n",
    "    preds[s:e] = pb.astype(\"float32\", copy=False)\n",
    "    del Xb, pb\n",
    "    gc.collect()\n",
    "preds.flush()\n",
    "print(\"inference done:\", preds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "regex = re.compile(r\"^test_\\d+_\\d+\\.parquet$\")\n",
    "fe_all_np = fs.glob(f\"{FE_SHA_DIR_NP}/test_*_*.parquet\")\n",
    "fe_all_np = [p for p in fe_all_np if regex.match(os.path.basename(p))]\n",
    "\n",
    "# 数值排序，确保 (lo,hi) 顺序一致\n",
    "def _key(p):\n",
    "    b = os.path.basename(p)[:-8]\n",
    "    a,b2 = b.split(\"_\")[-2:]\n",
    "    return (int(a), int(b2))\n",
    "fe_paths = [(\"az://\"+p) if not p.startswith(\"az://\") else p for p in sorted(fe_all_np, key=_key)]\n",
    "\n",
    "# 逐分片写 preds（带 KEYS）\n",
    "i = 0\n",
    "for p in fe_paths:\n",
    "    # 行数\n",
    "    k = pl.scan_parquet(p, storage_options=storage_options).select(pl.len()).collect(streaming=True).item()\n",
    "\n",
    "    # 取 KEYS\n",
    "    df_keys = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "                 .select(KEYS)\n",
    "                 .collect(streaming=True))\n",
    "\n",
    "    # 取对应预测切片\n",
    "    yhat = preds[i:i+k].copy()  # 小片复制到内存\n",
    "    df_pred = df_keys.with_columns(pl.Series(\"y_pred\", yhat.astype(\"float32\")))\n",
    "\n",
    "    # 写出同名窗口的预测文件\n",
    "    base = os.path.basename(p)[:-8]   # test_lo_hi\n",
    "    outp = f\"{PRED_DIR}/{base}_preds.parquet\"\n",
    "    if fs.exists(outp): fs.rm(outp)\n",
    "    df_pred.write_parquet(outp, compression=\"zstd\")\n",
    "\n",
    "    i += k\n",
    "    del df_keys, df_pred, yhat; gc.collect()\n",
    "\n",
    "print(\"pred shards written to:\", PRED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ad4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总全部预测分片\n",
    "preds_all = pl.scan_parquet(f\"{PRED_DIR}/test_*_preds.parquet\")\n",
    "\n",
    "# 选择评估日期范围（可根据你的 test 窗口，比如 1530..1698）\n",
    "DATE_LO, DATE_HI = 1530, 1698\n",
    "labels = (pl.scan_parquet(TEST_BASE_PATH, storage_options=storage_options)\n",
    "            .filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "            .select([*KEYS,\n",
    "                     pl.col(TARGET).cast(pl.Float32).alias(\"y_true\"),\n",
    "                     pl.col(WEIGHT).cast(pl.Float32).alias(\"w\")]))\n",
    "\n",
    "# join + 算指标\n",
    "df = preds_all.join(labels, on=KEYS, how=\"inner\").collect(streaming=True)\n",
    "\n",
    "# 若某些天没有权重，兜底为 1.0\n",
    "if \"w\" not in df.columns:\n",
    "    df = df.with_columns(pl.lit(1.0).alias(\"w\"))\n",
    "\n",
    "y = df[\"y_true\"].to_numpy()\n",
    "p = df[\"y_pred\"].to_numpy()\n",
    "w = df[\"w\"].to_numpy()\n",
    "\n",
    "def weighted_r2_zero_mean(y_true, y_pred, weight):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "    weight = np.asarray(weight, dtype=np.float64).ravel()\n",
    "    num = np.sum(weight * (y_true - y_pred)**2)\n",
    "    den = np.sum(weight * (y_true**2))\n",
    "    return 0.0 if den <= 0 else 1.0 - (num/den)\n",
    "\n",
    "#输出结果\n",
    "wr2 = weighted_r2_zero_mean(y, p, w)\n",
    "print(f\"Final weighted R^2 (zero-mean): {wr2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d98985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (JS .venv)",
   "language": "python",
   "name": "js-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
