{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6e401b",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c935a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Py versions: polars 1.21.0 lightgbm 4.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 环境与依赖\n",
    "\n",
    "# 基础包\n",
    "import tempfile\n",
    "\n",
    "import os, gc, glob, json, yaml, time\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "import lightgbm as lgb\n",
    "from dataclasses import dataclass\n",
    "import pyarrow.parquet as pq\n",
    "from typing import Sequence, Optional, Union, List, Tuple, Iterable, Mapping\n",
    "\n",
    "# Azure & 文件系统\n",
    "import fsspec\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"Py versions:\",\n",
    "      \"polars\", pl.__version__,\n",
    "      \"lightgbm\", lgb.__version__)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # 默认会加载当前目录下的 .env 文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e1ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw exists?  True\n"
     ]
    }
   ],
   "source": [
    "# 连接云空间\n",
    "\n",
    "ACC = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "KEY = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "\n",
    "if not ACC or not KEY:\n",
    "    raise RuntimeError(\"Azure credentials not found. Please set them in .env\")\n",
    "\n",
    "storage_options = {\"account_name\": ACC, \"account_key\": KEY}\n",
    "fs = fsspec.filesystem(\"az\", **storage_options)\n",
    "\n",
    "# 小函数：构造路径\n",
    "cfg = yaml.safe_load(open(\"exp/v1/config/data.yaml\"))\n",
    "CONTAINER = cfg[\"blob\"][\"container\"]\n",
    "PREFIX    = cfg[\"blob\"][\"prefix\"].strip(\"/\")\n",
    "\n",
    "def BLOB(*parts):      # 给 polars 用（需要 az://）\n",
    "    return \"az://\" + \"/\".join([CONTAINER, PREFIX, *parts]).replace(\"//\",\"/\")\n",
    "def NOPRO(*parts):     # 给 fs.glob/fs.exists 用（无协议）\n",
    "    return \"/\".join([CONTAINER, PREFIX, *parts]).replace(\"//\",\"/\")\n",
    "def LOC (*parts):      # 本地路径\n",
    "    return str(Path(cfg[\"local\"][\"root\"], *parts))\n",
    "\n",
    "# 自检：看看 raw 是否存在\n",
    "print(\"raw exists? \", fs.exists(NOPRO(cfg[\"paths\"][\"raw\"])))\n",
    "\n",
    "# 全局路径变量\n",
    "RAW_SHA_DIR_NP = NOPRO(cfg[\"paths\"][\"raw_shards\"])       # exp/v1/raw_shards\n",
    "FE_SHA_DIR_B   = BLOB(cfg[\"paths\"][\"fe_shards\"])         # az://.../exp/v1/fe_shards\n",
    "FE_SHA_DIR_NP  = NOPRO(cfg[\"paths\"][\"fe_shards\"])        # jackson/js_exp/exp/v1/fe_shards\n",
    "\n",
    "\n",
    "fs.mkdirs(RAW_SHA_DIR_NP, exist_ok=True)\n",
    "fs.mkdirs(FE_SHA_DIR_NP, exist_ok=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7fdc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义键列/范围/本地目录\n",
    "\n",
    "KEYS   = cfg[\"keys\"]\n",
    "TARGET = cfg[\"target\"]\n",
    "WEIGHT = cfg[\"weight\"]\n",
    "DATE_LO, DATE_HI = cfg[\"date_lo\"], cfg[\"date_hi\"]\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "\n",
    "# 创建本地高速区目录\n",
    "Path(LOC(cfg[\"paths\"][\"mm_local\"])).mkdir(parents=True, exist_ok=True)\n",
    "Path(LOC(cfg[\"paths\"][\"tmp_local\"])).mkdir(parents=True, exist_ok=True)\n",
    "Path(LOC(cfg[\"paths\"][\"cache\"])).mkdir(parents=True, exist_ok=True)   \n",
    "\n",
    "MM_DIR  = LOC(cfg[\"paths\"][\"mm_local\"])                # /mnt/data/mm/js_exp\n",
    "\n",
    "TMP_ROOT = LOC(cfg[\"paths\"][\"tmp_local\"])               # /mnt/data/tmp/js_exp\n",
    "BASE_DIR = BLOB(cfg['paths']['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ff2fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 92)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_31</th><th>feature_32</th><th>&hellip;</th><th>feature_51</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_0</th><th>responder_1</th><th>responder_2</th><th>responder_3</th><th>responder_4</th><th>responder_5</th><th>responder_6</th><th>responder_7</th><th>responder_8</th></tr><tr><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i8</td><td>i8</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>680</td><td>0</td><td>0</td><td>2.29816</td><td>0.851814</td><td>1.197591</td><td>0.219422</td><td>0.411698</td><td>2.057359</td><td>-0.542597</td><td>-3.4331</td><td>-1.090165</td><td>0.151888</td><td>11</td><td>7</td><td>76</td><td>-0.97142</td><td>0.670215</td><td>-0.502896</td><td>null</td><td>-0.070096</td><td>null</td><td>-1.308236</td><td>-2.120128</td><td>1.747068</td><td>-0.219661</td><td>0.791602</td><td>0.114922</td><td>-0.311672</td><td>0.156548</td><td>1.476346</td><td>1.301341</td><td>1.235173</td><td>-0.259882</td><td>-0.30125</td><td>-0.264237</td><td>null</td><td>&hellip;</td><td>0.322135</td><td>null</td><td>null</td><td>-2.273168</td><td>null</td><td>-1.627507</td><td>1.155271</td><td>null</td><td>-3.847984</td><td>-2.230864</td><td>-0.171579</td><td>-0.273791</td><td>-0.301876</td><td>-0.432933</td><td>-1.215778</td><td>-1.670469</td><td>-0.637963</td><td>0.803874</td><td>-0.21269</td><td>-0.764702</td><td>0.435278</td><td>-0.619145</td><td>null</td><td>null</td><td>-0.021034</td><td>-0.045094</td><td>-0.178144</td><td>-0.1951</td><td>-0.304665</td><td>0.164485</td><td>-0.205231</td><td>0.191064</td><td>-1.413209</td><td>0.375675</td><td>0.929775</td><td>-1.574939</td><td>1.101371</td></tr><tr><td>680</td><td>0</td><td>1</td><td>3.928745</td><td>0.534441</td><td>1.07974</td><td>0.038748</td><td>0.275343</td><td>2.135057</td><td>-0.541966</td><td>-2.774344</td><td>-1.048089</td><td>0.163768</td><td>11</td><td>7</td><td>76</td><td>-0.873847</td><td>1.794426</td><td>-0.226819</td><td>null</td><td>-0.328627</td><td>null</td><td>-0.870575</td><td>-1.204292</td><td>0.935869</td><td>-0.064365</td><td>1.273109</td><td>0.752816</td><td>-0.062281</td><td>0.687036</td><td>0.546603</td><td>1.229373</td><td>1.535235</td><td>-0.483758</td><td>-0.431013</td><td>-0.058963</td><td>null</td><td>&hellip;</td><td>0.505224</td><td>null</td><td>null</td><td>-1.074216</td><td>null</td><td>-1.909605</td><td>1.566016</td><td>null</td><td>-1.044804</td><td>-0.807186</td><td>-0.171579</td><td>-0.466983</td><td>-0.250124</td><td>-0.342297</td><td>-1.896279</td><td>-2.157645</td><td>-0.698755</td><td>1.743311</td><td>-0.069648</td><td>-0.933445</td><td>1.653637</td><td>-0.348816</td><td>null</td><td>null</td><td>-0.154413</td><td>-0.301091</td><td>-0.266495</td><td>-0.470271</td><td>0.089769</td><td>0.011395</td><td>0.092348</td><td>0.473781</td><td>0.397024</td><td>0.777026</td><td>0.826995</td><td>0.569681</td><td>1.986971</td></tr><tr><td>680</td><td>0</td><td>2</td><td>1.340433</td><td>-0.227643</td><td>0.764146</td><td>-0.243349</td><td>0.247027</td><td>2.347248</td><td>-0.478477</td><td>-2.660244</td><td>-1.261613</td><td>0.234425</td><td>81</td><td>2</td><td>59</td><td>-0.952889</td><td>-0.04806</td><td>-0.791763</td><td>null</td><td>-0.140953</td><td>null</td><td>-1.691419</td><td>-2.242023</td><td>-0.459649</td><td>-0.241993</td><td>-0.47108</td><td>-1.056603</td><td>-0.387841</td><td>-0.408962</td><td>0.359514</td><td>0.707161</td><td>0.669771</td><td>-0.641928</td><td>-0.645097</td><td>-0.362472</td><td>null</td><td>&hellip;</td><td>2.115924</td><td>null</td><td>null</td><td>0.095438</td><td>null</td><td>-0.664823</td><td>2.016616</td><td>null</td><td>2.085358</td><td>0.690842</td><td>-0.171579</td><td>0.022096</td><td>-0.018111</td><td>-0.015769</td><td>-2.270972</td><td>-1.826189</td><td>-0.908704</td><td>-0.051331</td><td>-0.658539</td><td>-1.011692</td><td>-0.008993</td><td>-0.363811</td><td>null</td><td>null</td><td>1.677642</td><td>1.705228</td><td>0.198109</td><td>0.152837</td><td>0.218281</td><td>0.060373</td><td>-0.164715</td><td>-0.132612</td><td>0.543831</td><td>-0.123519</td><td>-0.296969</td><td>0.547286</td><td>-0.049303</td></tr><tr><td>680</td><td>0</td><td>3</td><td>1.695526</td><td>0.267686</td><td>1.193612</td><td>-0.388798</td><td>0.030673</td><td>2.175273</td><td>-0.408371</td><td>-1.859344</td><td>-0.771972</td><td>0.104885</td><td>4</td><td>3</td><td>11</td><td>-1.005184</td><td>0.546772</td><td>-0.587481</td><td>null</td><td>-0.628245</td><td>null</td><td>-1.243775</td><td>-2.907238</td><td>-0.098398</td><td>0.004987</td><td>-0.320296</td><td>-1.193256</td><td>1.260476</td><td>1.621998</td><td>-0.255902</td><td>-1.644356</td><td>-1.292619</td><td>-0.541355</td><td>-0.703121</td><td>0.00681</td><td>null</td><td>&hellip;</td><td>-0.442253</td><td>null</td><td>null</td><td>-1.021894</td><td>null</td><td>-3.088617</td><td>1.440495</td><td>null</td><td>0.19264</td><td>0.270458</td><td>-0.171579</td><td>-0.387987</td><td>-0.30065</td><td>-0.169365</td><td>-1.324387</td><td>-2.20877</td><td>-0.975838</td><td>0.605086</td><td>-0.303872</td><td>-1.071495</td><td>0.445026</td><td>-0.465118</td><td>null</td><td>null</td><td>3.820025</td><td>4.335268</td><td>9.818627</td><td>11.179185</td><td>-0.012298</td><td>1.047678</td><td>-0.696032</td><td>0.960062</td><td>2.32889</td><td>0.718955</td><td>2.047506</td><td>3.691308</td><td>3.031337</td></tr><tr><td>680</td><td>0</td><td>5</td><td>2.700766</td><td>0.952372</td><td>0.861269</td><td>-0.375405</td><td>0.259099</td><td>2.497325</td><td>-0.618828</td><td>-2.754378</td><td>-0.479992</td><td>0.108627</td><td>2</td><td>10</td><td>171</td><td>-0.892601</td><td>0.207765</td><td>-0.420839</td><td>null</td><td>-0.522783</td><td>null</td><td>-1.711776</td><td>-2.078692</td><td>0.163208</td><td>0.023584</td><td>0.598875</td><td>-0.497226</td><td>0.231989</td><td>1.630302</td><td>-1.233744</td><td>-0.434498</td><td>0.37159</td><td>-0.586595</td><td>-0.512672</td><td>0.017691</td><td>null</td><td>&hellip;</td><td>1.003178</td><td>null</td><td>null</td><td>-0.92965</td><td>null</td><td>-1.621122</td><td>1.520507</td><td>null</td><td>-0.532552</td><td>-0.482004</td><td>-0.171579</td><td>-0.164008</td><td>-0.194068</td><td>-0.257875</td><td>-1.623473</td><td>-1.959616</td><td>-0.748592</td><td>0.022282</td><td>-0.561959</td><td>-0.995984</td><td>0.385183</td><td>-0.312552</td><td>null</td><td>null</td><td>-0.552556</td><td>-0.460608</td><td>-0.700707</td><td>-0.770234</td><td>-0.229585</td><td>-0.240741</td><td>-0.887929</td><td>-0.061485</td><td>0.691569</td><td>1.016049</td><td>0.103898</td><td>0.814866</td><td>2.07328</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 92)\n",
       "┌─────────┬─────────┬───────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ date_id ┆ time_id ┆ symbol_id ┆ weight   ┆ … ┆ responder_ ┆ responder_ ┆ responder_ ┆ responder_ │\n",
       "│ ---     ┆ ---     ┆ ---       ┆ ---      ┆   ┆ 5          ┆ 6          ┆ 7          ┆ 8          │\n",
       "│ i16     ┆ i16     ┆ i8        ┆ f32      ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆         ┆           ┆          ┆   ┆ f32        ┆ f32        ┆ f32        ┆ f32        │\n",
       "╞═════════╪═════════╪═══════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 680     ┆ 0       ┆ 0         ┆ 2.29816  ┆ … ┆ 0.375675   ┆ 0.929775   ┆ -1.574939  ┆ 1.101371   │\n",
       "│ 680     ┆ 0       ┆ 1         ┆ 3.928745 ┆ … ┆ 0.777026   ┆ 0.826995   ┆ 0.569681   ┆ 1.986971   │\n",
       "│ 680     ┆ 0       ┆ 2         ┆ 1.340433 ┆ … ┆ -0.123519  ┆ -0.296969  ┆ 0.547286   ┆ -0.049303  │\n",
       "│ 680     ┆ 0       ┆ 3         ┆ 1.695526 ┆ … ┆ 0.718955   ┆ 2.047506   ┆ 3.691308   ┆ 3.031337   │\n",
       "│ 680     ┆ 0       ┆ 5         ┆ 2.700766 ┆ … ┆ 1.016049   ┆ 0.103898   ┆ 0.814866   ┆ 2.07328    │\n",
       "└─────────┴─────────┴───────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据读取与快速EDA\n",
    "pattern = f\"{BLOB(cfg['paths']['raw'])}/train.parquet/partition_id=*[4-8]/*.parquet\"\n",
    "lf = pl.scan_parquet(\n",
    "    pattern,\n",
    "    storage_options={\"account_name\": ACC, \"account_key\": KEY},\n",
    ")\n",
    "\n",
    "\n",
    "test_pattern  = f\"{BLOB(cfg['paths']['raw'])}/train.parquet/partition_id=*[8-9]/*.parquet\" # 这里partition 8 仅用于测试部分的数据处理和历史特征工程\n",
    "\n",
    "test_data = pl.scan_parquet(\n",
    "    test_pattern,\n",
    "    storage_options={\"account_name\": ACC, \"account_key\": KEY},\n",
    ")\n",
    "\n",
    "lf.limit().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10bc10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_START_DATE_ID = 1429 # 真正的测试集开始于1530， 这里多取100天用于历史特征计算肯定足够了\n",
    "lf_test = test_data.filter(pl.col(\"date_id\") >= TEST_DATA_START_DATE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10ad82",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA：每个交易日的symbol覆盖情况,是否覆盖全程\n",
    "lf_s_d = lf.select(['date_id', 'symbol_id'])\n",
    "\n",
    "per_date = (\n",
    "    lf_s_d.group_by(\"date_id\")\n",
    "      .agg(pl.col(\"symbol_id\").n_unique().alias(\"n_symbols\"))\n",
    "      .sort(\"date_id\")\n",
    ")\n",
    "\n",
    "max_n = per_date.select(pl.max(\"n_symbols\")).collect().item()\n",
    "summary = per_date.with_columns([\n",
    "    pl.lit(max_n).alias(\"max_n\"),\n",
    "    (pl.col(\"n_symbols\") == max_n).alias(\"is_full_universe\")\n",
    "])\n",
    "\n",
    "dates_missing = summary.filter(pl.col(\"is_full_universe\") == False).select(\"date_id\")\n",
    "# summary.collect(); dates_missing.collect()\n",
    "\n",
    "dates_missing.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a362c607",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_sigma_clip_polars_plan(\n",
    "    source: Union[pl.LazyFrame, List[str]],\n",
    "    features: Sequence[str],\n",
    "    group_cols: Sequence[str] = (\"symbol_id\",),\n",
    "    sort_cols: Sequence[str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    window: int = 7,\n",
    "    k: float = 3.0,\n",
    "    ddof: int = 0,\n",
    "    min_valid: int = 2,\n",
    "    batch: int = 10,\n",
    "    cast_float32: bool = True\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"返回执行滚动 kσ 裁剪的 LazyFrame（不写盘）。\"\"\"\n",
    "    # 0) 输入\n",
    "    lf = source if isinstance(source, pl.LazyFrame) else pl.scan_parquet(source)\n",
    "\n",
    "    # 1) 仅选必要列\n",
    "    need_cols = list(dict.fromkeys([*group_cols, *sort_cols, *features]))\n",
    "    schema = lf.collect_schema()\n",
    "    missing = [c for c in need_cols if c not in schema]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns: {missing}\")\n",
    "    lf = lf.select(need_cols)\n",
    "\n",
    "\n",
    "    # 3) 分批构建裁剪表达式\n",
    "    for i in range(0, len(features), batch):\n",
    "        part = features[i:i+batch]\n",
    "        exprs = []\n",
    "        for c in part:\n",
    "            cnt = (\n",
    "                pl.col(c).is_not_null().cast(pl.Int32)\n",
    "                        .rolling_sum(window_size=window, min_samples=1)\n",
    "                        .over(group_cols)\n",
    "            )\n",
    "            mu  = pl.col(c).rolling_mean(window_size=window, min_samples=1).over(group_cols)\n",
    "            sd  = (pl.col(c).rolling_std(window_size=window, ddof=ddof, min_samples=1)\n",
    "                        .over(group_cols).fill_null(0.0))\n",
    "            lo, hi = (mu - k*sd), (mu + k*sd)\n",
    "\n",
    "            base = pl.col(c).cast(pl.Float32) if cast_float32 else pl.col(c)\n",
    "            if cast_float32:\n",
    "                lo, hi = lo.cast(pl.Float32), hi.cast(pl.Float32)\n",
    "\n",
    "            exprs.append(\n",
    "                pl.when(cnt >= min_valid).then(base.clip(lo, hi)).otherwise(base).alias(c)\n",
    "            )\n",
    "        lf = lf.with_columns(exprs)\n",
    "\n",
    "    return lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clip - train\n",
    "OUT_DIR = LOC(cfg[\"paths\"][\"cache\"])\n",
    "FEATURES_ALL = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "RESP_ALL = [f\"responder_{i}\" for i in range(9)]\n",
    "\n",
    "\n",
    "lf_clip = rolling_sigma_clip_polars_plan(\n",
    "    source=lf,\n",
    "    features=FEATURES_ALL,\n",
    "    group_cols=(\"symbol_id\",),\n",
    "    sort_cols=(\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    window=40, k=3.0, ddof=0, min_valid=8,\n",
    "    batch=5, cast_float32=True\n",
    ")\n",
    "\n",
    "lf_clip = lf_clip.sort(KEYS)\n",
    "\n",
    "lf_clip.collect(streaming=True).write_parquet(f\"{OUT_DIR}/clip.parquet\", compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179733d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip - test\n",
    "# 1. Clip - train\n",
    "OUT_DIR = LOC(cfg[\"paths\"][\"cache\"])\n",
    "FEATURES_ALL = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "RESP_ALL = [f\"responder_{i}\" for i in range(9)]\n",
    "\n",
    "\n",
    "lf_test_clip = rolling_sigma_clip_polars_plan(\n",
    "    source=lf_test,\n",
    "    features=FEATURES_ALL,\n",
    "    group_cols=(\"symbol_id\",),\n",
    "    sort_cols=(\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    window=40, k=3.0, ddof=0, min_valid=8,\n",
    "    batch=5, cast_float32=True\n",
    ")\n",
    "\n",
    "lf_test_clip = lf_test_clip.sort(KEYS)\n",
    "\n",
    "lf_test_clip.collect(streaming=True).write_parquet(f\"{OUT_DIR}/test_clip.parquet\", compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39206a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_impute_polars_no_na(\n",
    "    lf: pl.LazyFrame,\n",
    "    features: Optional[Sequence[str]] = None,            # 默认为 schema 中的 feature_*\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    open_ticks: Tuple[int,int] = (0, 5),                 # 开盘窗口 [t0, t1]\n",
    "    carry_days: int = 5,                                  # 开盘跨日承接 TTL(天)\n",
    "    intraday_limit: Optional[int] = None,                 # 日内 ffill 步数上限(None=不限)\n",
    "    crossday_same_tick_limit: Optional[int] = 2,          # 同一 time_id 跨日回退(+TTL)\n",
    "    train_end: Optional[int] = None,                      # 训练期截止 date_id（None=全局）\n",
    "    fallback: Optional[str] = \"mean\",                     # 'median'/'mean'/None（仅开盘仍缺时用）\n",
    "    ensure_sorted: bool = False,                          # 如上游未保证顺序，可在函数内排序\n",
    "    global_means: Optional[Mapping[str, float]] = None,   # 可选：外部注入全局均值\n",
    "    int_features: Optional[Sequence[str]] = None          # 可选：最终回铸为 Int 的特征名\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    因果填补（不写 *_na 列）：\n",
    "      1) 开盘窗口内：跨日 ffill(+TTL)，仍缺用训练期统计兜底（均值/中位数）\n",
    "      2) 日内：按 (symbol,date) ffill（可限步数）\n",
    "      3) 可选：同一 time_id 跨日回退(+TTL)\n",
    "      4) 兜底：per-symbol 均值 → 全局均值\n",
    "    输出列固定为 [keys + features]，类型统一为 Float32（可通过 int_features 回铸整数列）。\n",
    "    \"\"\"\n",
    "    gcol, dcol, tcol = keys\n",
    "    f32 = pl.Float32\n",
    "\n",
    "    # 0) 选列 + 类型统一 +（可选）排序\n",
    "    if features is None:\n",
    "        schema = lf.collect_schema()\n",
    "        features = sorted([c for c in schema if c.startswith(\"feature_\")])\n",
    "    need_cols = list(dict.fromkeys([gcol, dcol, tcol, *features]))\n",
    "    lf = lf.select(need_cols).with_columns([pl.col(c).cast(f32) for c in features])\n",
    "    if ensure_sorted:\n",
    "        lf = lf.sort([gcol, dcol, tcol])\n",
    "\n",
    "    # 训练期源（用于统计）\n",
    "    SRC_train = lf if train_end is None else lf.filter(pl.col(dcol) <= train_end)\n",
    "\n",
    "    # 1) 开盘子集\n",
    "    t0, t1 = open_ticks\n",
    "    lf_open = (\n",
    "        lf.select([gcol, dcol, tcol, *features])\n",
    "        .filter(pl.col(tcol).is_between(t0, t1))\n",
    "    )\n",
    "\n",
    "    # 2) 开盘：跨日承接 + TTL\n",
    "    open_exprs = []\n",
    "    for c in features:\n",
    "        last_non_null_date = (\n",
    "            pl.when(pl.col(c).is_not_null()).then(pl.col(dcol)).otherwise(None)\n",
    "            .forward_fill().over(gcol)\n",
    "        )\n",
    "        cand = pl.col(c).forward_fill().over(gcol)\n",
    "        gap  = (pl.col(dcol) - last_non_null_date).cast(pl.Int32)\n",
    "        open_exprs.append(\n",
    "            pl.when(pl.col(c).is_null() & cand.is_not_null()\n",
    "                    & last_non_null_date.is_not_null() & (gap <= carry_days))\n",
    "            .then(cand).otherwise(pl.col(c)).alias(c)\n",
    "        )\n",
    "    lf_open_ff = lf_open.with_columns(open_exprs)\n",
    "\n",
    "    # 3) 开盘仍缺 → 训练期统计兜底（聚合产物统一 Float32）\n",
    "    if fallback in (\"median\", \"mean\"):\n",
    "        aggs = [\n",
    "            (pl.col(c).median().cast(f32).alias(c) if fallback == \"median\"\n",
    "            else pl.col(c).mean().cast(f32).alias(c))\n",
    "            for c in features\n",
    "        ]\n",
    "        stats = SRC_train.select(aggs).collect(streaming=True).to_dicts()[0]\n",
    "        lf_open_filled = lf_open_ff.with_columns([\n",
    "            pl.when(pl.col(c).is_null()).then(pl.lit(stats[c]).cast(f32))\n",
    "            .otherwise(pl.col(c)).alias(c)\n",
    "            for c in features\n",
    "        ])\n",
    "    else:\n",
    "        lf_open_filled = lf_open_ff\n",
    "\n",
    "    # 4) 将开盘结果写回全量\n",
    "    suf = \"__open\"\n",
    "    lf_after_open = (\n",
    "        lf.join(lf_open_filled.select([gcol, dcol, tcol, *features]),\n",
    "                on=[gcol, dcol, tcol], how=\"left\", suffix=suf)\n",
    "        .with_columns([\n",
    "            pl.coalesce([pl.col(f\"{c}{suf}\"), pl.col(c)]).cast(f32).alias(c)\n",
    "            for c in features\n",
    "        ])\n",
    "        .drop([f\"{c}{suf}\" for c in features])\n",
    "    )\n",
    "\n",
    "    # 5) 日内 ffill\n",
    "    if intraday_limit is None:\n",
    "        lf_intra = lf_after_open.with_columns([\n",
    "            pl.col(c).forward_fill().over([gcol, dcol]).alias(c)\n",
    "            for c in features\n",
    "        ])\n",
    "    else:\n",
    "        intra_exprs = []\n",
    "        for c in features:\n",
    "            cand = pl.col(c).forward_fill().over([gcol, dcol])\n",
    "            last_non_null_time = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(tcol)).otherwise(None)\n",
    "                .forward_fill().over([gcol, dcol])\n",
    "            )\n",
    "            step_gap = (pl.col(tcol) - last_non_null_time).cast(pl.Int32)\n",
    "            intra_exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & cand.is_not_null()\n",
    "                        & last_non_null_time.is_not_null()\n",
    "                        & (step_gap <= intraday_limit))\n",
    "                .then(cand).otherwise(pl.col(c)).alias(c)\n",
    "            )\n",
    "        lf_intra = lf_after_open.with_columns(intra_exprs)\n",
    "\n",
    "    # 6) 同一 time_id 跨日回退 + TTL\n",
    "    lf_after_same = lf_intra\n",
    "    if crossday_same_tick_limit is not None:\n",
    "        exprs = []\n",
    "        for c in features:\n",
    "            last_non_null_date_same = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(dcol)).otherwise(None)\n",
    "                .forward_fill().over([gcol, tcol])\n",
    "            )\n",
    "            cand_same = pl.col(c).forward_fill().over([gcol, tcol])\n",
    "            gap2 = (pl.col(dcol) - last_non_null_date_same).cast(pl.Int32)\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & cand_same.is_not_null()\n",
    "                        & last_non_null_date_same.is_not_null()\n",
    "                        & (gap2 <= crossday_same_tick_limit))\n",
    "                .then(cand_same).otherwise(pl.col(c)).alias(c)\n",
    "            )\n",
    "        lf_after_same = lf_intra.with_columns(exprs)\n",
    "\n",
    "    # 7) 兜底①：per-symbol 均值（Float32）\n",
    "    sym_stats = (\n",
    "        SRC_train.select([gcol] + list(features))\n",
    "                .group_by(gcol)\n",
    "                .agg([pl.col(c).mean().cast(f32).alias(c) for c in features])\n",
    "    )\n",
    "    lf_lvl1 = (\n",
    "        lf_after_same.join(sym_stats, on=[gcol], how=\"left\", suffix=\"__sym\")\n",
    "                    .with_columns([\n",
    "                        pl.coalesce([pl.col(c), pl.col(f\"{c}__sym\")]).cast(f32).alias(c)\n",
    "                        for c in features\n",
    "                    ])\n",
    "                    .drop([f\"{c}__sym\" for c in features])\n",
    "    )\n",
    "\n",
    "    # 8) 兜底②：全局均值（无条件 coalesce；支持外部注入）\n",
    "    if global_means is None:\n",
    "        glob_stats = (\n",
    "            SRC_train.select([pl.col(c).mean().cast(f32).alias(c) for c in features])\n",
    "                    .collect(streaming=True).to_dicts()[0]\n",
    "        )\n",
    "    else:\n",
    "        glob_stats = {c: float(global_means[c]) for c in features if c in global_means}\n",
    "    lf_final = lf_lvl1.with_columns([\n",
    "        pl.coalesce([pl.col(c), pl.lit(glob_stats[c]).cast(f32)]).alias(c)\n",
    "        for c in features\n",
    "    ])\n",
    "\n",
    "    # 9) 固定输出列顺序；可选把部分列回铸为整型\n",
    "    lf_final = lf_final.select([gcol, dcol, tcol, *features])\n",
    "    if int_features:\n",
    "        lf_final = lf_final.with_columns([\n",
    "            pl.col(c).round().cast(pl.Int32).alias(c) for c in int_features\n",
    "        ])\n",
    "\n",
    "    return lf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "# I/O\n",
    "SRC_DIR = LOC(cfg[\"paths\"][\"cache\"])\n",
    "SRC = f\"{SRC_DIR}/clip.parquet\"\n",
    "\n",
    "OUT_DIR = LOC(cfg[\"paths\"][\"cache\"])\n",
    "OUT = f\"{OUT_DIR}/impute.parquet\"\n",
    "KEYS = [\"symbol_id\", \"date_id\", \"time_id\"]\n",
    "BATCH = 5                      # 内存吃紧可降到 2~3\n",
    "ENSURE_SORTED = False          # 如需保证批内顺序，可在 lf_b 后加 .sort(KEYS)\n",
    "EXPECTED = [*KEYS, *FEATURES_ALL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2 Impute\n",
    "\n",
    "# 0) 全局均值（Float32 + 兜底）\n",
    "GLOBAL_MEAN = (\n",
    "    pl.scan_parquet(str(SRC))\n",
    "      .select([pl.col(c).mean().fill_null(0.0).cast(pl.Float32).alias(c)\n",
    "               for c in FEATURES_ALL])\n",
    "      .collect(streaming=True)\n",
    "      .to_dicts()[0]\n",
    ")\n",
    "\n",
    "# 1) symbol 列表\n",
    "symbols = (\n",
    "    pl.scan_parquet(str(SRC))\n",
    "      .select(pl.col(\"symbol_id\").unique())\n",
    "      .collect(streaming=True)[\"symbol_id\"]\n",
    "      .to_list()\n",
    ")\n",
    "\n",
    "writer = None\n",
    "try:\n",
    "    for i in range(0, len(symbols), BATCH):\n",
    "        ids = symbols[i:i+BATCH]\n",
    "\n",
    "        # 批数据（列裁剪 + 固定顺序）\n",
    "        lf_b = (\n",
    "            pl.scan_parquet(str(SRC))\n",
    "              .filter(pl.col(\"symbol_id\").is_in(ids))\n",
    "              .select(EXPECTED)\n",
    "        )\n",
    "        # 如需保证批内顺序（Stage1 未排序时），打开下一行：\n",
    "        # if ENSURE_SORTED:\n",
    "        #     lf_b = lf_b.sort(KEYS)\n",
    "\n",
    "        # 因果填补（延用你的函数）\n",
    "        lf_imp_b = causal_impute_polars_no_na(\n",
    "            lf=lf_b,\n",
    "            features=FEATURES_ALL,\n",
    "            keys=tuple(KEYS),\n",
    "            open_ticks=(0, 10),\n",
    "            carry_days=5,\n",
    "            intraday_limit= 50,\n",
    "            crossday_same_tick_limit=5,\n",
    "            train_end=None,\n",
    "            fallback=\"mean\",\n",
    "            ensure_sorted=ENSURE_SORTED,\n",
    "            global_means=GLOBAL_MEAN,\n",
    "            int_features=None\n",
    "        )\n",
    "\n",
    "        # 再次固定列顺序与 dtype\n",
    "        lf_imp_b = lf_imp_b.select(EXPECTED).with_columns(\n",
    "            [pl.col(c).cast(pl.Float32) for c in FEATURES_ALL]\n",
    "        )\n",
    "\n",
    "        # 物化当前批\n",
    "        df_b = lf_imp_b.collect(streaming=True)\n",
    "        tbl  = df_b.to_arrow()\n",
    "\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(\n",
    "                str(OUT), tbl.schema,\n",
    "                compression=\"zstd\",\n",
    "                use_dictionary=True,\n",
    "                write_statistics=True\n",
    "            )\n",
    "\n",
    "        writer.write_table(tbl, row_group_size=300_000)  # 20~50万行更省内存\n",
    "        del df_b, tbl\n",
    "        gc.collect()\n",
    "finally:\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "# 3. join responders → 单文件增量写 --------------------\n",
    "# ==== 路径 ====\n",
    "SRC_IMP_DIR = LOC(cfg[\"paths\"][\"cache\"])                  # \"jackson/js_exp/cache\"\n",
    "SRC_IMP = f\"{SRC_IMP_DIR}/impute.parquet\"      # 本地路径（Step2 输出）\n",
    "CLEAN_DIR_NP = NOPRO(cfg[\"paths\"][\"clean\"])                  # \"jackson/js_exp/clean\"\n",
    "OUT_NP = f\"{CLEAN_DIR_NP}/final_clean.parquet\"               # 无协议路径（给 fs/arrow 用\n",
    "\n",
    "\n",
    "# 右表：lf.responders\n",
    "\n",
    "# ==== 参数 ====\n",
    "EXPECTED = [*KEYS, WEIGHT, *FEATURES_ALL, *RESP_ALL]\n",
    "BATCH = 2  # 每批若干 symbol\n",
    "\n",
    "# ==== 准备输出目录 & 如果已存在旧文件，可选择删除 ====\n",
    "fs.mkdirs(CLEAN_DIR_NP, exist_ok=True)\n",
    "if fs.exists(OUT_NP):\n",
    "    fs.rm(OUT_NP)   # 或者注释掉：有则报错，避免误覆盖\n",
    "\n",
    "# ==== 键类型对齐 ====\n",
    "imp_schema = pl.scan_parquet(str(SRC_IMP)).collect_schema()\n",
    "key_casts = {k: imp_schema[k] for k in KEYS}\n",
    "\n",
    "# 全量 symbol 列表\n",
    "symbols = (pl.scan_parquet(str(SRC_IMP))\n",
    "             .select(pl.col(\"symbol_id\").unique())\n",
    "             .collect(streaming=True)[\"symbol_id\"].to_list())\n",
    "\n",
    "\n",
    "# ==== 打开 Blob 文件句柄 + 分批写入行组 ====\n",
    "writer = None\n",
    "with fs.open(OUT_NP, \"wb\") as fout:\n",
    "    try:\n",
    "        for i in range(0, len(symbols), BATCH):\n",
    "            ids = symbols[i:i+BATCH]\n",
    "\n",
    "            # 左表：impute 后的特征（本地）\n",
    "            lf_left = (pl.scan_parquet(str(SRC_IMP))\n",
    "                         .filter(pl.col(\"symbol_id\").is_in(ids))\n",
    "                         .select([*KEYS, *FEATURES_ALL]))\n",
    "\n",
    "            # 右表：responders（Blob，记得 storage_options）\n",
    "            lf_right = (\n",
    "                lf.filter(pl.col(\"symbol_id\").is_in(ids))\n",
    "                .select([*KEYS, WEIGHT, *RESP_ALL])\n",
    "                .with_columns(\n",
    "                    *[pl.col(k).cast(key_casts[k]) for k in KEYS],\n",
    "                    pl.col(WEIGHT).cast(pl.Float32)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # join + 固定列顺序 + 强制类型（避免某批全空变成 NullType）\n",
    "            lf_joined = lf_left.join(lf_right, on=KEYS, how=\"left\").select(EXPECTED)\n",
    "            df_b = (\n",
    "                lf_joined\n",
    "                .with_columns(\n",
    "                    pl.col(WEIGHT).cast(pl.Float32),\n",
    "                    *[pl.col(c).cast(pl.Float32) for c in FEATURES_ALL + RESP_ALL]\n",
    "                )\n",
    "                .collect(streaming=True)\n",
    "            )\n",
    "\n",
    "\n",
    "            tbl = df_b.to_arrow()\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(\n",
    "                    fout, tbl.schema,\n",
    "                    compression=\"zstd\",\n",
    "                    use_dictionary=True,\n",
    "                    write_statistics=True,\n",
    "                )\n",
    "            writer.write_table(tbl, row_group_size=300_000)\n",
    "            del df_b, tbl; gc.collect()\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "print(\"✅ 写到 Blob：\", \"az://\" + OUT_NP)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# I/O\n",
    "TEST_SRC_DIR = LOC(cfg[\"paths\"][\"cache\"])\n",
    "TEST_SRC = f\"{TEST_SRC_DIR}/test_clip.parquet\"\n",
    "\n",
    "TEST_OUT_DIR = LOC(cfg[\"paths\"][\"cache\"])\n",
    "TEST_OUT = f\"{TEST_OUT_DIR}/test_impute.parquet\"\n",
    "KEYS = [\"symbol_id\", \"date_id\", \"time_id\"]\n",
    "BATCH = 5                      # 内存吃紧可降到 2~3\n",
    "ENSURE_SORTED = False          # 如需保证批内顺序，可在 lf_b 后加 .sort(KEYS)\n",
    "EXPECTED = [*KEYS, *FEATURES_ALL]\n",
    "\n",
    "\n",
    "# 2 Impute\n",
    "\n",
    "# 0) 全局均值（Float32 + 兜底）\n",
    "GLOBAL_MEAN = (\n",
    "    pl.scan_parquet(str(SRC)) # 这里仍然用训练集的均值\n",
    "      .select([pl.col(c).mean().fill_null(0.0).cast(pl.Float32).alias(c)\n",
    "               for c in FEATURES_ALL])\n",
    "      .collect(streaming=True)\n",
    "      .to_dicts()[0]\n",
    ")\n",
    "\n",
    "# 1) symbol 列表\n",
    "symbols = (\n",
    "    pl.scan_parquet(str(TEST_SRC))\n",
    "      .select(pl.col(\"symbol_id\").unique())\n",
    "      .collect(streaming=True)[\"symbol_id\"]\n",
    "      .to_list()\n",
    ")\n",
    "\n",
    "writer = None\n",
    "try:\n",
    "    for i in range(0, len(symbols), BATCH):\n",
    "        ids = symbols[i:i+BATCH]\n",
    "\n",
    "        # 批数据（列裁剪 + 固定顺序）\n",
    "        lf_b = (\n",
    "            pl.scan_parquet(str(TEST_SRC))\n",
    "              .filter(pl.col(\"symbol_id\").is_in(ids))\n",
    "              .select(EXPECTED)\n",
    "        )\n",
    "        # 如需保证批内顺序（Stage1 未排序时），打开下一行：\n",
    "        # if ENSURE_SORTED:\n",
    "        #     lf_b = lf_b.sort(KEYS)\n",
    "\n",
    "        # 因果填补（延用你的函数）\n",
    "        lf_imp_b = causal_impute_polars_no_na(\n",
    "            lf=lf_b,\n",
    "            features=FEATURES_ALL,\n",
    "            keys=tuple(KEYS),\n",
    "            open_ticks=(0, 10),\n",
    "            carry_days=5,\n",
    "            intraday_limit= 50,\n",
    "            crossday_same_tick_limit=5,\n",
    "            train_end=None,\n",
    "            fallback=\"mean\",\n",
    "            ensure_sorted=ENSURE_SORTED,\n",
    "            global_means=GLOBAL_MEAN, # 这里仍然用训练集的均值\n",
    "            int_features=None\n",
    "        )\n",
    "\n",
    "        # 再次固定列顺序与 dtype\n",
    "        lf_imp_b = lf_imp_b.select(EXPECTED).with_columns(\n",
    "            [pl.col(c).cast(pl.Float32) for c in FEATURES_ALL]\n",
    "        )\n",
    "\n",
    "        # 物化当前批\n",
    "        df_b = lf_imp_b.collect(streaming=True)\n",
    "        tbl  = df_b.to_arrow()\n",
    "\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(\n",
    "                str(TEST_OUT), tbl.schema,\n",
    "                compression=\"zstd\",\n",
    "                use_dictionary=True,\n",
    "                write_statistics=True\n",
    "            )\n",
    "\n",
    "        writer.write_table(tbl, row_group_size=300_000)  # 20~50万行更省内存\n",
    "        del df_b, tbl\n",
    "        gc.collect()\n",
    "finally:\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "# 3. join responders → 单文件增量写 --------------------\n",
    "# ==== 路径 ====\n",
    "TEST_SRC_IMP_DIR = LOC(cfg[\"paths\"][\"cache\"])                  # \"jackson/js_exp/cache\"\n",
    "TEST_SRC_IMP = f\"{TEST_SRC_IMP_DIR}/test_impute.parquet\"      # 本地路径（Step2 输出）\n",
    "TEST_CLEAN_DIR_NP = NOPRO(cfg[\"paths\"][\"clean\"])                  # \"jackson/js_exp/clean\"\n",
    "TEST_OUT_NP = f\"{TEST_CLEAN_DIR_NP}/test_final_clean.parquet\"               # 无协议路径（给 fs/arrow 用\n",
    "\n",
    "\n",
    "# 右表：lf.responders\n",
    "\n",
    "# ==== 参数 ====\n",
    "EXPECTED = [*KEYS, WEIGHT, *FEATURES_ALL, *RESP_ALL]\n",
    "BATCH = 2  # 每批若干 symbol\n",
    "\n",
    "# ==== 准备输出目录 & 如果已存在旧文件，可选择删除 ====\n",
    "fs.mkdirs(TEST_CLEAN_DIR_NP, exist_ok=True)\n",
    "if fs.exists(TEST_OUT_NP):\n",
    "    fs.rm(TEST_OUT_NP)   # 或者注释掉：有则报错，避免误覆盖\n",
    "\n",
    "# ==== 键类型对齐 ====\n",
    "imp_schema = pl.scan_parquet(str(TEST_SRC_IMP)).collect_schema()\n",
    "key_casts = {k: imp_schema[k] for k in KEYS}\n",
    "\n",
    "# 全量 symbol 列表\n",
    "symbols = (pl.scan_parquet(str(TEST_SRC_IMP))\n",
    "             .select(pl.col(\"symbol_id\").unique())\n",
    "             .collect(streaming=True)[\"symbol_id\"].to_list())\n",
    "\n",
    "\n",
    "# ==== 打开 Blob 文件句柄 + 分批写入行组 ====\n",
    "writer = None\n",
    "with fs.open(TEST_OUT_NP, \"wb\") as fout:\n",
    "    try:\n",
    "        for i in range(0, len(symbols), BATCH):\n",
    "            ids = symbols[i:i+BATCH]\n",
    "\n",
    "            # 左表：impute 后的特征（本地）\n",
    "            lf_left = (pl.scan_parquet(str(TEST_SRC_IMP))\n",
    "                         .filter(pl.col(\"symbol_id\").is_in(ids))\n",
    "                         .select([*KEYS, *FEATURES_ALL]))\n",
    "\n",
    "            # 右表：responders（Blob，记得 storage_options）\n",
    "            lf_right = (\n",
    "                lf_test.filter(pl.col(\"symbol_id\").is_in(ids))\n",
    "                .select([*KEYS, WEIGHT, *RESP_ALL])\n",
    "                .with_columns(\n",
    "                    *[pl.col(k).cast(key_casts[k]) for k in KEYS],\n",
    "                    pl.col(WEIGHT).cast(pl.Float32)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # join + 固定列顺序 + 强制类型（避免某批全空变成 NullType）\n",
    "            lf_joined = lf_left.join(lf_right, on=KEYS, how=\"left\").select(EXPECTED)\n",
    "            df_b = (\n",
    "                lf_joined\n",
    "                .with_columns(\n",
    "                    pl.col(WEIGHT).cast(pl.Float32),\n",
    "                    *[pl.col(c).cast(pl.Float32) for c in FEATURES_ALL + RESP_ALL]\n",
    "                )\n",
    "                .collect(streaming=True)\n",
    "            )\n",
    "\n",
    "\n",
    "            tbl = df_b.to_arrow()\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(\n",
    "                    fout, tbl.schema,\n",
    "                    compression=\"zstd\",\n",
    "                    use_dictionary=True,\n",
    "                    write_statistics=True,\n",
    "                )\n",
    "            writer.write_table(tbl, row_group_size=300_000)\n",
    "            del df_b, tbl; gc.collect()\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "print(\"✅ 写到 Blob：\", \"az://\" + TEST_OUT_NP)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018dae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b9eb33a",
   "metadata": {},
   "source": [
    "\n",
    "# 特征工程函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85363ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "\n",
    "# -----------------------------\n",
    "# A. prev-day tails + daily summaries\n",
    "# -----------------------------\n",
    "import polars as pl\n",
    "from typing import Sequence, Optional, Tuple\n",
    "\n",
    "def _resolve_prev_for_daily(\n",
    "    daily: pl.LazyFrame,\n",
    "    *,\n",
    "    keys: Tuple[str,str] = (\"symbol_id\",\"date_id\"),\n",
    "    cols: Sequence[str],\n",
    "    prev_soft_days: Optional[int],\n",
    "    cast_f32: bool = True,\n",
    ") -> pl.LazyFrame:\n",
    "    g_symbol, g_date = keys\n",
    "    exprs = []\n",
    "    for c in cols:\n",
    "        prev_row_val = pl.col(c).shift(1).over(g_symbol)\n",
    "        prev_row_day = pl.col(g_date).shift(1).over(g_symbol)\n",
    "\n",
    "        # strict d-1\n",
    "        prev_strict = pl.when(pl.col(g_date) == (prev_row_day + 1)).then(prev_row_val).otherwise(None)\n",
    "\n",
    "        if prev_soft_days is None:\n",
    "            resolved = prev_strict\n",
    "        else:\n",
    "            # last non-null day/value before d, computed per symbol\n",
    "            last_non_null_day = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(g_date))\n",
    "                  .otherwise(None)\n",
    "                  .forward_fill().over(g_symbol)\n",
    "                  .shift(1)\n",
    "            )\n",
    "            last_non_null_val = (\n",
    "                pl.col(c).forward_fill().over(g_symbol).shift(1)\n",
    "            )\n",
    "            gap = (pl.col(g_date) - last_non_null_day).cast(pl.Int32)\n",
    "\n",
    "            resolved = pl.coalesce([\n",
    "                prev_strict,\n",
    "                pl.when((gap <= int(prev_soft_days)) & last_non_null_val.is_not_null())\n",
    "                  .then(last_non_null_val),\n",
    "            ])\n",
    "\n",
    "        exprs.append((resolved.cast(pl.Float32) if cast_f32 else resolved).alias(c))\n",
    "\n",
    "    return daily.with_columns(exprs)\n",
    "\n",
    "\n",
    "def fe_prevday_tail_and_summaries(\n",
    "    lf: pl.LazyFrame,\n",
    "    *,\n",
    "    rep_cols: Sequence[str],                           # responders，用于“昨日尾部”和“日级摘要”\n",
    "    tail_lags: Sequence[int] = (1,),                   # 倒数第 L（会自动补 diffs 所需的 L+1）\n",
    "    tail_diffs: Sequence[int] = (1,),                 # dK = lag1 - lag(K+1)\n",
    "    rolling_windows: Sequence[int] | None = (3,5,20), # 对 prevday_close 做滚动\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    assume_sorted: bool = True,\n",
    "    cast_f32: bool = True,\n",
    "    prev_soft_days: Optional[int] = None,             # 严格 d-1 + ≤K 天回退（None=只严格 d-1）\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"一次日频聚合得到昨日尾部与日级摘要 → 统一上一日解析 → 回拼到 tick 级。\"\"\"\n",
    "    g_symbol, g_date, g_time = keys\n",
    "    if not assume_sorted:\n",
    "        lf = lf.sort(list(keys))\n",
    "\n",
    "    # --- 一次性日频聚合 ---\n",
    "    need_L = sorted(set(tail_lags) | {k+1 for k in tail_diffs} | {1})\n",
    "    agg_exprs = []\n",
    "    for r in rep_cols:\n",
    "        # 尾部（倒数第 L）\n",
    "        for L in need_L:\n",
    "            agg_exprs.append(\n",
    "                pl.col(r).sort_by(pl.col(g_time)).slice(-L, 1).first().alias(f\"{r}_prev_tail_lag{L}\")\n",
    "            )\n",
    "        # 当日统计\n",
    "        agg_exprs += [\n",
    "            pl.col(r).sort_by(pl.col(g_time)).last().alias(f\"{r}_prevday_close\"),\n",
    "            pl.col(r).mean().alias(f\"{r}_prevday_mean\"),\n",
    "            pl.col(r).std(ddof=0).alias(f\"{r}_prevday_std\"),\n",
    "        ]\n",
    "\n",
    "    daily = (lf.group_by([g_symbol, g_date]).agg(agg_exprs).sort([g_symbol, g_date]))\n",
    "\n",
    "    # 派生 dK\n",
    "    daily = daily.with_columns([\n",
    "        (pl.col(f\"{r}_prev_tail_lag1\") - pl.col(f\"{r}_prev_tail_lag{K+1}\")).alias(f\"{r}_prev_tail_d{K}\")\n",
    "        for r in rep_cols for K in tail_diffs\n",
    "    ])\n",
    "\n",
    "    # prev2day/overnight/rolling\n",
    "    daily = daily.with_columns([\n",
    "        pl.col(f\"{r}_prevday_close\").shift(1).over(g_symbol).alias(f\"{r}_prev2day_close\")\n",
    "        for r in rep_cols\n",
    "    ]).with_columns(\n",
    "        [\n",
    "            (pl.col(f\"{r}_prevday_close\") - pl.col(f\"{r}_prevday_mean\")).alias(f\"{r}_prevday_close_minus_mean\")\n",
    "            for r in rep_cols\n",
    "        ] + [\n",
    "            (pl.col(f\"{r}_prevday_close\") - pl.col(f\"{r}_prev2day_close\")).alias(f\"{r}_overnight_gap\")\n",
    "            for r in rep_cols\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if rolling_windows:\n",
    "        wins = sorted({int(w) for w in rolling_windows if int(w) > 1})\n",
    "        roll_exprs = []\n",
    "        for r in rep_cols:\n",
    "            for w in wins:\n",
    "                roll_exprs += [\n",
    "                    pl.col(f\"{r}_prevday_close\").rolling_mean(window_size=w, min_samples=1).over(g_symbol).alias(f\"{r}_close_roll{w}_mean\"),\n",
    "                    pl.col(f\"{r}_prevday_close\").rolling_std(window_size=w, ddof=0, min_samples=1).over(g_symbol).alias(f\"{r}_close_roll{w}_std\"),\n",
    "                ]\n",
    "        daily = daily.with_columns(roll_exprs)\n",
    "\n",
    "\n",
    "    # === 关键：把所有“非键列”转为“对当日 d 生效的上一日值”（严格 d-1 + ≤K 回退） ===\n",
    "    prev_cols = [c for c in daily.collect_schema().names() if c not in (g_symbol, g_date)]\n",
    "    daily_prev = _resolve_prev_for_daily(\n",
    "        daily, keys=(g_symbol, g_date), cols=prev_cols, prev_soft_days=prev_soft_days, cast_f32=cast_f32\n",
    "    )\n",
    "\n",
    "    # 回拼到 tick 级\n",
    "    return lf.join(daily_prev, on=[g_symbol, g_date], how=\"left\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# B: same time_id cross-day\n",
    "# -----------------------------\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Sequence, Iterable, Optional, Tuple\n",
    "\n",
    "def fe_same_timeid_crossday(\n",
    "    lf: pl.LazyFrame,\n",
    "    rep_cols: Sequence[str],\n",
    "    ndays: int = 3,\n",
    "    stats_rep_cols: Optional[Sequence[str]] = None,\n",
    "    add_prev1_multirep: bool = True,\n",
    "    batch_size: int = 5,\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    assume_sorted: bool = True,\n",
    "    cast_f32: bool = True,\n",
    "    # <<< 新增：控制“严格 d-k / 宽松 K 天”\n",
    "    prev_soft_days: Optional[int] = None,   # None=不限制；比如 5 表示仅接受 gap<=5 天内的 prev{k}\n",
    "    strict_k: bool = False,                 # True=严格 d-k（gap==k）；忽略 prev_soft_days\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    同一 time_id 跨日 prev{k} + 统计（均值/方差/斜率）。\n",
    "\n",
    "    新增：\n",
    "    - prev_soft_days: 若设定，则对每个 prev{k} 仅保留 (date - date.shift(k)) <= K 的值，过旧置空。\n",
    "    - strict_k: 若 True，则严格 d-k（gap==k）；此时 prev_soft_days 被忽略。\n",
    "    默认维持原语义（纯 shift），仅在传参时启用过滤/严格。\n",
    "    \"\"\"\n",
    "    g_symbol, g_date, g_time = keys\n",
    "    if not assume_sorted:\n",
    "        # 保证 (symbol, time) 维度上 date 递增，再做 shift(k).over([symbol,time]) 是因果安全的\n",
    "        lf = lf.sort([g_symbol, g_time, g_date])\n",
    "\n",
    "    if stats_rep_cols is None:\n",
    "        stats_rep_cols = list(rep_cols)\n",
    "\n",
    "    def _chunks(lst, k):\n",
    "        for i in range(0, len(lst), k):\n",
    "            yield lst[i:i+k]\n",
    "\n",
    "    lf_cur = lf\n",
    "\n",
    "    # 1) prev{k}（可选：严格/TTL 过滤）\n",
    "    for batch in _chunks(list(rep_cols), batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            for k in range(1, ndays+1):\n",
    "                val_k  = pl.col(r).shift(k).over([g_symbol, g_time])\n",
    "                day_k  = pl.col(g_date).shift(k).over([g_symbol, g_time])\n",
    "                gap_k  = (pl.col(g_date) - day_k).cast(pl.Int32)\n",
    "\n",
    "                if strict_k:\n",
    "                    # 严格 d-k：仅 gap==k 保留\n",
    "                    val_k = pl.when(gap_k == k).then(val_k).otherwise(None)\n",
    "                elif prev_soft_days is not None:\n",
    "                    # 宽松 K 天：仅 gap<=K 保留（最近 k 个“可见历史”仍可能跨缺口，但限制不超过 K 天）\n",
    "                    val_k = pl.when(gap_k <= int(prev_soft_days)).then(val_k).otherwise(None)\n",
    "                # else: 保持原逻辑（纯 shift）\n",
    "\n",
    "                if cast_f32:\n",
    "                    val_k = val_k.cast(pl.Float32)\n",
    "                exprs.append(val_k.alias(f\"{r}_same_t_prev{k}\"))\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 2) mean/std（忽略 null）\n",
    "    for batch in _chunks([r for r in stats_rep_cols if r in rep_cols], batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            cols = [f\"{r}_same_t_prev{k}\" for k in range(1, ndays+1)]\n",
    "            vals = pl.concat_list([pl.col(c) for c in cols]).list.drop_nulls()\n",
    "            m = vals.list.mean()\n",
    "            s = vals.list.std(ddof=0)\n",
    "            if cast_f32:\n",
    "                m = m.cast(pl.Float32); s = s.cast(pl.Float32)\n",
    "            exprs += [\n",
    "                m.alias(f\"{r}_same_t_last{ndays}_mean\"),\n",
    "                s.alias(f\"{r}_same_t_last{ndays}_std\"),\n",
    "            ]\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 3) slope（标准化后与标准化时间权重点积 / 有效样本数）\n",
    "    x = np.arange(1, ndays+1, dtype=np.float64)\n",
    "    x = (x - x.mean()) / (x.std() + 1e-9)\n",
    "    x_lits = [pl.lit(float(v)) for v in x]\n",
    "\n",
    "    for batch in _chunks([r for r in stats_rep_cols if r in rep_cols], batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            cols = [f\"{r}_same_t_prev{k}\" for k in range(1, ndays+1)]\n",
    "            mean_ref = pl.col(f\"{r}_same_t_last{ndays}_mean\")\n",
    "            std_ref  = pl.col(f\"{r}_same_t_last{ndays}_std\")\n",
    "            terms = [((pl.col(c) - mean_ref) / (std_ref + 1e-9)) * x_lits[i]\n",
    "                    for i,c in enumerate(cols)]\n",
    "            n_eff = pl.sum_horizontal([pl.col(c).is_not_null().cast(pl.Int32) for c in cols]).cast(pl.Float32)\n",
    "            den   = pl.when(n_eff > 0).then(n_eff).otherwise(pl.lit(1.0))\n",
    "            slope = (pl.sum_horizontal(terms) / den)\n",
    "            if cast_f32: slope = slope.cast(pl.Float32)\n",
    "            exprs.append(slope.alias(f\"{r}_same_t_last{ndays}_slope\"))\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 4) 跨 responder 的 prev1 行内统计\n",
    "    if add_prev1_multirep and len(rep_cols) > 0:\n",
    "        n_rep = len(rep_cols)\n",
    "        prev1_cols = [f\"{r}_same_t_prev1\" for r in rep_cols]\n",
    "        prev1_list = pl.concat_list([pl.col(c) for c in prev1_cols]).list.drop_nulls()\n",
    "        m1 = prev1_list.list.mean()\n",
    "        s1 = prev1_list.list.std(ddof=0)\n",
    "        if cast_f32: m1 = m1.cast(pl.Float32); s1 = s1.cast(pl.Float32)\n",
    "        lf_cur = lf_cur.with_columns([\n",
    "            m1.alias(f\"prev1_same_t_mean_{n_rep}rep\"),\n",
    "            s1.alias(f\"prev1_same_t_std_{n_rep}rep\"),\n",
    "        ])\n",
    "\n",
    "    return lf_cur\n",
    "\n",
    "\n",
    "\n",
    "# C 系列：历史值、收益率、差分、极值归一化、指数加权均值等\n",
    "\n",
    "def build_history_features_polars(\n",
    "    *,\n",
    "    lf: Optional[pl.LazyFrame] = None,\n",
    "    paths: Optional[Sequence[str]] = None,\n",
    "    feature_cols: Sequence[str],\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    group_cols: Sequence[str] = (\"symbol_id\",),\n",
    "    assume_sorted: bool = False,\n",
    "    cast_f32: bool = True,\n",
    "    batch_size: int = 10,\n",
    "    lags: Iterable[int] = (1, 3),\n",
    "    ret_periods: Iterable[int] = (1,),\n",
    "    diff_periods: Iterable[int] = (1,),\n",
    "    rz_windows: Iterable[int] = (5,),\n",
    "    keep_rmean_rstd: bool = True,\n",
    "    ewm_spans: Iterable[int] = (10,),\n",
    "    cs_cols: Optional[Sequence[str]] = None,\n",
    "    cs_by: Sequence[str] = (\"date_id\",\"time_id\"),\n",
    "    prev_soft_days: Optional[int] = None,\n",
    ") -> pl.LazyFrame:\n",
    "    assert lf is not None or paths is not None, \"provide either lf or paths\"\n",
    "    if lf is None:\n",
    "        lf = pl.scan_parquet(list(paths))\n",
    "\n",
    "    g_sym, g_date, g_time = keys\n",
    "    by_grp = list(group_cols)\n",
    "    by_cs  = list(cs_by)\n",
    "    feature_cols = list(feature_cols)\n",
    "\n",
    "    need_cols = [*keys, *feature_cols]\n",
    "    schema = lf.collect_schema()\n",
    "    miss = [c for c in need_cols if c not in schema]\n",
    "    if miss:\n",
    "        raise KeyError(f\"Columns not found: {miss}\")\n",
    "\n",
    "    lf_out = lf.select(need_cols)\n",
    "    if not assume_sorted:\n",
    "        lf_out = lf_out.sort(list(keys))\n",
    "\n",
    "    def _chunks(lst, k):\n",
    "        for i in range(0, len(lst), k):\n",
    "            yield lst[i:i+k]\n",
    "\n",
    "    # ---- 规范化参数：None/[] -> 空元组；并去重/转 int/保正数 ----\n",
    "    def _clean_iter(x):\n",
    "        if not x:\n",
    "            return tuple()\n",
    "        return tuple(int(v) for v in x)\n",
    "    def _clean_pos_sorted_unique(x):\n",
    "        if not x:\n",
    "            return tuple()\n",
    "        return tuple(sorted({int(v) for v in x if int(v) >= 1}))\n",
    "\n",
    "    LAGS   = _clean_pos_sorted_unique(lags)\n",
    "    K_RET  = _clean_pos_sorted_unique(ret_periods)\n",
    "    K_DIFF = _clean_pos_sorted_unique(diff_periods)\n",
    "    RZW    = _clean_pos_sorted_unique(rz_windows)\n",
    "    SPANS  = _clean_pos_sorted_unique(ewm_spans)\n",
    "\n",
    "    # ---- TTL helpers ----\n",
    "    def _ttl_mask(k: int) -> pl.Expr:\n",
    "        if prev_soft_days is None:\n",
    "            return pl.lit(True)\n",
    "        return (pl.col(g_date) - pl.col(g_date).shift(k).over(by_grp)) <= int(prev_soft_days)\n",
    "\n",
    "    def _gate(expr: pl.Expr, k: int) -> pl.Expr:\n",
    "        if prev_soft_days is None:\n",
    "            return expr\n",
    "        return pl.when(_ttl_mask(k)).then(expr).otherwise(None)\n",
    "\n",
    "    # C1 lags（可选）\n",
    "    if LAGS:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for L in LAGS:\n",
    "                for c in batch:\n",
    "                    e = pl.col(c).shift(L).over(by_grp)\n",
    "                    e = _gate(e, L)\n",
    "                    if cast_f32: e = e.cast(pl.Float32)\n",
    "                    exprs.append(e.alias(f\"{c}__lag{L}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "    # C2 returns（可选）\n",
    "    if K_RET:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for c in batch:\n",
    "                cur = pl.col(c)\n",
    "                for k in K_RET:\n",
    "                    prev = (pl.col(f\"{c}__lag{k}\") if k in LAGS\n",
    "                            else pl.col(c).shift(k).over(by_grp))\n",
    "                    mask = _ttl_mask(k)\n",
    "                    ret = pl.when(mask & prev.is_not_null() & (prev != 0)).then(cur / prev - 1.0).otherwise(None)\n",
    "                    if cast_f32: ret = ret.cast(pl.Float32)\n",
    "                    exprs.append(ret.alias(f\"{c}__ret{k}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "    # C3 diffs（可选）\n",
    "    if K_DIFF:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for c in batch:\n",
    "                cur = pl.col(c)\n",
    "                for k in K_DIFF:\n",
    "                    prevk = pl.col(c).shift(k).over(by_grp)\n",
    "                    d = pl.when(_ttl_mask(k)).then(cur - prevk).otherwise(None)\n",
    "                    if cast_f32: d = d.cast(pl.Float32)\n",
    "                    exprs.append(d.alias(f\"{c}__diff{k}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "    # 仅当 RZ/EWM 需要、且没有 lag1 时，才建临时 lag1\n",
    "    need_tmp_lag1 = (bool(RZW) or bool(SPANS)) and (1 not in LAGS)\n",
    "    if need_tmp_lag1:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            lf_out = lf_out.with_columns([\n",
    "                pl.col(c).shift(1).over(by_grp).alias(f\"{c}__lag1_tmp\") for c in batch\n",
    "            ])\n",
    "\n",
    "    def _lag1_col(c: str) -> pl.Expr:\n",
    "        return pl.col(f\"{c}__lag1\") if 1 in LAGS else pl.col(f\"{c}__lag1_tmp\")\n",
    "\n",
    "    # C4 rolling r-z（可选）\n",
    "    if RZW:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            base_cols = []\n",
    "            for c in batch:\n",
    "                base_alias = f\"{c}__tminus1_base\"\n",
    "                base_cols.append(_gate(_lag1_col(c), 1).alias(base_alias))\n",
    "            lf_out = lf_out.with_columns(base_cols)\n",
    "\n",
    "            roll_exprs = []\n",
    "            for c in batch:\n",
    "                base = pl.col(f\"{c}__tminus1_base\")\n",
    "                for w in RZW:\n",
    "                    m  = base.rolling_mean(window_size=w, min_samples=1).over(by_grp)\n",
    "                    s  = base.rolling_std(window_size=w, ddof=0, min_samples=2).over(by_grp)\n",
    "                    rz = (base - m) / (s + 1e-9)\n",
    "                    if cast_f32:\n",
    "                        m = m.cast(pl.Float32); s = s.cast(pl.Float32); rz = rz.cast(pl.Float32)\n",
    "                    if keep_rmean_rstd:\n",
    "                        roll_exprs += [\n",
    "                            m.alias(f\"{c}__rmean{w}\"),\n",
    "                            s.alias(f\"{c}__rstd{w}\"),\n",
    "                            rz.alias(f\"{c}__rz{w}\"),\n",
    "                        ]\n",
    "                    else:\n",
    "                        roll_exprs.append(rz.alias(f\"{c}__rz{w}\"))\n",
    "            lf_out = lf_out.with_columns(roll_exprs)\n",
    "            lf_out = lf_out.drop([f\"{c}__tminus1_base\" for c in batch])\n",
    "\n",
    "    # C5 EWM（可选）\n",
    "    if SPANS:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            base_cols = []\n",
    "            for c in batch:\n",
    "                base_alias = f\"{c}__tminus1_base\"\n",
    "                base_cols.append(_gate(_lag1_col(c), 1).alias(base_alias))\n",
    "            lf_out = lf_out.with_columns(base_cols)\n",
    "\n",
    "            ewm_exprs = []\n",
    "            for c in batch:\n",
    "                base = pl.col(f\"{c}__tminus1_base\")\n",
    "                for s in SPANS:\n",
    "                    ema = base.ewm_mean(span=int(s), adjust=False, ignore_nulls=True).over(by_grp)\n",
    "                    if cast_f32: ema = ema.cast(pl.Float32)\n",
    "                    ewm_exprs.append(ema.alias(f\"{c}__ewm{s}\"))\n",
    "            lf_out = lf_out.with_columns(ewm_exprs)\n",
    "            lf_out = lf_out.drop([f\"{c}__tminus1_base\" for c in batch])\n",
    "\n",
    "    if need_tmp_lag1:\n",
    "        lf_out = lf_out.drop([f\"{c}__lag1_tmp\" for c in feature_cols])\n",
    "\n",
    "    # C6 cross-section rank（可选）\n",
    "    if cs_cols:\n",
    "        cs_cols = [c for c in cs_cols if c in feature_cols]\n",
    "        if cs_cols:\n",
    "            lf_out = lf_out.with_columns([\n",
    "                _gate(pl.col(c).shift(1).over(by_grp), 1).alias(f\"{c}__tminus1_base\")\n",
    "                for c in cs_cols\n",
    "            ])\n",
    "            cs_n_tbl = (\n",
    "                lf_out.select(list(by_cs))\n",
    "                      .group_by(list(by_cs))\n",
    "                      .len()\n",
    "                      .rename({\"len\": \"__cs_n\"})\n",
    "            )\n",
    "            lf_out = lf_out.join(cs_n_tbl, on=list(by_cs), how=\"left\")\n",
    "            lf_out = lf_out.with_columns([\n",
    "                pl.when(pl.col(f\"{c}__tminus1_base\").is_null())\n",
    "                  .then(None)\n",
    "                  .otherwise(pl.col(f\"{c}__tminus1_base\").rank(method=\"average\"))\n",
    "                  .over(list(by_cs))\n",
    "                  .alias(f\"{c}__cs_rank_raw\")\n",
    "                for c in cs_cols\n",
    "            ])\n",
    "            lf_out = lf_out.with_columns([\n",
    "                pl.when(pl.col(f\"{c}__tminus1_base\").is_null())\n",
    "                  .then(None)\n",
    "                  .otherwise(pl.col(f\"{c}__cs_rank_raw\") / pl.col(\"__cs_n\"))\n",
    "                  .cast(pl.Float32 if cast_f32 else pl.Float64)\n",
    "                  .alias(f\"{c}__csrank\")\n",
    "                for c in cs_cols\n",
    "            ])\n",
    "            lf_out = lf_out.drop(\n",
    "                [f\"{c}__tminus1_base\" for c in cs_cols] +\n",
    "                [f\"{c}__cs_rank_raw\"  for c in cs_cols]\n",
    "            )\n",
    "\n",
    "    return lf_out\n",
    "\n",
    "def collect_write(lf: pl.LazyFrame, path: str, compression: str = \"zstd\"):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        df = lf.collect(streaming=True)\n",
    "    except TypeError:\n",
    "        df = lf.collect()\n",
    "    df.write_parquet(path, compression=compression)\n",
    "    del df\n",
    "    \n",
    "@dataclass\n",
    "class StageA:\n",
    "    tail_lags: Sequence[int]\n",
    "    tail_diffs: Sequence[int]\n",
    "    rolling_windows: Optional[Sequence[int]]\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    assume_sorted: bool = True\n",
    "    cast_f32: bool = True\n",
    "\n",
    "@dataclass\n",
    "class StageB:\n",
    "    ndays: int\n",
    "    stats_rep_cols: Optional[Sequence[str]] = None\n",
    "    add_prev1_multirep: bool = True\n",
    "    batch_size: int = 5\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    strict_k: bool = False\n",
    "    assume_sorted: bool = True\n",
    "    cast_f32: bool = True\n",
    "\n",
    "# C 的每个操作可选；None / [] 表示跳过该操作\n",
    "@dataclass\n",
    "class StageC:\n",
    "    lags: Optional[Iterable[int]] = None\n",
    "    ret_periods: Optional[Iterable[int]] = None\n",
    "    diff_periods: Optional[Iterable[int]] = None\n",
    "    rz_windows: Optional[Iterable[int]] = None\n",
    "    ewm_spans: Optional[Iterable[int]] = None\n",
    "    keep_rmean_rstd: bool = True\n",
    "    cs_cols: Optional[Sequence[str]] = None\n",
    "    cs_by: Sequence[str] = (\"date_id\",\"time_id\")\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    batch_size: Optional[int] = 10\n",
    "    assume_sorted: bool = True\n",
    "    cast_f32: bool = True\n",
    "\n",
    "\n",
    "def run_staged_engineering(\n",
    "    lf_base: pl.LazyFrame,\n",
    "    *,\n",
    "    keys: Sequence[str],\n",
    "    rep_cols: Sequence[str],\n",
    "    feature_cols: Sequence[str],\n",
    "    out_dir: str,\n",
    "    A: StageA | None = None,\n",
    "    B: StageB | None = None,\n",
    "    C: StageC | None = None,\n",
    "):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    paths = {}\n",
    "\n",
    "    # ---------- A ----------\n",
    "    if A is not None:\n",
    "        lf_resp = lf_base.select([*keys, *rep_cols])\n",
    "        lf_a_full = fe_prevday_tail_and_summaries(\n",
    "            lf_resp,\n",
    "            rep_cols=rep_cols,\n",
    "            tail_lags=A.tail_lags,\n",
    "            tail_diffs=A.tail_diffs,\n",
    "            rolling_windows=A.rolling_windows,\n",
    "            keys=tuple(keys),\n",
    "            assume_sorted=A.assume_sorted,\n",
    "            cast_f32=A.cast_f32,\n",
    "            prev_soft_days=A.prev_soft_days,\n",
    "        )\n",
    "        drop = set(keys) | set(rep_cols)\n",
    "        a_cols = [c for c in lf_a_full.collect_schema().names() if c not in drop]\n",
    "        pA = f\"{out_dir}/stage_a.parquet\"\n",
    "        collect_write(lf_a_full.select([*keys, *a_cols]), pA)\n",
    "        paths[\"A\"] = pA\n",
    "\n",
    "    # ---------- B ----------\n",
    "    if B is not None:\n",
    "        lf_resp = lf_base.select([*keys, *rep_cols])\n",
    "        lf_b_full = fe_same_timeid_crossday(\n",
    "            lf_resp,\n",
    "            rep_cols=rep_cols,\n",
    "            ndays=B.ndays,\n",
    "            stats_rep_cols=B.stats_rep_cols,\n",
    "            add_prev1_multirep=B.add_prev1_multirep,\n",
    "            batch_size=B.batch_size,\n",
    "            keys=tuple(keys),\n",
    "            assume_sorted=B.assume_sorted,\n",
    "            cast_f32=B.cast_f32,\n",
    "            prev_soft_days=B.prev_soft_days,\n",
    "            strict_k=B.strict_k,\n",
    "        )\n",
    "        drop = set(keys) | set(rep_cols)\n",
    "        b_cols = [c for c in lf_b_full.collect_schema().names() if c not in drop]\n",
    "        pB = f\"{out_dir}/stage_b.parquet\"\n",
    "        collect_write(lf_b_full.select([*keys, *b_cols]), pB)\n",
    "        paths[\"B\"] = pB\n",
    "\n",
    "    # ---------- C：按“操作”分别输出单文件（不分窗口、不分 batch） ----------\n",
    "    if C is not None:\n",
    "        paths[\"C\"] = {}\n",
    "\n",
    "        def _do_op(op_name: str, **op_flags):\n",
    "            lf_src = lf_base.select([*keys, *feature_cols])\n",
    "            lf_d = build_history_features_polars(\n",
    "                lf=lf_src,\n",
    "                feature_cols=feature_cols,\n",
    "                keys=tuple(keys),\n",
    "                group_cols=(\"symbol_id\",),\n",
    "                assume_sorted=C.assume_sorted,\n",
    "                cast_f32=C.cast_f32,\n",
    "                batch_size=C.batch_size,\n",
    "                lags=op_flags.get(\"lags\"),\n",
    "                ret_periods=op_flags.get(\"ret_periods\"),\n",
    "                diff_periods=op_flags.get(\"diff_periods\"),\n",
    "                rz_windows=op_flags.get(\"rz_windows\"),\n",
    "                keep_rmean_rstd=C.keep_rmean_rstd,\n",
    "                ewm_spans=op_flags.get(\"ewm_spans\"),\n",
    "                cs_cols=op_flags.get(\"cs_cols\"),\n",
    "                cs_by=C.cs_by,\n",
    "                prev_soft_days=C.prev_soft_days\n",
    "            ).drop(feature_cols)\n",
    "            p = f\"{out_dir}/stage_c_{op_name}.parquet\"\n",
    "            collect_write(lf_d, p)\n",
    "            paths[\"C\"][op_name] = [p]\n",
    "\n",
    "        if C.lags:         _do_op(\"lags\",   lags=C.lags)\n",
    "        if C.ret_periods:  _do_op(\"ret\",    ret_periods=C.ret_periods)\n",
    "        if C.diff_periods: _do_op(\"diff\",   diff_periods=C.diff_periods)\n",
    "        if C.rz_windows:   _do_op(\"rz\",     rz_windows=C.rz_windows)\n",
    "        if C.ewm_spans:    _do_op(\"ewm\",    ewm_spans=C.ewm_spans)\n",
    "        if C.cs_cols:      _do_op(\"csrank\", cs_cols=C.cs_cols)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def _join_from(lf_left: pl.LazyFrame, path: str, lo: int, hi: int) -> pl.LazyFrame:\n",
    "    cols = FILE2COLS.get(path, [])\n",
    "    if not cols: return lf_left\n",
    "    lf_right = (pl.scan_parquet(path)\n",
    "                  .select([*KEYS, *cols])\n",
    "                  .filter(pl.col(\"date_id\").is_between(lo, hi))\n",
    "                  .sort(KEYS))\n",
    "    return lf_left.join(lf_right, on=KEYS, how=\"left\")\n",
    "\n",
    "def build_slice(lo: int, hi: int) -> pd.DataFrame:\n",
    "    lf = (pl.scan_parquet(PARQUET_PATHS)\n",
    "            .select([*KEYS, TARGET])\n",
    "            .filter(pl.col(\"date_id\").is_between(lo, hi))\n",
    "            .sort(KEYS))\n",
    "    for p in FILE2COLS:\n",
    "        lf = _join_from(lf, p, lo, hi)\n",
    "    return lf.collect(streaming=True).to_pandas()\n",
    "\n",
    "def weighted_r2_zero_mean(y_true, y_pred, weight) -> float:\n",
    "    \"\"\"\n",
    "    Sample-weighted zero-mean R^2 used in Jane Street:\n",
    "        R^2 = 1 - sum_i w_i (y_i - yhat_i)^2 / sum_i w_i y_i^2\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "    weight = np.asarray(weight, dtype=np.float64).ravel()\n",
    "    assert y_true.shape == y_pred.shape == weight.shape\n",
    "\n",
    "    num = np.sum(weight * (y_true - y_pred) ** 2)\n",
    "    den = np.sum(weight * (y_true ** 2))\n",
    "    if den <= 0:\n",
    "        return 0.0  # safe fallback (shouldn't happen on the full JS eval)\n",
    "    return 1.0 - (num / den)\n",
    "\n",
    "def lgb_wr2_eval(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    w = train_data.get_weight()\n",
    "    if w is None:\n",
    "        w = np.ones_like(y)\n",
    "    score = weighted_r2_zero_mean(y, preds, w)\n",
    "    return ('wr2', score, True)  # higher is better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c19477",
   "metadata": {},
   "source": [
    "# 特征选择- 初选 (选特征，省略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b61d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, glob\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = [\"/mnt/data/js/clean/final_clean.parquet\"]\n",
    "KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "TARGET = \"responder_6\"\n",
    "WEIGHT = 'weight'\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)]\n",
    "\n",
    "OUT_DIR = \"/mnt/data/js/cache/first_selection\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- A: prev-day tails + daily summaries ---\n",
    "A = StageA(\n",
    "    tail_lags=(1,),\n",
    "    tail_diffs=(1,),\n",
    "    rolling_windows=(5,),\n",
    "    prev_soft_days=3,          # allow fallback up to 3 calendar days\n",
    ")\n",
    "\n",
    "# --- B: same time_id cross-day ---\n",
    "B = StageB(\n",
    "    ndays=3,                   # prev{1..3} at same time_id\n",
    "    stats_rep_cols=None,       # default: use rep_cols\n",
    "    add_prev1_multirep=True,\n",
    "    batch_size=5,\n",
    "    prev_soft_days=3,          # TTL for gaps\n",
    "    strict_k=False,            # allow ≤K-day gaps instead of strict d-k\n",
    ")\n",
    "\n",
    "# --- C: history features (keep it tiny) ---\n",
    "C = StageC(\n",
    "    lags=(1, ),\n",
    "    ret_periods=(1,),\n",
    "    diff_periods=(1,),\n",
    "    rz_windows=(5,),\n",
    "    ewm_spans=(10,),\n",
    "    keep_rmean_rstd=True,\n",
    "    cs_cols=None,        # must be subset of feature_cols\n",
    "    cs_by=(\"date_id\",\"time_id\"),\n",
    "    prev_soft_days=3,\n",
    ")\n",
    "\n",
    "# example call\n",
    "paths = run_staged_engineering(\n",
    "    lf_base=lf_base,                # your base LazyFrame\n",
    "    keys=KEYS,\n",
    "    rep_cols=REP_COLS,         # updated to use REP_COLS\n",
    "    feature_cols=FEATURE_COLS, # updated to use FEATURE_COLS\n",
    "    out_dir=OUT_DIR,\n",
    "    A=A, B=B, C=C,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e1720",
   "metadata": {},
   "source": [
    "0. 准备与切分天数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4df6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STAGE_PATHS = [\n",
    "    \"/mnt/data/js/cache/first_selection/stage_a.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_b.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_lags.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_ret.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_diff.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_rz.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_ewm.parquet\",\n",
    "]\n",
    "\n",
    "DATE_LO, DATE_HI = 1200, 1400\n",
    "OUT_DIR = \"/mnt/data/js/cache/first_selection/run_full\"\n",
    "SHARD_DIR = f\"{OUT_DIR}/shards_all\"\n",
    "Path(SHARD_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lf_base = pl.scan_parquet(BASE_PATH)\n",
    "# 仅拿目标区间的base\n",
    "lf_range = lf_base.filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "\n",
    "# days & split\n",
    "days = (lf_range.select(pl.col(\"date_id\").unique().sort())\n",
    "                .collect(streaming=True)[\"date_id\"].to_list())\n",
    "cut = int(len(days) * 0.8)\n",
    "train_days, val_days = days[:cut], days[cut:]\n",
    "print(f\"[split] train {len(train_days)} days, val {len(val_days)} days, range={days[0]}..{days[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eddc42",
   "metadata": {},
   "source": [
    "1.收集全量特征列名（并集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 来自 base 的特征列\n",
    "feat_cols = set(FEATURE_COLS)\n",
    "\n",
    "# 各 stage 全部列（除 KEYS/TARGET/WEIGHT）\n",
    "for p in STAGE_PATHS:\n",
    "    if not os.path.exists(p):\n",
    "        print(f\"[skip] missing: {p}\")\n",
    "        continue\n",
    "    names = pl.scan_parquet(p).collect_schema().names()\n",
    "    for c in names:\n",
    "        if c not in KEYS and c not in (TARGET, WEIGHT):\n",
    "            feat_cols.add(c)\n",
    "\n",
    "feat_cols = sorted(feat_cols)\n",
    "print(f\"[cols] total feature columns = {len(feat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278b8fe",
   "metadata": {},
   "source": [
    "2. 写“天片”—把所有列拼上并立刻落盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01289383",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_PER_SHARD = 16\n",
    "\n",
    "# 左表（含 base 的 FEATURE_COLS）\n",
    "lf_left_base = (\n",
    "    lf_range\n",
    "    .select([*KEYS, TARGET, WEIGHT, *[pl.col(c) for c in FEATURE_COLS]])\n",
    ")\n",
    "\n",
    "# 为每个 stage 准备元信息（列名 + 文件大小，先拼小文件更省内存）\n",
    "stage_meta = []\n",
    "for p in STAGE_PATHS:\n",
    "    if not os.path.exists(p): \n",
    "        continue\n",
    "    scan = pl.scan_parquet(p).filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "    cols = [c for c in scan.collect_schema().names() if c not in KEYS]\n",
    "    if cols:\n",
    "        stage_meta.append({\"path\": p, \"cols\": cols, \"size\": os.path.getsize(p)})\n",
    "stage_meta.sort(key=lambda d: d[\"size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637955db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_shards(tag, days_list):\n",
    "    ds = sorted(days_list)\n",
    "    for i in range(0, len(ds), DAYS_PER_SHARD):\n",
    "        batch = set(ds[i:i+DAYS_PER_SHARD])\n",
    "\n",
    "        # 当前片的左表\n",
    "        lf_chunk = lf_left_base.filter(pl.col(\"date_id\").is_in(batch))\n",
    "        already = set(lf_chunk.collect_schema().names())\n",
    "\n",
    "        # 逐 stage 拼接（右表只取该片天数 + 只取未存在列）\n",
    "        for m in stage_meta:\n",
    "            need = [c for c in m[\"cols\"] if c not in already]\n",
    "            if not need:\n",
    "                continue\n",
    "            lf_add = (pl.scan_parquet(m[\"path\"])\n",
    "                        .filter(pl.col(\"date_id\").is_in(batch))\n",
    "                        .select([*KEYS, *need]))\n",
    "            lf_chunk = lf_chunk.join(lf_add, on=KEYS, how=\"left\")\n",
    "            already.update(need)\n",
    "\n",
    "        # 统一 float32 并落盘（列按 feat_cols 顺序对齐；片内缺失的列自然是 null）\n",
    "        present = [c for c in feat_cols if c in already]\n",
    "        cast_feats = [pl.col(c).cast(pl.Float32).alias(c) for c in present]\n",
    "        lf_out = lf_chunk.select([\n",
    "            *KEYS,\n",
    "            pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "            pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "            *cast_feats,\n",
    "        ])\n",
    "        out_path = f\"{SHARD_DIR}/{tag}_shard_{i//DAYS_PER_SHARD:04d}.parquet\"\n",
    "        lf_out.sink_parquet(out_path, compression=\"zstd\")\n",
    "        print(f\"[{tag}] wrote {out_path}\")\n",
    "        gc.collect()\n",
    "\n",
    "write_shards(\"train\", train_days)\n",
    "write_shards(\"val\",   val_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173b9e",
   "metadata": {},
   "source": [
    "3. 从 shards 构建 memmap 数组 （恒定内存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memmap_from_shards(glob_pat, feat_cols, prefix):\n",
    "    paths = sorted(glob.glob(glob_pat))\n",
    "    counts = [pl.scan_parquet(p).select(pl.len()).collect(streaming=True).item() for p in paths]\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    print(f\"[memmap] {glob_pat}: {len(paths)} files, {n_rows} rows, {n_feat} features\")\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    i = 0\n",
    "    for p, k in zip(paths, counts):\n",
    "        lf = pl.scan_parquet(p)\n",
    "        names = set(lf.collect_schema().names())\n",
    "        exprs = [\n",
    "            (pl.col(c).cast(pl.Float32).alias(c) if c in names\n",
    "             else pl.lit(None, dtype=pl.Float32).alias(c))\n",
    "            for c in feat_cols\n",
    "        ]\n",
    "        df = lf.select([\n",
    "            pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "            pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "            *exprs\n",
    "        ]).collect(streaming=True)\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(TARGET)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(WEIGHT)).to_numpy().ravel()\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush()\n",
    "    return X, y, w\n",
    "\n",
    "train_X, train_y, train_w = memmap_from_shards(f\"{SHARD_DIR}/train_shard_*.parquet\", feat_cols, f\"{OUT_DIR}/train\")\n",
    "val_X,   val_y,   val_w   = memmap_from_shards(f\"{SHARD_DIR}/val_shard_*.parquet\",   feat_cols, f\"{OUT_DIR}/val\")\n",
    "\n",
    "print(\"train shapes:\", train_X.shape, train_y.shape, train_w.shape)\n",
    "print(\"val   shapes:\", val_X.shape,   val_y.shape,   val_w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101d7bd",
   "metadata": {},
   "source": [
    "4. LightGBM 训练 + 重要性 （一次性全列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76652e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(train_X, label=train_y, weight=train_w,\n",
    "                     feature_name=feat_cols, free_raw_data=True)\n",
    "dval   = lgb.Dataset(val_X,   label=val_y,   weight=val_w,\n",
    "                     feature_name=feat_cols, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\", metric=\"None\",\n",
    "    num_threads=16, seed=42, deterministic=True, first_metric_only=True,\n",
    "    learning_rate=0.05, num_leaves=31, max_depth=-1, min_data_in_leaf=20,\n",
    "    # 内存友好\n",
    "    max_bin=63, bin_construct_sample_cnt=100_000, min_data_in_bin=3,\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params, dtrain,\n",
    "    valid_sets=[dval, dtrain], valid_names=[\"val\",\"train\"],\n",
    "    num_boost_round=1000, callbacks=[lgb.early_stopping(50)],\n",
    "    feval=lgb_wr2_eval,   # 你的评估函数\n",
    ")\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": model.feature_name(),\n",
    "    \"gain\": model.feature_importance(\"gain\"),\n",
    "    \"split\": model.feature_importance(\"split\"),\n",
    "}).sort_values(\"gain\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(imp.head(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.to_csv(f\"{OUT_DIR}/imp_1r.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7305fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.read_csv(f\"{OUT_DIR}/imp_1r.csv\")\n",
    "top_feats = imp.loc[imp.gain > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864927ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = top_feats['feature'].str.extract(r'^(feature_\\d{2}|responder_\\d)', expand=False)\n",
    "top_feats['family'] = fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f31e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_feats = top_feats.groupby('family').agg(\n",
    "    n = ('feature', 'count'),\n",
    "    gain = ('gain', 'sum'),\n",
    "    split = ('split', 'sum'),\n",
    ").reset_index().sort_values('gain', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03415ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fam_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_feat = fam_feats['family'].str.startswith('feature_', na=False)\n",
    "mask_resp = fam_feats[\"family\"].str.startswith(\"responder_\", na=False)\n",
    "features_only   = fam_feats[mask_feat].sort_values(\"gain\", ascending=False)\n",
    "responders_only = fam_feats[mask_resp].sort_values(\"gain\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = features_only['family'][:79] # select all\n",
    "selected_resps = responders_only['family'][:9] # select all\n",
    "\n",
    "# save the Series (no index)\n",
    "selected_features.to_csv(f\"{OUT_DIR}/selected_features_1r.csv\", index=False, header=False)\n",
    "selected_resps.to_csv(f\"{OUT_DIR}/selected_responders_1r.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230fcef",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集参数\n",
    "\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)] #FEATURE_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_features_1r.csv\", header=None).squeeze().tolist()\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)] #REP_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_responders_1r.csv\", header=None).squeeze().tolist()\n",
    "\n",
    "BASE_DIR = BLOB(cfg['paths']['clean'])\n",
    "\n",
    "\n",
    "BASE_PATH = [f\"{BASE_DIR}/final_clean.parquet\"]\n",
    "lf_base = pl.scan_parquet(BASE_PATH, storage_options=fs.storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99826859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test参数\n",
    "\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)] #FEATURE_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_features_1r.csv\", header=None).squeeze().tolist()\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)] #REP_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_responders_1r.csv\", header=None).squeeze().tolist()\n",
    "\n",
    "TEST_BASE_DIR = BLOB(cfg['paths']['clean'])\n",
    "\n",
    "TEST_BASE_PATH = [f\"{TEST_BASE_DIR}/test_final_clean.parquet\"]\n",
    "lf_test_base = pl.scan_parquet(TEST_BASE_PATH, storage_options=fs.storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- step 1: shard raw -------\n",
    "\n",
    "CORE_DAYS = 30                 # 每片保留的核心天数\n",
    "# 例：A 的最大 rolling=60 天；C 的最大 tick 窗口=968 → 1 天\n",
    "PAD_DAYS = 60   # 自行按实际 A/B/C 调整\n",
    "\n",
    "# 所有交易日\n",
    "days = (lf_base.select(pl.col(\"date_id\").unique().sort())\n",
    "            .collect(streaming=True)[\"date_id\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ef340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST \n",
    "\n",
    "# ------- step 1: shard raw -------\n",
    "\n",
    "CORE_DAYS = 30                 # 每片保留的核心天数\n",
    "# 例：A 的最大 rolling=60 天；C 的最大 tick 窗口=968 → 1 天\n",
    "PAD_DAYS = 60   # 自行按实际 A/B/C 调整\n",
    "\n",
    "# 所有交易日\n",
    "test_days = (lf_test_base.select(pl.col(\"date_id\").unique().sort())\n",
    "            .collect(streaming=True)[\"date_id\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be658d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分片输出\n",
    "for start_idx in range(PAD_DAYS, len(days), CORE_DAYS):\n",
    "    core_lo_idx = start_idx\n",
    "    core_hi_idx = min(start_idx + CORE_DAYS - 1, len(days) - 1)\n",
    "    pad_lo_idx  = core_lo_idx - PAD_DAYS\n",
    "\n",
    "    core_lo, core_hi = days[core_lo_idx], days[core_hi_idx]\n",
    "    pad_lo           = days[pad_lo_idx]\n",
    "\n",
    "    out_path = f\"{RAW_SHA_DIR}/raw_{core_lo}_{core_hi}.parquet\"\n",
    "\n",
    "    # 如需覆盖旧文件，先删除（可选）\n",
    "    if fs.exists(out_path.replace(\"az://\",\"\")):\n",
    "        fs.rm(out_path.replace(\"az://\",\"\"))\n",
    "\n",
    "    (lf_base\n",
    "        .filter(pl.col(\"date_id\").is_between(pad_lo, core_hi))\n",
    "        .select([*KEYS, *FEATURE_COLS, *REP_COLS, WEIGHT])\n",
    "        .sink_parquet(\n",
    "            out_path,\n",
    "            compression=\"zstd\",\n",
    "            storage_options=storage_options,   # ← 用你构造 FS 的那个 dict\n",
    "            statistics=True,                   # 可选：写入列统计\n",
    "            maintain_order=True,             # 可选：严格保持当前行序\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST \n",
    "\n",
    "RAW_SHA_DIR_B = BLOB(cfg['paths']['raw_shards'])\n",
    "os.makedirs(RAW_SHA_DIR_B, exist_ok=True)\n",
    "\n",
    "\n",
    "# 分片输出\n",
    "for start_idx in range(PAD_DAYS, len(test_days), CORE_DAYS):\n",
    "    core_lo_idx = start_idx\n",
    "    core_hi_idx = min(start_idx + CORE_DAYS - 1, len(test_days) - 1)\n",
    "    pad_lo_idx  = core_lo_idx - PAD_DAYS\n",
    "\n",
    "    core_lo, core_hi = test_days[core_lo_idx], test_days[core_hi_idx]\n",
    "    pad_lo           = test_days[pad_lo_idx]  # updated to use test_days\n",
    "\n",
    "    out_path = f\"{RAW_SHA_DIR_B}/test_raw_{core_lo}_{core_hi}.parquet\" # added \"test_\"#\n",
    "\n",
    "    # 如需覆盖旧文件，先删除（可选）\n",
    "    if fs.exists(out_path.replace(\"az://\",\"\")):\n",
    "        fs.rm(out_path.replace(\"az://\",\"\"))\n",
    "\n",
    "    (lf_test_base\n",
    "        .filter(pl.col(\"date_id\").is_between(pad_lo, core_hi))\n",
    "        .select([*KEYS, *FEATURE_COLS, *REP_COLS, WEIGHT])\n",
    "        .sink_parquet(\n",
    "            out_path,\n",
    "            compression=\"zstd\",\n",
    "            storage_options=storage_options,   # ← 用你构造 FS 的那个 dict\n",
    "            statistics=True,                   # 可选：写入列统计\n",
    "            maintain_order=True,             # 可选：严格保持当前行序\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- step 2: FE per raw shard (A+B once, C batched internally via C.batch_size) -------\n",
    "\n",
    "# params (tweak as needed)\n",
    "A = StageA(\n",
    "    tail_lags=(1,2,3,5),\n",
    "    tail_diffs=(1,2),\n",
    "    rolling_windows=(5,20,60),\n",
    "    prev_soft_days=7,\n",
    ")\n",
    "B = StageB(\n",
    "    ndays=5, stats_rep_cols=None, add_prev1_multirep=True,\n",
    "    batch_size=8, prev_soft_days=7, strict_k=False,\n",
    ")\n",
    "C = StageC(\n",
    "    lags=(1,16,64,100,256,968),\n",
    "    ret_periods=(1,16,64,100,256),\n",
    "    diff_periods=(1,16,64,100,256,968),\n",
    "    rz_windows=(16,64,100,256,968),\n",
    "    ewm_spans=(16,64,100,256,968),\n",
    "    keep_rmean_rstd=False,                 # 列多时建议关；需要时再开\n",
    "    cs_cols=None, cs_by=(\"date_id\",\"time_id\"),\n",
    "    prev_soft_days=7,\n",
    "    batch_size=4,                         # ← 内部分批规模；OOM 就降到 8/6\n",
    "    assume_sorted=True, cast_f32=True,\n",
    ")\n",
    "\n",
    "# ---- 数值排序，确保 raw_740_769 在 raw_1010_1039 之前 ----\n",
    "def numeric_key(p):\n",
    "    stem = Path(p).stem            # 'raw_1010_1039'\n",
    "    a, b = map(int, stem.split('_')[-2:])\n",
    "    return (a, b)\n",
    "\n",
    "\n",
    "\n",
    "# ---- 写出到 az://，去掉 mkdir；兼容 storage_options / cloud_options ----\n",
    "def trim_core(in_path: str, out_path_blob: str, lo: int, hi: int):\n",
    "    lf = (pl.scan_parquet(in_path)  # 本地读\n",
    "            .filter(pl.col(\"date_id\").is_between(lo, hi)))\n",
    "\n",
    "    opts = dict(compression=\"zstd\", statistics=True, maintain_order=True)\n",
    "\n",
    "    # 新版本大多用 storage_options；个别版本参数名是 cloud_options\n",
    "    try:\n",
    "        lf.sink_parquet(out_path_blob, storage_options=storage_options, **opts)\n",
    "    except TypeError:\n",
    "        lf.sink_parquet(out_path_blob, cloud_options=storage_options, **opts)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def fe_on_raw_shard(raw_path_blob: str, *, core_lo: int, core_hi: int,\n",
    "                    A=None, B=None, C=None,\n",
    "                    keys=KEYS, rep_cols=REP_COLS, feature_cols=FEATURE_COLS,\n",
    "                    fe_dir_blob=FE_SHA_DIR_B, tmp_root=TMP_ROOT):\n",
    "    # raw_path_blob 是 az://... 的路径\n",
    "    lf_slice = pl.scan_parquet(raw_path_blob, storage_options=storage_options)\n",
    "\n",
    "    # 临时工作区仍用本地，跑完删除\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_root) as tmp_dir:\n",
    "        res = run_staged_engineering(\n",
    "            lf_base=lf_slice,\n",
    "            keys=keys, rep_cols=rep_cols, feature_cols=feature_cols,\n",
    "            out_dir=tmp_dir, A=A, B=B, C=C\n",
    "        )\n",
    "\n",
    "        if \"A\" in res:\n",
    "            trim_core(res[\"A\"], f\"{fe_dir_blob}/A_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "        if \"B\" in res:\n",
    "            trim_core(res[\"B\"], f\"{fe_dir_blob}/B_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "        for op, paths in res.get(\"C\", {}).items():\n",
    "            for p in paths:\n",
    "                trim_core(p, f\"{fe_dir_blob}/C_{op}_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "# 列出 Blob 上的 raw_* 分片（无协议 → az://）\n",
    "raw_list_np = sorted(fs.glob(f\"{RAW_SHA_DIR_NP}/raw_*_*.parquet\"), key=numeric_key)\n",
    "for raw_np in raw_list_np:\n",
    "    lo, hi = map(int, Path(raw_np).stem.split(\"_\")[-2:])\n",
    "    raw_blob = \"az://\" + raw_np\n",
    "    fe_on_raw_shard(raw_blob, core_lo=lo, core_hi=hi, A=A, B=B, C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# ------- step 2: FE per raw shard (A+B once, C batched internally via C.batch_size) -------\n",
    "\n",
    "# params (tweak as needed)\n",
    "A = StageA(\n",
    "    tail_lags=(1,2,3,5),\n",
    "    tail_diffs=(1,2),\n",
    "    rolling_windows=(5,20,60),\n",
    "    prev_soft_days=7,\n",
    ")\n",
    "B = StageB(\n",
    "    ndays=5, stats_rep_cols=None, add_prev1_multirep=True,\n",
    "    batch_size=8, prev_soft_days=7, strict_k=False,\n",
    ")\n",
    "C = StageC(\n",
    "    lags=(1,16,64,100,256,968),\n",
    "    ret_periods=(1,16,64,100,256),\n",
    "    diff_periods=(1,16,64,100,256,968),\n",
    "    rz_windows=(16,64,100,256,968),\n",
    "    ewm_spans=(16,64,100,256,968),\n",
    "    keep_rmean_rstd=False,                 # 列多时建议关；需要时再开\n",
    "    cs_cols=None, cs_by=(\"date_id\",\"time_id\"),\n",
    "    prev_soft_days=7,\n",
    "    batch_size=4,                         # ← 内部分批规模；OOM 就降到 8/6\n",
    "    assume_sorted=True, cast_f32=True,\n",
    ")\n",
    "\n",
    "# ---- 数值排序，确保 raw_740_769 在 raw_1010_1039 之前 ----\n",
    "def numeric_key(p):\n",
    "    stem = Path(p).stem            # 'raw_1010_1039'\n",
    "    a, b = map(int, stem.split('_')[-2:])\n",
    "    return (a, b)\n",
    "\n",
    "\n",
    "\n",
    "# ---- 写出到 az://，去掉 mkdir；兼容 storage_options / cloud_options ----\n",
    "def trim_core(in_path: str, out_path_blob: str, lo: int, hi: int):\n",
    "    lf = (pl.scan_parquet(in_path)  # 本地读\n",
    "            .filter(pl.col(\"date_id\").is_between(lo, hi)))\n",
    "\n",
    "    opts = dict(compression=\"zstd\", statistics=True, maintain_order=True)\n",
    "\n",
    "    # 新版本大多用 storage_options；个别版本参数名是 cloud_options\n",
    "    try:\n",
    "        lf.sink_parquet(out_path_blob, storage_options=storage_options, **opts)\n",
    "    except TypeError:\n",
    "        lf.sink_parquet(out_path_blob, cloud_options=storage_options, **opts)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def fe_on_raw_shard(raw_path_blob: str, *, core_lo: int, core_hi: int,\n",
    "                    A=None, B=None, C=None,\n",
    "                    keys=KEYS, rep_cols=REP_COLS, feature_cols=FEATURE_COLS,\n",
    "                    fe_dir_blob=FE_SHA_DIR_B, tmp_root=TMP_ROOT):\n",
    "    # raw_path_blob 是 az://... 的路径\n",
    "    lf_slice = pl.scan_parquet(raw_path_blob, storage_options=storage_options)\n",
    "\n",
    "    # 临时工作区仍用本地，跑完删除\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_root) as tmp_dir:\n",
    "        res = run_staged_engineering(\n",
    "            lf_base=lf_slice,\n",
    "            keys=keys, rep_cols=rep_cols, feature_cols=feature_cols,\n",
    "            out_dir=tmp_dir, A=A, B=B, C=C\n",
    "        )\n",
    "\n",
    "        if \"A\" in res:\n",
    "            trim_core(res[\"A\"], f\"{fe_dir_blob}/test_A_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "        if \"B\" in res:\n",
    "            trim_core(res[\"B\"], f\"{fe_dir_blob}/test_B_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "        for op, paths in res.get(\"C\", {}).items(): # 这里最好改为path\n",
    "            for p in paths: # path (s)\n",
    "                trim_core(p, f\"{fe_dir_blob}/test_C_{op}_{core_lo}_{core_hi}.parquet\", core_lo, core_hi)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "# 列出 Blob 上的 raw_* 分片（无协议 → az://）\n",
    "raw_list_np = sorted(fs.glob(f\"{RAW_SHA_DIR_NP}/test_raw_*_*.parquet\"), key=numeric_key)\n",
    "for raw_np in raw_list_np:\n",
    "    lo, hi = map(int, Path(raw_np).stem.split(\"_\")[-2:])\n",
    "    raw_blob = \"az://\" + raw_np\n",
    "    fe_on_raw_shard(raw_blob, core_lo=lo, core_hi=hi, A=A, B=B, C=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117700e2",
   "metadata": {},
   "source": [
    "# 特征选择 二选（选参数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b84f17",
   "metadata": {},
   "source": [
    "至此，我们已经获得了全量数据的满参数下的各片特征量，接下来选小量样本 进行特征参数选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92abcb1",
   "metadata": {},
   "source": [
    "样本集_特征选择\n",
    "\n",
    "- 选择目标日期的数据集并合并\n",
    "- 划分训练集，验证集\n",
    "- 简单参数lgb训练\n",
    "- 特征重要性排序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a2a43",
   "metadata": {},
   "source": [
    "1. 基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列名与区间\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)]  #pd.read_csv(\"/mnt/data/js/config/selected_features_1r.csv\",\n",
    "DATE_LO, DATE_HI = 1200, 1400 # 指定训练/验证的 date_id 范围, 后期转为全部训练集\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1788951",
   "metadata": {},
   "source": [
    "2.枚举窗口 + 拿到训练、验证天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 Blob 列出全部 fe_shards 分片（返回不带协议的路径，要手动加 az://）\n",
    "\n",
    "fe_all = fs.glob(f\"{FE_SHA_DIR_B}/*_*.parquet\")\n",
    "fe_all = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in fe_all]\n",
    "\n",
    "# 按日期范围筛选\n",
    "wins = set()\n",
    "for p in fe_all:\n",
    "    base = p.split(\"/\")[-1]  # e.g. C_lags_1220_1249.parquet\n",
    "    lo = int(base.split(\"_\")[-2]); hi = int(base.split(\"_\")[-1].split(\".\")[0])\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "\n",
    "# 取得该区间实际天，并按 80/20 切分（天为单位，避免泄露）\n",
    "days = (pl.scan_parquet(BASE_PATH, storage_options=storage_options)\n",
    "        .filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "        .select(pl.col(\"date_id\").unique().sort())\n",
    "        .collect(streaming=True)[\"date_id\"].to_list())\n",
    "assert days, \"no days found in range\"\n",
    "cut = int(len(days)*0.8)\n",
    "train_days, val_days = set(days[:cut]), set(days[cut:])\n",
    "print(f\"days total={len(days)}  train={len(train_days)}  val={len(val_days)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643f6de",
   "metadata": {},
   "source": [
    "3. 按窗口拼接 (A + B + 所有 C_*) → 直接写 train/val 分片（无大表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "\n",
    "    # 基表 (带 TARGET/WEIGHT + 基础特征)\n",
    "    lf = (pl.scan_parquet(BASE_PATH, storage_options=storage_options)\n",
    "            .filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "            .select([*KEYS, TARGET, WEIGHT, *[pl.col(c) for c in FEATURE_COLS]]))\n",
    "\n",
    "    protocol = FE_SHA_DIR_B.split(\"://\", 1)[0] + \"://\"\n",
    "\n",
    "    # 若 fe_all 可能是无协议，先归一化\n",
    "    fe_set = {p if p.startswith(protocol) else protocol + p for p in fe_all}\n",
    "\n",
    "    fe_files = []\n",
    "    for name in (f\"A_{lo}_{hi}.parquet\", f\"B_{lo}_{hi}.parquet\"):\n",
    "        p = f\"{FE_SHA_DIR_B}/{name}\"  # 带协议\n",
    "        if p in fe_set:\n",
    "            fe_files.append(p)\n",
    "\n",
    "    # 同窗口所有 C_* 分片\n",
    "    c_files = fs.glob(f\"{FE_SHA_DIR_NP}/C_*_{lo}_{hi}.parquet\")  # 无协议\n",
    "    fe_files += [p if p.startswith(protocol) else protocol + p for p in sorted(c_files)]\n",
    "\n",
    "\n",
    "    # 逐个左连接（scan 到 Blob 时一定加 storage_options）\n",
    "    already = set(lf.collect_schema().names())\n",
    "    for fp in fe_files:\n",
    "        ds = pl.scan_parquet(fp, storage_options=storage_options)\n",
    "        names = ds.collect_schema().names()\n",
    "        add_cols = [c for c in names if c not in already]\n",
    "        if add_cols:\n",
    "            lf = lf.join(ds.select([*KEYS, *add_cols]), on=KEYS, how=\"left\")\n",
    "            already.update(add_cols)\n",
    "\n",
    "\n",
    "    # 统一 float32；注意：此窗口缺失的特征列**不写入**→ 之后 memmap 会自动补 None\n",
    "    feat_present = [c for c in already if c not in (*KEYS, TARGET, WEIGHT)]\n",
    "    select_exprs = [\n",
    "        *KEYS,\n",
    "        pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "        pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "        *[pl.col(c).cast(pl.Float32).alias(c) for c in feat_present],\n",
    "    ]\n",
    "    lf_win = lf.select(select_exprs)\n",
    "\n",
    "    # 直接流式写 train/val 分片（不 materialize 整个窗口）\n",
    "    out_train = f\"{FE_SHA_DIR_B}/train_{lo}_{hi}.parquet\"\n",
    "    out_val   = f\"{FE_SHA_DIR_B}/val_{lo}_{hi}.parquet\"\n",
    "    (lf_win.filter(pl.col(\"date_id\").is_in(list(train_days)))\n",
    "           .sink_parquet(out_train, compression=\"zstd\", storage_options=storage_options))\n",
    "    (lf_win.filter(pl.col(\"date_id\").is_in(list(val_days)))\n",
    "           .sink_parquet(out_val, compression=\"zstd\", storage_options=storage_options))\n",
    "\n",
    "    print(f\"[{lo}_{hi}] write: {os.path.basename(out_train)}, {os.path.basename(out_val)}\")\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d7d31",
   "metadata": {},
   "source": [
    "4.生成最终特征清单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, polars as pl\n",
    "\n",
    "# 任选一个训练分片当“列模板”\n",
    "ref = FE_SHA_DIR_B + \"/train_1190_1219.parquet\"\n",
    "names = pl.scan_parquet(ref, storage_options=storage_options).collect_schema().names()\n",
    "\n",
    "# 直接从这个分片拿特征列（已包含 base + engineered）\n",
    "feat_cols = [c for c in names if c not in (*KEYS, TARGET, WEIGHT)]\n",
    "print(f\"final feature list size = {len(feat_cols)}\")\n",
    "\n",
    "df_feat = pd.DataFrame({'feature': feat_cols})\n",
    "\n",
    "df_feat.to_csv(\"exp/v1/config/input_sets/input_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95fc78",
   "metadata": {},
   "source": [
    "5.构建memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "\n",
    "def shard2memmap(glob_pat, feat_cols, prefix, fs=None, storage_options=None,\n",
    "                 target_col=TARGET, weight_col=WEIGHT):\n",
    "    \"\"\"\n",
    "    将若干 parquet 分片顺序拼接为三份 memmap: X(float32), y(float32), w(float32)\n",
    "    - 支持本地路径和 az:// 路径（自动选择 glob 方式）\n",
    "    - 缺失的特征列用 None (=> NaN) 补齐，列顺序以 feat_cols 为准\n",
    "    - 会在同目录写出 {prefix}_{X,y,w}.float32.mmap 以及 {prefix}.meta.json\n",
    "    \"\"\"\n",
    "\n",
    "    def _is_az(p: str) -> bool:\n",
    "        return isinstance(p, str) and p.startswith(\"az://\")\n",
    "\n",
    "    def _glob_paths(pattern: str):\n",
    "        # 远程：用 fsspec.filesystem(\"az\").glob(无协议) → 再补上 az://\n",
    "        if _is_az(pattern):\n",
    "            assert fs is not None, \"fs must be provided for az:// glob\"\n",
    "            nopro = pattern[len(\"az://\"):]               # 去协议\n",
    "            listed = fs.glob(nopro)                       # 无协议\n",
    "            return [\"az://\" + p for p in sorted(listed)]  # 归一化为带协议\n",
    "        else:\n",
    "            import glob as _glob\n",
    "            return sorted(_glob.glob(pattern))            # 本地\n",
    "\n",
    "    def _scan(p: str):\n",
    "        if _is_az(p):\n",
    "            return pl.scan_parquet(p, storage_options=storage_options)\n",
    "        else:\n",
    "            return pl.scan_parquet(p)\n",
    "\n",
    "    # 1) 列出分片\n",
    "    paths = _glob_paths(glob_pat)\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No shards matched: {glob_pat}\")\n",
    "\n",
    "    # 2) 统计每个分片的行数（流式）\n",
    "    counts = []\n",
    "    for p in paths:\n",
    "        k = _scan(p).select(pl.len()).collect(streaming=True).item()\n",
    "        counts.append(int(k))\n",
    "\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    print(f\"[memmap] {glob_pat}: {len(paths)} files, {n_rows} rows, {n_feat} features\")\n",
    "\n",
    "    # 3) 分配 memmap（本地）\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    # 4) 逐分片流式写入\n",
    "    i = 0\n",
    "    for p, k in zip(paths, counts):\n",
    "        lf = _scan(p)\n",
    "        names = set(lf.collect_schema().names())\n",
    "        exprs = [\n",
    "            (pl.col(c).cast(pl.Float32).alias(c) if c in names\n",
    "             else pl.lit(None, dtype=pl.Float32).alias(c))\n",
    "            for c in feat_cols\n",
    "        ]\n",
    "        df = (lf.select([\n",
    "                pl.col(target_col).cast(pl.Float32).alias(target_col),\n",
    "                pl.col(weight_col).cast(pl.Float32).alias(weight_col),\n",
    "                *exprs\n",
    "             ])\n",
    "             .collect(streaming=True))\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(target_col)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(weight_col)).to_numpy().ravel()\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush()\n",
    "\n",
    "    # 5) 保存 meta\n",
    "    meta = {\"n_rows\": int(n_rows), \"n_feat\": int(n_feat), \"dtype\": \"float32\", \"ts\": time.time(),\n",
    "            \"features\": list(feat_cols), \"target\": target_col, \"weight\": weight_col}\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "    return X, y, w\n",
    "\n",
    "\n",
    "train_X, train_y, train_w = shard2memmap(\n",
    "    f\"{FE_SHA_DIR_B}/train_*.parquet\", feat_cols, f\"{MM_DIR}/train\",\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "val_X,   val_y,   val_w   = shard2memmap(\n",
    "    f\"{FE_SHA_DIR_B}/val_*.parquet\",   feat_cols, f\"{MM_DIR}/val\",\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "print(\"train shapes:\", train_X.shape, train_y.shape, train_w.shape)\n",
    "print(\"val   shapes:\", val_X.shape,   val_y.shape,   val_w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bf999",
   "metadata": {},
   "source": [
    "6.训练LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ddd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np\n",
    "\n",
    "def load_memmap(prefix, readonly=True):\n",
    "    meta_path = f\"{prefix}.meta.json\"\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"missing {meta_path}\")\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    dtype = np.dtype(meta.get(\"dtype\", \"float32\"))\n",
    "    n_rows, n_feat = int(meta[\"n_rows\"]), int(meta[\"n_feat\"])\n",
    "    mode = \"r\" if readonly else \"r+\"\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows,))\n",
    "    feat_cols = meta.get(\"features\")  # 用保存的列顺序，避免不一致\n",
    "    return X, y, w, feat_cols, meta\n",
    "\n",
    "def ensure_memmap(prefix, glob_pat, feat_cols, *, fs=None, storage_options=None,\n",
    "                  target_col=\"responder_6\", weight_col=\"weight\"):\n",
    "    try:\n",
    "        return load_memmap(prefix)\n",
    "    except FileNotFoundError:\n",
    "        X, y, w = shard2memmap(glob_pat, feat_cols, prefix, fs=fs, storage_options=storage_options,\n",
    "                               target_col=target_col, weight_col=weight_col)\n",
    "        # 重新加载一次，拿回 meta 和规范的 feat_cols\n",
    "        return load_memmap(prefix)\n",
    "\n",
    "# —— 使用：\n",
    "train_X, train_y, train_w, feat_cols, _ = ensure_memmap(\n",
    "    f\"{MM_DIR}/train\", f\"{FE_SHA_DIR_B}/train_*.parquet\", feat_cols,\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "val_X, val_y, val_w, _, _ = ensure_memmap(\n",
    "    f\"{MM_DIR}/val\", f\"{FE_SHA_DIR_B}/val_*.parquet\", feat_cols,\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "\n",
    "\n",
    "def weighted_r2_zero_mean(y_true, y_pred, weight):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "    weight = np.asarray(weight, dtype=np.float64).ravel()\n",
    "    num = np.sum(weight * (y_true - y_pred)**2)\n",
    "    den = np.sum(weight * (y_true**2))\n",
    "    return 0.0 if den <= 0 else 1.0 - (num/den)\n",
    "\n",
    "def lgb_wr2_eval(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    w = train_data.get_weight()\n",
    "    if w is None:\n",
    "        w = np.ones_like(y)\n",
    "    return (\"wr2\", weighted_r2_zero_mean(y, preds, w), True)\n",
    "\n",
    "dtrain = lgb.Dataset(train_X, label=train_y, weight=train_w, feature_name=feat_cols, free_raw_data=True)\n",
    "dval   = lgb.Dataset(val_X,   label=val_y,   weight=val_w,   feature_name=feat_cols, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"None\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    max_depth=10,\n",
    "    min_data_in_leaf=150,                # ← CHANGED: 20 -> 150，稳住叶子，省显存\n",
    "    num_threads=16,\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    first_metric_only=True,\n",
    "    max_bin=63,\n",
    "    bin_construct_sample_cnt=200_000,\n",
    "    min_data_in_bin=3,\n",
    "    device_type=\"gpu\",\n",
    "    gpu_device_id=0,                     # ← ADDED: 明确使用第0块GPU\n",
    "    feature_fraction=0.7,                # ← ADDED: 列采样，提速+防过拟合\n",
    "    bagging_fraction=0.7,                # ← ADDED: 行采样\n",
    "    bagging_freq=1,                      # ← ADDED\n",
    "    lambda_l2=3.0,                       # ← ADDED: 稳定分裂\n",
    "    # extra_trees=True,                  # （可选）再稳一点时打开\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params, dtrain,\n",
    "    valid_sets=[dval],                   # ← CHANGED: 只监控验证集，更快\n",
    "    valid_names=[\"val\"],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50)],\n",
    "    feval=lgb_wr2_eval,\n",
    ")\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": model.feature_name(),\n",
    "    \"gain\": model.feature_importance(\"gain\"),\n",
    "    \"split\": model.feature_importance(\"split\"),\n",
    "}).sort_values(\"gain\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(imp.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_imp = imp[imp.gain > 0]\n",
    "top_imp = imp[imp.split > 5]\n",
    "len(top_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95eb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_imp.to_csv(f\"exp/v1/config/input_sets/top_imp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0cf82",
   "metadata": {},
   "source": [
    "# 去共线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATHS = [\"/mnt/data/js/final_clean.parquet\"]\n",
    "KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "TARGET = \"responder_6\"\n",
    "FEATURE_COLS = pd.read_csv('/home/admin_ml/Jackson/projects/selected_features.csv')['family'].tolist()\n",
    "REP_COLS = pd.read_csv('/home/admin_ml/Jackson/projects/selected_resps.csv')['family'].tolist()\n",
    "\n",
    "lf_base = pl.scan_parquet(PARQUET_PATHS).select(KEYS+FEATURE_COLS+REP_COLS)\n",
    "\n",
    "\n",
    "lf_slice = lf_base.filter((pl.col(\"date_id\") >= 1200) & (pl.col(\"date_id\") <= 1400))\n",
    "\n",
    "PARAMS = dict(\n",
    "        prev_soft_days=7,\n",
    "        tail_lags=(2, 5, 20, 40, 100),\n",
    "        tail_diffs=(2, 5,),\n",
    "        rolling_windows=(5, 20),\n",
    "        same_time_ndays=5,\n",
    "        strict_k=False,\n",
    "        hist_lags=(1,2,5,10,20,100),\n",
    "        ret_periods=(1,5,20),\n",
    "        diff_periods=(1,5),\n",
    "        rz_windows=(5,20),\n",
    "        ewm_spans=(5,40,100),\n",
    "        cs_cols=None,       # <- keep this small to avoid blow-up\n",
    "    )\n",
    "\n",
    "lf_eng = run_engineering_on_slice(lf_slice, **PARAMS)\n",
    "\n",
    "feats = pd.read_csv(\"/home/admin_ml/Jackson/projects/final_fi_mean.csv\")[\"feature\"].tolist()\n",
    "\n",
    "lf_small = lf_eng.select(feats[:500])\n",
    "lf_small.collect(streaming=True).write_parquet(\"/mnt/data/js/X_ready.parquet\", compression=\"zstd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7dbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lf = pl.scan_parquet(\"/mnt/data/js/X_ready.parquet\")\n",
    "\n",
    "df = lf.collect(streaming=True).to_pandas()\n",
    "\n",
    "# Correlation (pairwise complete obs) + guard on min observations\n",
    "min_obs = max(50, int(0.3 * len(df)))  # tweak as you like\n",
    "C = df.corr(method=\"pearson\", min_periods=min_obs).abs().fillna(0.0)\n",
    "\n",
    "# Ensure to align rows & cols to the same (priority) order, fill any NaNs with 0\n",
    "order = feats\n",
    "C = C.reindex(index=order, columns=order).fillna(0.0).copy()\n",
    "\n",
    "\n",
    "# --- Prepare NumPy array for the greedy loop ---\n",
    "A = C.values\n",
    "np.fill_diagonal(A, 0.0)  # ensure the value is smaller than thresh, so the feature won't be dropped by value'1'\n",
    "m = len(order)\n",
    "\n",
    "# --- Greedy de-correlation (keep-by-priority, drop neighbors) ---\n",
    "THRESH = 0.97\n",
    "keep_mask = np.ones(m, dtype=bool)\n",
    "\n",
    "for i in range(m):\n",
    "    if not keep_mask[i]:\n",
    "        continue  # already removed by a higher-priority pick\n",
    "    # only check j > i (upper triangle) among still-active features\n",
    "    active_slice = keep_mask[i+1:]\n",
    "    drop = (A[i, i+1:] >= THRESH) & active_slice\n",
    "    active_slice[drop] = False  # marks into keep_mask[i+1:] via view\n",
    "keep = [order[i] for i in range(m) if keep_mask[i]]\n",
    "\n",
    "\n",
    "pd.DataFrame({'feature': keep}).to_csv(\"final_selected_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4ba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fca06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93ac3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5d57402",
   "metadata": {},
   "source": [
    "# 全数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a1d1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'az://jackson/js_exp/clean/final_clean.parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{BASE_DIR}/final_clean.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b127982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 92)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>weight</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_31</th><th>feature_32</th><th>&hellip;</th><th>feature_51</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_0</th><th>responder_1</th><th>responder_2</th><th>responder_3</th><th>responder_4</th><th>responder_5</th><th>responder_6</th><th>responder_7</th><th>responder_8</th></tr><tr><td>i8</td><td>i16</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>680</td><td>0</td><td>2.29816</td><td>0.851814</td><td>1.197591</td><td>0.219422</td><td>0.411698</td><td>2.057359</td><td>-0.542597</td><td>-3.4331</td><td>-1.090165</td><td>0.151888</td><td>11.0</td><td>7.0</td><td>76.0</td><td>-0.97142</td><td>0.670215</td><td>-0.502896</td><td>-0.519751</td><td>-0.070096</td><td>-0.5044</td><td>-1.308236</td><td>-2.120128</td><td>1.747068</td><td>-0.219661</td><td>0.791602</td><td>0.114922</td><td>-0.311672</td><td>0.156548</td><td>1.476346</td><td>1.301341</td><td>1.235173</td><td>-0.259882</td><td>-0.30125</td><td>-0.264237</td><td>0.233658</td><td>&hellip;</td><td>0.322135</td><td>-0.244606</td><td>0.177445</td><td>-2.273168</td><td>0.211338</td><td>-1.627507</td><td>1.155271</td><td>-0.019618</td><td>-3.847984</td><td>-2.230864</td><td>-0.171579</td><td>-0.273791</td><td>-0.301876</td><td>-0.432933</td><td>-1.215778</td><td>-1.670469</td><td>-0.637963</td><td>0.803874</td><td>-0.21269</td><td>-0.764702</td><td>0.435278</td><td>-0.619145</td><td>-0.016643</td><td>-0.02328</td><td>-0.021034</td><td>-0.045094</td><td>-0.178144</td><td>-0.1951</td><td>-0.304665</td><td>0.164485</td><td>-0.205231</td><td>0.191064</td><td>-1.413209</td><td>0.375675</td><td>0.929775</td><td>-1.574939</td><td>1.101371</td></tr><tr><td>0</td><td>680</td><td>1</td><td>2.29816</td><td>0.344538</td><td>0.735081</td><td>0.013578</td><td>0.61738</td><td>1.956999</td><td>-0.371554</td><td>-0.526306</td><td>-0.684286</td><td>0.155305</td><td>11.0</td><td>7.0</td><td>76.0</td><td>-0.810169</td><td>1.103803</td><td>-0.268897</td><td>-0.519751</td><td>-0.320673</td><td>-0.5044</td><td>-0.762257</td><td>-2.215312</td><td>1.747068</td><td>-0.219661</td><td>0.791602</td><td>0.114922</td><td>-0.311672</td><td>0.156548</td><td>1.476346</td><td>1.301341</td><td>1.235173</td><td>-0.259882</td><td>-0.30125</td><td>-0.264237</td><td>0.233658</td><td>&hellip;</td><td>-0.512957</td><td>-0.244606</td><td>0.177445</td><td>-1.259565</td><td>0.211338</td><td>-2.49033</td><td>0.259405</td><td>-0.019618</td><td>-2.635671</td><td>-2.046605</td><td>-0.171579</td><td>-0.305304</td><td>-0.346962</td><td>-0.363018</td><td>-1.361503</td><td>-2.374111</td><td>-0.566751</td><td>1.22803</td><td>-0.274947</td><td>-1.316323</td><td>1.123081</td><td>-0.398956</td><td>-0.016643</td><td>-0.02328</td><td>-0.079538</td><td>-0.106091</td><td>-0.319045</td><td>-0.286955</td><td>0.260829</td><td>0.497995</td><td>0.709419</td><td>0.324715</td><td>-0.794916</td><td>0.437495</td><td>0.089887</td><td>-1.605929</td><td>0.104144</td></tr><tr><td>0</td><td>680</td><td>2</td><td>2.29816</td><td>-0.180962</td><td>1.383893</td><td>-0.650321</td><td>0.700879</td><td>2.427377</td><td>-0.535128</td><td>-0.81285</td><td>-0.7363</td><td>0.116907</td><td>11.0</td><td>7.0</td><td>76.0</td><td>-0.991004</td><td>1.118573</td><td>-0.340604</td><td>-0.519751</td><td>-0.310225</td><td>-0.5044</td><td>-0.147581</td><td>-2.163665</td><td>1.747068</td><td>-0.219661</td><td>0.791602</td><td>0.114922</td><td>-0.311672</td><td>0.156548</td><td>1.476346</td><td>1.301341</td><td>1.235173</td><td>-0.259882</td><td>-0.30125</td><td>-0.264237</td><td>0.233658</td><td>&hellip;</td><td>-0.799781</td><td>-0.244606</td><td>0.177445</td><td>-0.404231</td><td>0.211338</td><td>-2.945363</td><td>0.656708</td><td>-0.019618</td><td>-2.228918</td><td>-2.241239</td><td>-0.171579</td><td>-0.295761</td><td>-0.35494</td><td>-0.405964</td><td>-0.669037</td><td>-1.851066</td><td>-0.997381</td><td>0.942939</td><td>-0.204669</td><td>-1.064232</td><td>0.616298</td><td>-0.293038</td><td>-0.016643</td><td>-0.02328</td><td>0.05686</td><td>0.015116</td><td>-0.22286</td><td>-0.204045</td><td>0.398035</td><td>0.52747</td><td>0.488063</td><td>0.751176</td><td>-0.785524</td><td>1.308435</td><td>0.250879</td><td>-1.45394</td><td>1.760168</td></tr><tr><td>0</td><td>680</td><td>3</td><td>2.29816</td><td>-0.124249</td><td>1.744584</td><td>0.897987</td><td>0.096003</td><td>2.132334</td><td>-0.851425</td><td>-2.684773</td><td>-1.338934</td><td>0.151086</td><td>11.0</td><td>7.0</td><td>76.0</td><td>-1.11294</td><td>1.4363</td><td>-0.206028</td><td>-0.519751</td><td>-0.39899</td><td>-0.5044</td><td>-1.310143</td><td>-1.819511</td><td>1.747068</td><td>-0.219661</td><td>0.791602</td><td>0.114922</td><td>-0.311672</td><td>0.156548</td><td>1.476346</td><td>1.301341</td><td>1.235173</td><td>-0.259882</td><td>-0.30125</td><td>-0.264237</td><td>0.233658</td><td>&hellip;</td><td>-0.155125</td><td>-0.244606</td><td>0.177445</td><td>-1.505502</td><td>0.211338</td><td>-1.772148</td><td>0.813329</td><td>-0.019618</td><td>-2.694696</td><td>-2.044638</td><td>-0.171579</td><td>-0.321374</td><td>-0.258883</td><td>-0.379521</td><td>-0.746425</td><td>-2.052708</td><td>-0.812054</td><td>1.020951</td><td>-0.088977</td><td>-0.992124</td><td>0.737115</td><td>-0.358254</td><td>-0.016643</td><td>-0.02328</td><td>-0.056656</td><td>-0.061023</td><td>-0.228405</td><td>-0.303751</td><td>-0.308972</td><td>0.194933</td><td>-0.048652</td><td>0.191667</td><td>-1.004579</td><td>0.555013</td><td>0.56471</td><td>-1.449821</td><td>1.333068</td></tr><tr><td>0</td><td>680</td><td>4</td><td>2.29816</td><td>0.814995</td><td>1.38898</td><td>-0.253808</td><td>0.489444</td><td>2.055804</td><td>-0.897863</td><td>-0.891241</td><td>-1.131095</td><td>0.17518</td><td>11.0</td><td>7.0</td><td>76.0</td><td>-1.03724</td><td>0.772707</td><td>-0.237228</td><td>-0.519751</td><td>-0.206644</td><td>-0.348021</td><td>-1.22889</td><td>-1.41934</td><td>1.747068</td><td>-0.219661</td><td>0.791602</td><td>0.114922</td><td>-0.311672</td><td>0.156548</td><td>1.476346</td><td>1.301341</td><td>1.235173</td><td>-0.259882</td><td>-0.30125</td><td>-0.264237</td><td>0.233658</td><td>&hellip;</td><td>-0.509942</td><td>-0.244606</td><td>0.177445</td><td>-0.196582</td><td>0.211338</td><td>-2.116526</td><td>0.851539</td><td>-0.019618</td><td>-3.177497</td><td>-2.24501</td><td>-0.171579</td><td>-0.291239</td><td>-0.331689</td><td>-0.280069</td><td>-1.394332</td><td>-1.034399</td><td>-0.762234</td><td>0.542012</td><td>-0.144765</td><td>-1.023163</td><td>0.307331</td><td>-0.37056</td><td>-0.016643</td><td>-0.02328</td><td>-0.074767</td><td>-0.091099</td><td>-0.191449</td><td>-0.263896</td><td>-0.543977</td><td>0.164482</td><td>-0.485082</td><td>0.173833</td><td>-1.178417</td><td>1.133403</td><td>0.435787</td><td>-1.653541</td><td>2.013243</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 92)\n",
       "┌───────────┬─────────┬─────────┬─────────┬───┬─────────────┬────────────┬────────────┬────────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ weight  ┆ … ┆ responder_5 ┆ responder_ ┆ responder_ ┆ responder_ │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ ---     ┆   ┆ ---         ┆ 6          ┆ 7          ┆ 8          │\n",
       "│ i8        ┆ i16     ┆ i16     ┆ f32     ┆   ┆ f32         ┆ ---        ┆ ---        ┆ ---        │\n",
       "│           ┆         ┆         ┆         ┆   ┆             ┆ f32        ┆ f32        ┆ f32        │\n",
       "╞═══════════╪═════════╪═════════╪═════════╪═══╪═════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0         ┆ 680     ┆ 0       ┆ 2.29816 ┆ … ┆ 0.375675    ┆ 0.929775   ┆ -1.574939  ┆ 1.101371   │\n",
       "│ 0         ┆ 680     ┆ 1       ┆ 2.29816 ┆ … ┆ 0.437495    ┆ 0.089887   ┆ -1.605929  ┆ 0.104144   │\n",
       "│ 0         ┆ 680     ┆ 2       ┆ 2.29816 ┆ … ┆ 1.308435    ┆ 0.250879   ┆ -1.45394   ┆ 1.760168   │\n",
       "│ 0         ┆ 680     ┆ 3       ┆ 2.29816 ┆ … ┆ 0.555013    ┆ 0.56471    ┆ -1.449821  ┆ 1.333068   │\n",
       "│ 0         ┆ 680     ┆ 4       ┆ 2.29816 ┆ … ┆ 1.133403    ┆ 0.435787   ┆ -1.653541  ┆ 2.013243   │\n",
       "└───────────┴─────────┴─────────┴─────────┴───┴─────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lx = pl.scan_parquet(f\"{BASE_DIR}/final_clean.parquet\", storage_options=fs.storage_options)\n",
    "\n",
    "lx.limit(5).collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_LO, DATE_HI = 680, 1529 # 指定训练/验证的 date_id 范围, 后期转为全部训练集\n",
    "\n",
    "BASE_PATH = f\"{BASE_DIR}/final_clean.parquet\"\n",
    "\n",
    "# 从 Blob 列出全部 fe_shards 分片（返回不带协议的路径，要手动加 az://）\n",
    "fe_all = fs.glob(f\"{FE_SHA_DIR_B}/*_*.parquet\")\n",
    "fe_all = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in fe_all]\n",
    "\n",
    "# 按日期范围筛选\n",
    "wins = set()\n",
    "for p in fe_all:\n",
    "    base = p.split(\"/\")[-1]  # e.g. C_lags_1220_1249.parquet\n",
    "    lo = int(base.split(\"_\")[-2]); hi = int(base.split(\"_\")[-1].split(\".\")[0])\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "\n",
    "# 取得该区间实际天，并按 80/20 切分（天为单位，避免泄露）\n",
    "days = (pl.scan_parquet(BASE_PATH, storage_options=storage_options)\n",
    "        .filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "        .select(pl.col(\"date_id\").unique().sort())\n",
    "        .collect(streaming=True)[\"date_id\"].to_list())\n",
    "assert days, \"no days found in range\"\n",
    "cut = int(len(days)*0.8)\n",
    "train_days, val_days = set(days[:cut]), set(days[cut:])\n",
    "print(f\"days total={len(days)}  train={len(train_days)}  val={len(val_days)}\")\n",
    "\n",
    "\n",
    "\n",
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "\n",
    "    # 基表 (带 TARGET/WEIGHT + 基础特征)\n",
    "    lf = (pl.scan_parquet(BASE_PATH, storage_options=storage_options)\n",
    "            .filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "            .select([*KEYS, TARGET, WEIGHT, *[pl.col(c) for c in FEATURE_COLS]]))\n",
    "\n",
    "    protocol = FE_SHA_DIR_B.split(\"://\", 1)[0] + \"://\"\n",
    "\n",
    "    # 若 fe_all 可能是无协议，先归一化\n",
    "    fe_set = {p if p.startswith(protocol) else protocol + p for p in fe_all}\n",
    "\n",
    "    fe_files = []\n",
    "    for name in (f\"A_{lo}_{hi}.parquet\", f\"B_{lo}_{hi}.parquet\"):\n",
    "        p = f\"{FE_SHA_DIR_B}/{name}\"  # 带协议\n",
    "        if p in fe_set:\n",
    "            fe_files.append(p)\n",
    "\n",
    "    # 同窗口所有 C_* 分片\n",
    "    c_files = fs.glob(f\"{FE_SHA_DIR_NP}/C_*_{lo}_{hi}.parquet\")  # 无协议\n",
    "    fe_files += [p if p.startswith(protocol) else protocol + p for p in sorted(c_files)]\n",
    "\n",
    "\n",
    "    # 逐个左连接（scan 到 Blob 时一定加 storage_options）\n",
    "    already = set(lf.collect_schema().names())\n",
    "    for fp in fe_files:\n",
    "        ds = pl.scan_parquet(fp, storage_options=storage_options)\n",
    "        names = ds.collect_schema().names()\n",
    "        add_cols = [c for c in names if c not in already]\n",
    "        if add_cols:\n",
    "            lf = lf.join(ds.select([*KEYS, *add_cols]), on=KEYS, how=\"left\")\n",
    "            already.update(add_cols)\n",
    "\n",
    "\n",
    "    # 统一 float32；注意：此窗口缺失的特征列**不写入**→ 之后 memmap 会自动补 None\n",
    "    feat_present = [c for c in already if c not in (*KEYS, TARGET, WEIGHT)]\n",
    "    select_exprs = [\n",
    "        *KEYS,\n",
    "        pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "        pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "        *[pl.col(c).cast(pl.Float32).alias(c) for c in feat_present],\n",
    "    ]\n",
    "    lf_win = lf.select(select_exprs)\n",
    "\n",
    "    # 直接流式写 train/val 分片（不 materialize 整个窗口）\n",
    "    out_train = f\"{FE_SHA_DIR_B}/train_{lo}_{hi}.parquet\"\n",
    "    out_val   = f\"{FE_SHA_DIR_B}/val_{lo}_{hi}.parquet\"\n",
    "    (lf_win.filter(pl.col(\"date_id\").is_in(list(train_days)))\n",
    "           .sink_parquet(out_train, compression=\"zstd\", storage_options=storage_options))\n",
    "    (lf_win.filter(pl.col(\"date_id\").is_in(list(val_days)))\n",
    "           .sink_parquet(out_val, compression=\"zstd\", storage_options=storage_options))\n",
    "\n",
    "    print(f\"[{lo}_{hi}] write: {os.path.basename(out_train)}, {os.path.basename(out_val)}\")\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, time\n",
    "\n",
    "\n",
    "feat_cols = pd.read_csv('exp/v1/config/input_sets/top_imp.csv')\n",
    "feat_cols = feat_cols['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34c818a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8986fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [*feat_cols, 'time_id']      # 或 feat_cols + ['time_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eef55bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f3a19bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[memmap] az://jackson/js_exp/exp/v1/fe_shards/train_*.parquet: 27 files, 21216624 rows, 513 features\n",
      "[memmap] az://jackson/js_exp/exp/v1/fe_shards/val_*.parquet: 27 files, 6140024 rows, 513 features\n",
      "train shapes: (21216624, 513) (21216624,) (21216624,)\n",
      "val   shapes: (6140024, 513) (6140024,) (6140024,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def shard2memmap(glob_pat, feat_cols, prefix, fs=None, storage_options=None,\n",
    "                 target_col=TARGET, weight_col=WEIGHT):\n",
    "    \"\"\"\n",
    "    将若干 parquet 分片顺序拼接为三份 memmap: X(float32), y(float32), w(float32)\n",
    "    - 支持本地路径和 az:// 路径（自动选择 glob 方式）\n",
    "    - 缺失的特征列用 None (=> NaN) 补齐，列顺序以 feat_cols 为准\n",
    "    - 会在同目录写出 {prefix}_{X,y,w}.float32.mmap 以及 {prefix}.meta.json\n",
    "    \"\"\"\n",
    "\n",
    "    def _is_az(p: str) -> bool:\n",
    "        return isinstance(p, str) and p.startswith(\"az://\")\n",
    "\n",
    "    def _glob_paths(pattern: str):\n",
    "        # 远程：用 fsspec.filesystem(\"az\").glob(无协议) → 再补上 az://\n",
    "        if _is_az(pattern):\n",
    "            assert fs is not None, \"fs must be provided for az:// glob\"\n",
    "            nopro = pattern[len(\"az://\"):]               # 去协议\n",
    "            listed = fs.glob(nopro)                       # 无协议\n",
    "            return [\"az://\" + p for p in sorted(listed)]  # 归一化为带协议\n",
    "        else:\n",
    "            import glob as _glob\n",
    "            return sorted(_glob.glob(pattern))            # 本地\n",
    "\n",
    "    def _scan(p: str):\n",
    "        if _is_az(p):\n",
    "            return pl.scan_parquet(p, storage_options=storage_options)\n",
    "        else:\n",
    "            return pl.scan_parquet(p)\n",
    "\n",
    "    # 1) 列出分片\n",
    "    paths = _glob_paths(glob_pat)\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No shards matched: {glob_pat}\")\n",
    "\n",
    "    # 2) 统计每个分片的行数（流式）\n",
    "    counts = []\n",
    "    for p in paths:\n",
    "        k = _scan(p).select(pl.len()).collect(streaming=True).item()\n",
    "        counts.append(int(k))\n",
    "\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    print(f\"[memmap] {glob_pat}: {len(paths)} files, {n_rows} rows, {n_feat} features\")\n",
    "\n",
    "    # 3) 分配 memmap（本地）\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    # 4) 逐分片流式写入\n",
    "    i = 0\n",
    "    for p, k in zip(paths, counts):\n",
    "        lf = _scan(p)\n",
    "        names = set(lf.collect_schema().names())\n",
    "        exprs = [\n",
    "            (pl.col(c).cast(pl.Float32).alias(c) if c in names\n",
    "             else pl.lit(None, dtype=pl.Float32).alias(c))\n",
    "            for c in feat_cols\n",
    "        ]\n",
    "        df = (lf.select([\n",
    "                pl.col(target_col).cast(pl.Float32).alias(target_col),\n",
    "                pl.col(weight_col).cast(pl.Float32).alias(weight_col),\n",
    "                *exprs\n",
    "             ])\n",
    "             .collect(streaming=True))\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(target_col)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(weight_col)).to_numpy().ravel()\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush()\n",
    "\n",
    "    # 5) 保存 meta\n",
    "    meta = {\"n_rows\": int(n_rows), \"n_feat\": int(n_feat), \"dtype\": \"float32\", \"ts\": time.time(),\n",
    "            \"features\": list(feat_cols), \"target\": target_col, \"weight\": weight_col}\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "    return X, y, w\n",
    "\n",
    "\n",
    "train_X, train_y, train_w = shard2memmap(\n",
    "    f\"{FE_SHA_DIR_B}/train_*.parquet\", feat_cols, f\"{MM_DIR}/train\",\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "val_X,   val_y,   val_w   = shard2memmap(\n",
    "    f\"{FE_SHA_DIR_B}/val_*.parquet\",   feat_cols, f\"{MM_DIR}/val\",\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "print(\"train shapes:\", train_X.shape, train_y.shape, train_w.shape)\n",
    "print(\"val   shapes:\", val_X.shape,   val_y.shape,   val_w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Although \"deterministic\" is set, the results ran by GPU may be non-deterministic.\n",
      "[LightGBM] [Warning] Although \"deterministic\" is set, the results ran by GPU may be non-deterministic.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 32319\n",
      "[LightGBM] [Info] Number of data points in the train set: 21216624, number of used features: 513\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 427 dense feature groups (8660.04 MB) transferred to GPU in 2.003924 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] Although \"deterministic\" is set, the results ran by GPU may be non-deterministic.\n",
      "[LightGBM] [Info] Start training from score -0.003603\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tval's wr2: 0.00757362\n",
      "[100]\tval's wr2: 0.0106474\n",
      "[150]\tval's wr2: 0.0124919\n",
      "[200]\tval's wr2: 0.0138949\n",
      "[250]\tval's wr2: 0.0146271\n",
      "[300]\tval's wr2: 0.0152303\n",
      "[350]\tval's wr2: 0.0154838\n",
      "[400]\tval's wr2: 0.0157228\n",
      "[450]\tval's wr2: 0.0159548\n",
      "[500]\tval's wr2: 0.0160824\n",
      "[550]\tval's wr2: 0.0162302\n",
      "[600]\tval's wr2: 0.0162849\n",
      "[650]\tval's wr2: 0.0163209\n",
      "[700]\tval's wr2: 0.0162854\n",
      "[750]\tval's wr2: 0.0163108\n",
      "[800]\tval's wr2: 0.0163065\n",
      "[850]\tval's wr2: 0.0162816\n",
      "Early stopping, best iteration is:\n",
      "[681]\tval's wr2: 0.0163726\n",
      "                     feature           gain  split\n",
      "0                 feature_06  793436.010590    396\n",
      "1                 feature_36  294038.092285    351\n",
      "2                 feature_59  251581.238411    464\n",
      "3                 feature_07  249577.963425    403\n",
      "4                 feature_04  238318.042381    358\n",
      "5                 feature_60  187469.625641    438\n",
      "6                    time_id  165774.206116    402\n",
      "7         feature_08__ewm968  137445.837059    409\n",
      "8          feature_54__ewm16  136017.808594    293\n",
      "9          feature_05__rz968  135649.156036    365\n",
      "10         feature_07__ewm16  135602.394096    364\n",
      "11         feature_04__ewm16  132331.238907    248\n",
      "12         feature_68__ewm16  130417.296204    321\n",
      "13         feature_07__rz968  129061.995247    308\n",
      "14                feature_13  115595.503021    190\n",
      "15        feature_08__ewm256  115574.116745    335\n",
      "16        feature_60__diff16  114164.917145    224\n",
      "17         feature_56__ewm16  109551.279739    221\n",
      "18         feature_08__ewm64  108233.066040    313\n",
      "19         feature_04__rz968  107539.648582    177\n",
      "20         feature_38__rz968  107077.967094    268\n",
      "21        feature_38__ewm968  105494.015114    278\n",
      "22         feature_61__ret64  105364.678696    296\n",
      "23         feature_51__ewm16  103072.277908    217\n",
      "24        feature_05__ewm968  102510.161285    305\n",
      "25         feature_03__ewm64  101669.187820    264\n",
      "26  responder_8_prevday_mean   99716.534645    224\n",
      "27        feature_02__ewm100   97212.235809    258\n",
      "28        feature_08__ewm100   96971.111946    278\n",
      "29        feature_61__diff16   96013.528915    257\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np\n",
    "\n",
    "def load_memmap(prefix, readonly=True):\n",
    "    meta_path = f\"{prefix}.meta.json\"\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"missing {meta_path}\")\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    dtype = np.dtype(meta.get(\"dtype\", \"float32\"))\n",
    "    n_rows, n_feat = int(meta[\"n_rows\"]), int(meta[\"n_feat\"])\n",
    "    mode = \"r\" if readonly else \"r+\"\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=dtype, mode=mode, shape=(n_rows,))\n",
    "    feat_cols = meta.get(\"features\")  # 用保存的列顺序，避免不一致\n",
    "    return X, y, w, feat_cols, meta\n",
    "\n",
    "def ensure_memmap(prefix, glob_pat, feat_cols, *, fs=None, storage_options=None,\n",
    "                  target_col=\"responder_6\", weight_col=\"weight\"):\n",
    "    try:\n",
    "        return load_memmap(prefix)\n",
    "    except FileNotFoundError:\n",
    "        X, y, w = shard2memmap(glob_pat, feat_cols, prefix, fs=fs, storage_options=storage_options,\n",
    "                               target_col=target_col, weight_col=weight_col)\n",
    "        # 重新加载一次，拿回 meta 和规范的 feat_cols\n",
    "        return load_memmap(prefix)\n",
    "\n",
    "# —— 使用：\n",
    "train_X, train_y, train_w, feat_cols, _ = ensure_memmap(\n",
    "    f\"{MM_DIR}/train\", f\"{FE_SHA_DIR_B}/train_*.parquet\", feat_cols,\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "val_X, val_y, val_w, _, _ = ensure_memmap(\n",
    "    f\"{MM_DIR}/val\", f\"{FE_SHA_DIR_B}/val_*.parquet\", feat_cols,\n",
    "    fs=fs, storage_options=storage_options)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def weighted_r2_zero_mean(y_true, y_pred, weight):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "    weight = np.asarray(weight, dtype=np.float64).ravel()\n",
    "    num = np.sum(weight * (y_true - y_pred)**2)\n",
    "    den = np.sum(weight * (y_true**2))\n",
    "    return 0.0 if den <= 0 else 1.0 - (num/den)\n",
    "\n",
    "def lgb_wr2_eval(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    w = train_data.get_weight()\n",
    "    if w is None:\n",
    "        w = np.ones_like(y)\n",
    "    return (\"wr2\", weighted_r2_zero_mean(y, preds, w), True)\n",
    "\n",
    "dtrain = lgb.Dataset(train_X, label=train_y, weight=train_w, feature_name=feat_cols, free_raw_data=True)\n",
    "dval   = lgb.Dataset(val_X,   label=val_y,   weight=val_w,   feature_name=feat_cols, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"None\",\n",
    "    learning_rate=0.03,              # ↓ 更小学习率，配合更多轮数\n",
    "    num_leaves=127,                  # ↑ 增强模型容量\n",
    "    max_depth=10,\n",
    "    min_data_in_leaf=200,            # ↑ 稳定、抑制过拟合\n",
    "    num_threads=16,\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    first_metric_only=True,\n",
    "\n",
    "    # 直方图/分箱（GPU 友好）\n",
    "    max_bin=63,\n",
    "    bin_construct_sample_cnt=400_000,# ↑ 分箱更准（全量训练）\n",
    "    min_data_in_bin=3,\n",
    "\n",
    "    # GPU\n",
    "    device_type=\"gpu\",\n",
    "    gpu_device_id=0,\n",
    "\n",
    "    # 采样 + 正则\n",
    "    feature_fraction=0.65,           # 列采样\n",
    "    feature_fraction_bynode=0.8,     # 结点级列采样\n",
    "    bagging_fraction=0.70,           # 行采样\n",
    "    bagging_freq=1,\n",
    "    lambda_l2=5.0,                   # L2 正则更稳\n",
    "    extra_trees=True,                # 更稳更快（常见于大数据）\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params, dtrain,\n",
    "    valid_sets=[dval], valid_names=[\"val\"],   # 只监控验证集，收敛更快\n",
    "    num_boost_round=4000,                     # ↑ 配合小 lr\n",
    "    callbacks=[lgb.early_stopping(200), lgb.log_evaluation(50)],\n",
    "    feval=lgb_wr2_eval,\n",
    ")\n",
    "\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": model.feature_name(),\n",
    "    \"gain\": model.feature_importance(\"gain\"),\n",
    "    \"split\": model.feature_importance(\"split\"),\n",
    "}).sort_values(\"gain\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(imp.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 如果不存在则创建目录\n",
    "os.makedirs(\"exp/v1/models\", exist_ok=True)\n",
    "\n",
    "# save model\n",
    "model.save_model(\"exp/v1/models/lgbm_final.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c531e6",
   "metadata": {},
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bf46f",
   "metadata": {},
   "source": [
    "首先对测试集做特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b47d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 参数（与训练期基本一致，仅改为 test 路径/前缀） ======\n",
    "import os, gc\n",
    "import polars as pl\n",
    "\n",
    "DATE_LO, DATE_HI = 1530, 1698  # 可按需调整或覆盖\n",
    "\n",
    "# 从 Blob 列出全部 test fe_shards 分片（无协议 → 统一加协议）\n",
    "fe_all_np = fs.glob(f\"{FE_SHA_DIR_NP}/test_*_*.parquet\")\n",
    "protocol = FE_SHA_DIR_B.split(\"://\", 1)[0] + \"://\"\n",
    "fe_all = [p if p.startswith(protocol) else protocol + p for p in fe_all_np]\n",
    "\n",
    "# ---- 解析所有窗口，并按日期范围筛选（与训练期相同逻辑）----\n",
    "wins = set()\n",
    "for p in fe_all_np:\n",
    "    base = os.path.basename(p)                         # e.g. test_C_lags_1220_1249.parquet\n",
    "    stem = base[:-8]                                   # 去掉 .parquet\n",
    "    parts = stem.split(\"_\")\n",
    "    lo, hi = int(parts[-2]), int(parts[-1])            # 最后两段是 lo/hi\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "# ---- 获取该区间内实际存在的 date_id（仅用于 sanity/log；不做 train/val 切分）----\n",
    "days = (pl.scan_parquet(TEST_BASE_PATH, storage_options=storage_options)\n",
    "          .filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "          .select(pl.col(\"date_id\").unique().sort())\n",
    "          .collect(streaming=True)[\"date_id\"].to_list())\n",
    "assert days, \"no days found in range\"\n",
    "print(f\"days total={len(days)}\")\n",
    "\n",
    "# ---- 主循环：对每个 (lo,hi) 窗口横向拼接后，写出 test_{lo}_{hi}.parquet ----\n",
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "\n",
    "    # 基表（仅 KEYS + 可选 WEIGHT + 基础特征），并固定排序保证因果/可复现\n",
    "    # 注意：WEIGHT 在 test 里可能不存在，做个存在性判断\n",
    "    base_scan = (pl.scan_parquet(TEST_BASE_PATH, storage_options=storage_options)\n",
    "                   .filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "                   .sort(KEYS))\n",
    "\n",
    "    lf = base_scan.select([*KEYS, WEIGHT, *[pl.col(c) for c in FEATURE_COLS]])\n",
    "\n",
    "    # 归一化分片路径集合，方便存在性判断\n",
    "    fe_set = {p if p.startswith(protocol) else protocol + p for p in fe_all_np}\n",
    "\n",
    "    # A/B 文件（test_ 前缀）\n",
    "    fe_files = []\n",
    "    for name in (f\"test_A_{lo}_{hi}.parquet\", f\"test_B_{lo}_{hi}.parquet\"):\n",
    "        p = f\"{FE_SHA_DIR_B}/{name}\"  # 带协议\n",
    "        if p in fe_set or fs.exists(p):\n",
    "            fe_files.append(p)\n",
    "\n",
    "    # 同窗口所有 C_* 分片（无协议 → 加协议）\n",
    "    c_files_np = fs.glob(f\"{FE_SHA_DIR_NP}/test_C_*_{lo}_{hi}.parquet\")\n",
    "    fe_files += [p if p.startswith(protocol) else protocol + p for p in sorted(c_files_np)]\n",
    "\n",
    "    # 逐个左连接（严格只补充新列，避免重复；scan 到 Blob 时加 storage_options）\n",
    "    already = set(lf.collect_schema().names())\n",
    "    for fp in fe_files:\n",
    "        ds = pl.scan_parquet(fp, storage_options=storage_options)\n",
    "        names = ds.collect_schema().names()\n",
    "        add_cols = [c for c in names if c not in already]\n",
    "        if add_cols:\n",
    "            lf = lf.join(ds.select([*KEYS, *add_cols]), on=KEYS, how=\"left\")\n",
    "            already.update(add_cols)\n",
    "\n",
    "    # 统一为 Float32；test 无 TARGET，所以只保留 KEYS + 可选 WEIGHT + 所有特征\n",
    "    keep_non_key = [c for c in already if c not in set(KEYS)]\n",
    "    # 确保 WEIGHT 不在特征转换列表里被重复处理\n",
    "    feat_present = [c for c in keep_non_key if c not in (*KEYS, WEIGHT)]\n",
    "\n",
    "    select_exprs = [\n",
    "        *KEYS, \n",
    "        pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT), \n",
    "        *[pl.col(c).cast(pl.Float32).alias(c) for c in feat_present], ]\n",
    "    lf_win = lf.select(select_exprs)\n",
    "\n",
    "    # 写出单文件 test_{lo}_{hi}.parquet（流式，不 materialize 全窗口）\n",
    "    out_test = f\"{FE_SHA_DIR_B}/test_{lo}_{hi}.parquet\"\n",
    "    # 覆盖旧文件（若存在）\n",
    "    if fs.exists(out_test):\n",
    "        fs.rm(out_test)\n",
    "\n",
    "    lf_win.sink_parquet(\n",
    "        out_test,\n",
    "        compression=\"zstd\",\n",
    "        statistics=True,\n",
    "        storage_options=storage_options,\n",
    "        maintain_order=True,  # 上面已 sort(KEYS)，这里维持写出顺序\n",
    "    )\n",
    "\n",
    "    print(f\"[{lo}_{hi}] write: {os.path.basename(out_test)}  (cols={len(select_exprs)})\")\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00571c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, json, time, re\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "feat_cols = pd.read_csv('exp/v1/config/input_sets/top_imp.csv')['feature'].tolist()\n",
    "\n",
    "def shard2memmap(fe_paths, feat_cols, prefix, storage_options=None):\n",
    "    \"\"\"\n",
    "    将若干 parquet 分片顺序拼接为一份 memmap: X(float32)\n",
    "    - 仅提取 feat_cols（缺失列用 NaN 补齐），列顺序以 feat_cols 为准\n",
    "    - 输出：{prefix}_X.float32.mmap 与 {prefix}.meta.json\n",
    "    - fe_paths: 已经挑选/过滤好的最终 test_{lo}_{hi}.parquet 列表（可含 az:// 或本地路径）\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) 规范化 + 数值排序\n",
    "    def _numeric_key(p: str):\n",
    "        b = os.path.basename(p)\n",
    "        stem = b[:-8]  # 去掉 .parquet\n",
    "        parts = stem.split(\"_\")\n",
    "        try:\n",
    "            lo, hi = int(parts[-2]), int(parts[-1])\n",
    "            return (lo, hi)\n",
    "        except Exception:\n",
    "            return (b, b)\n",
    "    fe_paths = sorted(fe_paths, key=_numeric_key)\n",
    "\n",
    "    # 1) 兜底：确保有文件、确保输出目录存在\n",
    "    if not fe_paths:\n",
    "        raise FileNotFoundError(\"fe_paths is empty.\")\n",
    "    os.makedirs(os.path.dirname(prefix), exist_ok=True)\n",
    "\n",
    "    # 2) 统计每个分片的行数（流式）\n",
    "    counts = []\n",
    "    for p in fe_paths:\n",
    "        lf = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "              if p.startswith(\"az://\") else pl.scan_parquet(p))\n",
    "        k = lf.select(pl.len()).collect(streaming=True).item()\n",
    "        counts.append(int(k))\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    print(f\"[memmap]: {len(fe_paths)} files, {n_rows} rows, {n_feat} features\")\n",
    "\n",
    "    # 3) 分配 memmap（本地）\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "\n",
    "    # 4) 逐分片流式写入\n",
    "    i = 0\n",
    "    for p, k in zip(fe_paths, counts):\n",
    "        lf = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "              if p.startswith(\"az://\") else pl.scan_parquet(p))\n",
    "        names = set(lf.collect_schema().names())\n",
    "        exprs = [\n",
    "            (pl.col(c).cast(pl.Float32).alias(c) if c in names\n",
    "             else pl.lit(None, dtype=pl.Float32).alias(c))\n",
    "            for c in feat_cols\n",
    "        ]\n",
    "        df = lf.select(exprs).collect(streaming=True)\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush()\n",
    "\n",
    "    # 5) 保存 meta（含来源文件）\n",
    "    meta = {\n",
    "        \"n_rows\": int(n_rows),\n",
    "        \"n_feat\": int(n_feat),\n",
    "        \"dtype\": \"float32\",\n",
    "        \"ts\": time.time(),\n",
    "        \"features\": list(feat_cols),\n",
    "        \"paths\": list(fe_paths),\n",
    "    }\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ---- 选取最终 test_{lo}_{hi}.parquet 文件 ----\n",
    "regex = re.compile(r\"^test_\\d+_\\d+\\.parquet$\")\n",
    "fe_all_np = fs.glob(f\"{FE_SHA_DIR_NP}/test_*_*.parquet\")\n",
    "fe_all_np = [p for p in fe_all_np if regex.match(os.path.basename(p))]\n",
    "fe_paths = [(\"az://\" + p) if not p.startswith(\"az://\") else p for p in fe_all_np]\n",
    "\n",
    "# ---- 生成测试 memmap ----\n",
    "test_X = shard2memmap(\n",
    "    fe_paths, feat_cols, prefix=f\"{MM_DIR}/test\", storage_options=storage_options\n",
    ")\n",
    "print(\"test shape:\", test_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 载入模型， 用memmap 批量处理\n",
    "\n",
    "MODEL_PATH = \"exp/v1/models/lgbm_final.txt\"\n",
    "\n",
    "\n",
    "PRED_DIR = \"exp/v1/predictions\"\n",
    "os.makedirs(PRED_DIR, exist_ok=True)\n",
    "\n",
    "# 读取 memmap\n",
    "with open(f\"{MM_DIR}/test.meta.json\", \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "n_rows, n_feat = meta[\"n_rows\"], meta[\"n_feat\"]\n",
    "test_X = np.memmap(f\"{MM_DIR}/test_X.float32.mmap\", dtype=\"float32\", mode=\"r\",\n",
    "                   shape=(n_rows, n_feat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca31137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入模型\n",
    "model = lgb.Booster(model_file=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2748e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 50_000  # 按你机器内存调整\n",
    "preds = np.memmap(f\"{PRED_DIR}/test_pred.float32.mmap\", dtype=\"float32\", mode=\"w+\",\n",
    "                  shape=(n_rows,))\n",
    "for s in range(0, n_rows, BATCH):\n",
    "    e = min(s + BATCH, n_rows)\n",
    "    Xb = test_X[s:e]\n",
    "    # scikit/LightGBM/XGBoost 都支持 .predict(np.ndarray)\n",
    "    pb = model.predict(Xb)\n",
    "    preds[s:e] = pb.astype(\"float32\", copy=False)\n",
    "    del Xb, pb\n",
    "    gc.collect()\n",
    "preds.flush()\n",
    "print(\"inference done:\", preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "regex = re.compile(r\"^test_\\d+_\\d+\\.parquet$\")\n",
    "fe_all_np = fs.glob(f\"{FE_SHA_DIR_NP}/test_*_*.parquet\")\n",
    "fe_all_np = [p for p in fe_all_np if regex.match(os.path.basename(p))]\n",
    "\n",
    "# 数值排序，确保 (lo,hi) 顺序一致\n",
    "def _key(p):\n",
    "    b = os.path.basename(p)[:-8]\n",
    "    a,b2 = b.split(\"_\")[-2:]\n",
    "    return (int(a), int(b2))\n",
    "fe_paths = [(\"az://\"+p) if not p.startswith(\"az://\") else p for p in sorted(fe_all_np, key=_key)]\n",
    "\n",
    "# 逐分片写 preds（带 KEYS）\n",
    "i = 0\n",
    "for p in fe_paths:\n",
    "    # 行数\n",
    "    k = pl.scan_parquet(p, storage_options=storage_options).select(pl.len()).collect(streaming=True).item()\n",
    "\n",
    "    # 取 KEYS\n",
    "    df_keys = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "                 .select(KEYS)\n",
    "                 .collect(streaming=True))\n",
    "\n",
    "    # 取对应预测切片\n",
    "    yhat = preds[i:i+k].copy()  # 小片复制到内存\n",
    "    df_pred = df_keys.with_columns(pl.Series(\"y_pred\", yhat.astype(\"float32\")))\n",
    "\n",
    "    # 写出同名窗口的预测文件\n",
    "    base = os.path.basename(p)[:-8]   # test_lo_hi\n",
    "    outp = f\"{PRED_DIR}/{base}_preds.parquet\"\n",
    "    if fs.exists(outp): fs.rm(outp)\n",
    "    df_pred.write_parquet(outp, compression=\"zstd\")\n",
    "\n",
    "    i += k\n",
    "    del df_keys, df_pred, yhat; gc.collect()\n",
    "\n",
    "print(\"pred shards written to:\", PRED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ad4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总全部预测分片\n",
    "preds_all = pl.scan_parquet(f\"{PRED_DIR}/test_*_preds.parquet\")\n",
    "\n",
    "# 选择评估日期范围（可根据你的 test 窗口，比如 1530..1698）\n",
    "DATE_LO, DATE_HI = 1530, 1698\n",
    "labels = (pl.scan_parquet(TEST_BASE_PATH, storage_options=storage_options)\n",
    "            .filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "            .select([*KEYS,\n",
    "                     pl.col(TARGET).cast(pl.Float32).alias(\"y_true\"),\n",
    "                     pl.col(WEIGHT).cast(pl.Float32).alias(\"w\")]))\n",
    "\n",
    "# join + 算指标\n",
    "df = preds_all.join(labels, on=KEYS, how=\"inner\").collect(streaming=True)\n",
    "\n",
    "# 若某些天没有权重，兜底为 1.0\n",
    "if \"w\" not in df.columns:\n",
    "    df = df.with_columns(pl.lit(1.0).alias(\"w\"))\n",
    "\n",
    "y = df[\"y_true\"].to_numpy()\n",
    "p = df[\"y_pred\"].to_numpy()\n",
    "w = df[\"w\"].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d98985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_r2_zero_mean(y_true, y_pred, weight):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "    weight = np.asarray(weight, dtype=np.float64).ravel()\n",
    "    num = np.sum(weight * (y_true - y_pred)**2)\n",
    "    den = np.sum(weight * (y_true**2))\n",
    "    return 0.0 if den <= 0 else 1.0 - (num/den)\n",
    "\n",
    "#输出结果\n",
    "wr2 = weighted_r2_zero_mean(y, p, w)\n",
    "print(f\"Final weighted R^2 (zero-mean): {wr2:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (JS .venv)",
   "language": "python",
   "name": "js-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
