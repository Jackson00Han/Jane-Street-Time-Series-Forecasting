{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5206c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3074b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "import time, numpy as np, polars as pl, pandas as pd, torch, lightning as L\n",
    "from pathlib import Path\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder, GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "\n",
    "def _now(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")\n",
    "\n",
    "# ---- 关键配置（按需改）----\n",
    "target_col = cfg[\"target\"]                 # e.g. \"responder_6\"\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "weight_col = cfg[\"weight\"]\n",
    "\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "base_features   = [\"feature_36\", \"feature_06\"]\n",
    "resp_his_feats  = [\"responder_5_prevday_std\", \"responder_3_prevday_std\", \"responder_4_prev_tail_d1\"]\n",
    "feat_his_feats  = [\"feature_08__ewm5\", \"feature_53__rstd3\"]\n",
    "feature_cols = list(dict.fromkeys(base_features + resp_his_feats + feat_his_feats))\n",
    "\n",
    "need_cols = list(dict.fromkeys([g_sym, g_date, g_time, weight_col, target_col] + time_features + feature_cols))\n",
    "\n",
    "# CV & 训练\n",
    "N_SPLITS   = 5\n",
    "GAP_DAYS   = 7\n",
    "TRAIN_TO_VAL = 4               # 训练:验证 = 4:1\n",
    "ENC_LEN    = 36\n",
    "PRED_LEN   = 1\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3\n",
    "HIDDEN     = 128\n",
    "HEADS      = 4\n",
    "DROPOUT    = 0.2\n",
    "MAX_EPOCHS_PER_SHARD = 1\n",
    "CHUNK_DAYS = 15               # 训练分片：每片多少天\n",
    "\n",
    "print(\"config ready\")\n",
    "\n",
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "glob_pat  = f\"{panel_dir}/*.parquet\"\n",
    "if not fs.glob(glob_pat.replace(\"az://\", \"\")):\n",
    "    raise FileNotFoundError(f\"No parquet shards under: {glob_pat}\")\n",
    "\n",
    "lf_raw = pl.scan_parquet(glob_pat, storage_options=storage_options)\n",
    "\n",
    "# 全局 time_idx 仅一次\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\")\n",
    "if not Path(grid_path).exists():\n",
    "    lf_grid = (\n",
    "        lf_raw.select([g_date, g_time]).unique()\n",
    "              .sort([g_date, g_time])\n",
    "              .with_row_index(\"time_idx\")\n",
    "              .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "    )\n",
    "    ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "    lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")\n",
    "\n",
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# 接入 time_idx + 只保留所需列（仍是 Lazy）\n",
    "lf0 = (\n",
    "    lf_raw.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "          .select(need_cols + [\"time_idx\"])\n",
    "          .sort([g_date, g_time, g_sym])\n",
    ")\n",
    "\n",
    "print(f\"[{_now()}] lazyframe ready\")\n",
    "\n",
    "# 全量天列表\n",
    "all_days = (\n",
    "    lf0.select(pl.col(g_date)).unique().sort(by=g_date)\n",
    "       .collect(streaming=True).get_column(g_date).to_numpy()\n",
    ")\n",
    "\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0: return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0: return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days): break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "\n",
    "# 选“标准化统计区间”上界：第1折训练天的最大值（不含验证）\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"stats_hi (for global z-score) = {stats_hi}; first-fold train days end at this day.\")\n",
    "\n",
    "clean_dir = P(\"local\", \"tft/clean_by_day\")\n",
    "ensure_dir_local(clean_dir)\n",
    "\n",
    "# ========== 1) 连续特征：一次性处理 inf->null、打缺失标记、组内 ffill、兜底 0 ==========\n",
    "# 先做 flag（要基于原始缺失），再做填充；合并成两次 with_columns，避免在 for 循环里多次改列\n",
    "inf2null_exprs = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
    "                  for c in feature_cols]\n",
    "flags_exprs    = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\")\n",
    "                  for c in feature_cols]\n",
    "fill_exprs     = [pl.col(c).forward_fill().over(g_sym).fill_null(0.0).alias(c)\n",
    "                  for c in feature_cols]\n",
    "\n",
    "lf_clean = (\n",
    "    lf0.with_columns(inf2null_exprs)         # inf -> null\n",
    "       .with_columns(flags_exprs)            # 缺失标记（基于原始缺失）\n",
    "       .with_columns(fill_exprs)             # 组内 ffill + 兜底 0\n",
    ")\n",
    "\n",
    "# ========== 2) 训练区间（<= stats_hi）按组计算 mu/std（一次） ==========\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .group_by(g_sym)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in feature_cols] +\n",
    "         [pl.col(c).std().alias(f\"std_{c}\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 2) 训练期全局统计（作为回退）\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in feature_cols] +\n",
    "            [pl.col(c).std().alias(f\"std_{c}_glb\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 3) 把全局统计作为常量列加到每行（cross join 方式）\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\")\n",
    "\n",
    "# 4) join per-symbol 统计，并对每个特征做回退 & z-score\n",
    "\n",
    "lf_z = lf_z.join(lf_stats_sym, on=g_sym, how=\"left\")\n",
    "\n",
    "# 对每个特征做回退 & z-score\n",
    "eps = 1e-6\n",
    "z_cols = []\n",
    "for c in feature_cols:\n",
    "    mu_c_sym, std_c_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_c_glb, std_c_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    c_z = f\"{c}_z\"\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_c_sym).is_null()).then(pl.col(mu_c_glb)).otherwise(pl.col(mu_c_sym)).alias(f\"{c}_mu_use\"),\n",
    "        pl.when(pl.col(std_c_sym).is_null() | (pl.col(std_c_sym) == 0)).then(pl.col(std_c_glb)).otherwise(pl.col(std_c_sym)).alias(f\"{c}_std_use\")\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(f\"{c}_mu_use\")) / (pl.col(f\"{c}_std_use\") + eps)).alias(c_z)\n",
    "    ).drop([mu_c_glb, std_c_glb, mu_c_sym, std_c_sym, f\"{c}_mu_use\", f\"{c}_std_use\"])\n",
    "    z_cols.append(c_z)\n",
    "\n",
    "# 5) 输出列（z_特征 + isna 标记 + 时间/分类/目标/权重）\n",
    "namark_cols = [f\"{c}__isna\" for c in feature_cols]\n",
    "out_cols = [g_sym, g_date, g_time, \"time_idx\", weight_col, target_col] + time_features + z_cols + namark_cols\n",
    "\n",
    "lf_out = lf_z.select(out_cols).sort([g_date, g_time, g_sym])\n",
    "\n",
    "# ========== 4) 分块收集 + 分区写盘 ==========\n",
    "# 关键：不要“逐天 collect”，而是每次收集一批天，然后一次性按 day 分区写入，显著减少 IO 次数\n",
    "CHUNK_DAYS = 20  # 可根据机器内存/速度调整；比如 10~30 天一块\n",
    "day_list = list(map(int, all_days))\n",
    "day_chunks = [day_list[i:i+CHUNK_DAYS] for i in range(0, len(day_list), CHUNK_DAYS)]\n",
    "\n",
    "for ci, chunk in enumerate(day_chunks, 1):\n",
    "    df_chunk = (\n",
    "        lf_out.filter(pl.col(g_date).is_in(chunk))\n",
    "              .collect(streaming=True)              # 单次 collect 一大块\n",
    "    )\n",
    "    # 将symbol_id 转为 category\n",
    "    \n",
    "    # 利用 polars 的分区写出（需要较新的 polars；若你的版本不支持 partition_by，退化为循环写）\n",
    "    try:\n",
    "        df_chunk.write_parquet(\n",
    "            Path(clean_dir).as_posix(),\n",
    "            compression=\"zstd\",\n",
    "            # ★ 这行很关键：一次性把该 chunk 里所有 day 写成多个子文件\n",
    "            partition_by=[g_date],                 # polars>=0.20 支持；不支持就改成下面的 fallback\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fallback：分天写（比原先好，因为前面的 join/with_columns 已经批量完成，只剩 IO）\n",
    "        for d in chunk:\n",
    "            out_path = Path(clean_dir) / f\"day={d}.parquet\"\n",
    "            df_chunk.filter(pl.col(g_date) == d).write_parquet(out_path.as_posix(), compression=\"zstd\")\n",
    "    print(f\"[{_now()}] chunk {ci}/{len(day_chunks)} -> days {chunk[0]}..{chunk[-1]} written\")\n",
    "\n",
    "print(f\"[{_now()}] cleaned & standardized shards written to {clean_dir}\")\n",
    "\n",
    "# === Cell 1: Template & Validation ===\n",
    "import glob, numpy as np, polars as pl, pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "\n",
    "# 假设以下变量已由你前置代码给出：\n",
    "# cfg, P, clean_dir, folds_by_day, g_sym, g_date, g_time, target_col, weight_col\n",
    "# time_features, z_cols, namark_cols, ENC_LEN, PRED_LEN, BATCH_SIZE\n",
    "\n",
    "fold_idx = 1\n",
    "train_days, val_days = folds_by_day[fold_idx-1]\n",
    "days_sorted = np.sort(train_days)\n",
    "\n",
    "# ---- 验证集 ----\n",
    "val_paths = []\n",
    "for d in val_days:\n",
    "    val_paths.extend(glob.glob(f\"{clean_dir}/date_id={d}/*.parquet\"))\n",
    "\n",
    "pdf_val = pl.scan_parquet(val_paths).collect(streaming=True).to_pandas()\n",
    "pdf_val[g_sym] = pdf_val[g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "# ---- Template（用第一天分片建立，固化 encoders/scalers）----\n",
    "first_train_day = int(days_sorted[0])\n",
    "tmpl_paths = glob.glob(f\"{clean_dir}/date_id={first_train_day}/*.parquet\")\n",
    "pdf_tmpl = pl.scan_parquet(tmpl_paths).collect(streaming=True).to_pandas()\n",
    "pdf_tmpl[g_sym] = pdf_tmpl[g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "unknown_reals = time_features + z_cols + namark_cols\n",
    "# sklearn 的 FunctionTransformer 在 func=None 时就是恒等变换，并且是可 pickling 的\n",
    "identity_scalers = {c: FunctionTransformer(validate=False) for c in unknown_reals}\n",
    "\n",
    "\n",
    "\n",
    "template = TimeSeriesDataSet(\n",
    "    pdf_tmpl.sort_values([g_sym, \"time_idx\"]),\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target_col,\n",
    "    group_ids=[g_sym],\n",
    "    weight=weight_col,\n",
    "    max_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN,\n",
    "    static_categoricals=[g_sym],\n",
    "    time_varying_unknown_reals=unknown_reals,\n",
    "    target_normalizer=None,\n",
    "    categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=True, add_target_scales=True, add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    scalers=identity_scalers,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    template, data=pdf_val.sort_values([g_sym, \"time_idx\"]), stop_randomization=True\n",
    ")\n",
    "val_loader = validation.to_dataloader(\n",
    "    train=False, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "len(val_loader), pdf_tmpl.shape, pdf_val.shape\n",
    "\n",
    "# === Cell 2: IterableDataset 真·流式（按分片产出 batch） + 加权 WR² ===\n",
    "import random, torch, polars as pl, glob\n",
    "from torch.utils.data import IterableDataset\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "class ShardedBatchStream(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        template_tsd,\n",
    "        shard_days,\n",
    "        clean_dir: str,\n",
    "        g_sym: str,\n",
    "        batch_size: int = 1024,\n",
    "        num_workers: int = 4,\n",
    "        shuffle_within_shard: bool = True,\n",
    "        buffer_batches: int = 0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.template = template_tsd\n",
    "        self.days = list(map(int, shard_days))\n",
    "        self.clean_dir = clean_dir\n",
    "        self.g_sym = g_sym\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle_within_shard = shuffle_within_shard\n",
    "        self.buffer_batches = buffer_batches\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = random.Random(self.seed)\n",
    "        days = self.days[:]\n",
    "        rng.shuffle(days)\n",
    "\n",
    "        from collections import deque\n",
    "        buf = deque()\n",
    "\n",
    "        for d in days:\n",
    "            paths = glob.glob(f\"{self.clean_dir}/date_id={d}/*.parquet\")\n",
    "            if not paths:\n",
    "                continue\n",
    "\n",
    "            pdf = pl.scan_parquet(paths).collect(streaming=True).to_pandas()\n",
    "            if pdf.empty:\n",
    "                continue\n",
    "            pdf[self.g_sym] = pdf[self.g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "            tsds = TimeSeriesDataSet.from_dataset(\n",
    "                self.template,\n",
    "                data=pdf.sort_values([self.g_sym, \"time_idx\"]),\n",
    "                stop_randomization=True,\n",
    "            )\n",
    "\n",
    "            dl = tsds.to_dataloader(\n",
    "                train=True,\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "                shuffle=self.shuffle_within_shard,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=self.num_workers > 0,\n",
    "            )\n",
    "\n",
    "            for batch in dl:\n",
    "                if self.buffer_batches > 0:\n",
    "                    buf.append(batch)\n",
    "                    if len(buf) >= self.buffer_batches:\n",
    "                        k = rng.randrange(len(buf))\n",
    "                        if k:\n",
    "                            buf.rotate(-k)\n",
    "                        yield buf.popleft()\n",
    "                else:\n",
    "                    yield batch\n",
    "\n",
    "        while buf:\n",
    "            yield buf.popleft()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27967fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "L.seed_everything(int(cfg.get(\"seed\", 42)), workers=True)\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\"\n",
    ")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "# 创建模型\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    template,\n",
    "    loss=MAE(),\n",
    "    learning_rate=float(cfg.get(\"tft\", {}).get(\"lr\", 1e-3)),\n",
    "    hidden_size=int(cfg.get(\"tft\", {}).get(\"hidden_size\", 128)),\n",
    "    attention_head_size=int(cfg.get(\"tft\", {}).get(\"heads\", 4)),\n",
    "    dropout=float(cfg.get(\"tft\", {}).get(\"dropout\", 0.2)),\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "VAL_EVERY_STEPS = 500\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\", devices=1, precision=32,\n",
    "    max_epochs=1,\n",
    "    val_check_interval=VAL_EVERY_STEPS,\n",
    "    num_sanity_val_steps=0,\n",
    "    gradient_clip_val=0.5,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=early_stop_callback,\n",
    "    logger=logger,\n",
    "    default_root_dir=P(\"local\", \"tft/ckpts\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: 构建流式训练集 & 单次 fit ===\n",
    "train_stream = ShardedBatchStream(\n",
    "    template_tsd=template,\n",
    "    shard_days=days_sorted,\n",
    "    clean_dir=clean_dir,\n",
    "    g_sym=g_sym,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=4,\n",
    "    shuffle_within_shard=True,\n",
    "    buffer_batches=16,   # 0 代表关闭跨分片缓冲打乱；8~64 可微调\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_stream, val_dataloaders=val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
