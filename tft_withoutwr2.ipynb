{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d5206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-26 11:08:00] imports ok\n",
      "[2025-09-26 11:08:00] lazyframe ready\n",
      "[2025-09-26 11:08:00] built 5 folds\n"
     ]
    }
   ],
   "source": [
    "# ========= 0) Imports & 全局配置（沿用你现有的） =========\n",
    "from __future__ import annotations\n",
    "import time, numpy as np, polars as pl, pandas as pd, torch, lightning as L, glob, random, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "\n",
    "def _now(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")\n",
    "\n",
    "# ---- 关键配置（按需改）----\n",
    "target_col = cfg[\"target\"]\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]\n",
    "weight_col = cfg[\"weight\"]\n",
    "\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "base_features   = [\"feature_36\", \"feature_06\"]\n",
    "resp_his_feats  = [\"responder_5_prevday_std\", \"responder_3_prevday_std\", \"responder_4_prev_tail_d1\"]\n",
    "feat_his_feats  = [\"feature_08__ewm5\", \"feature_53__rstd3\"]\n",
    "feature_cols = list(dict.fromkeys(base_features + resp_his_feats + feat_his_feats))\n",
    "\n",
    "need_cols = list(dict.fromkeys([g_sym, g_date, g_time, weight_col, target_col] + time_features + feature_cols))\n",
    "\n",
    "# CV & 训练\n",
    "N_SPLITS   = 5\n",
    "GAP_DAYS   = 7\n",
    "TRAIN_TO_VAL = 4\n",
    "ENC_LEN    = 36\n",
    "PRED_LEN   = 1\n",
    "BATCH_SIZE = 1024\n",
    "LR = float(cfg.get(\"tft\", {}).get(\"lr\", 1e-3))\n",
    "HIDDEN     = int(cfg.get(\"tft\", {}).get(\"hidden_size\", 128))\n",
    "HEADS      = int(cfg.get(\"tft\", {}).get(\"heads\", 4))\n",
    "DROPOUT    = float(cfg.get(\"tft\", {}).get(\"dropout\", 0.2))\n",
    "VAL_EVERY_STEPS = 500\n",
    "SEED = int(cfg.get(\"seed\", 42))\n",
    "\n",
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "glob_pat  = f\"{panel_dir}/*.parquet\"\n",
    "if not fs.glob(glob_pat.replace(\"az://\", \"\")):\n",
    "    raise FileNotFoundError(f\"No parquet shards under: {glob_pat}\")\n",
    "\n",
    "lf_raw = pl.scan_parquet(glob_pat, storage_options=storage_options)\n",
    "\n",
    "# ---- 全局 time_idx 网格（仅一次）----\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\")\n",
    "if not Path(grid_path).exists():\n",
    "    lf_grid = (\n",
    "        lf_raw.select([g_date, g_time]).unique()\n",
    "              .sort([g_date, g_time])\n",
    "              .with_row_index(\"time_idx\")\n",
    "              .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "    )\n",
    "    ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "    lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")\n",
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# ---- 预处理基础 LazyFrame（尚未标准化）----\n",
    "lf0 = (\n",
    "    lf_raw.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "          .select(need_cols + [\"time_idx\"])\n",
    "          .sort([g_date, g_time, g_sym])\n",
    ")\n",
    "print(f\"[{_now()}] lazyframe ready\")\n",
    "\n",
    "# ---- 全量天列表 ----\n",
    "all_days = (\n",
    "    lf0.select(pl.col(g_date)).unique().sort(by=g_date)\n",
    "       .collect(streaming=True).get_column(g_date).to_numpy()\n",
    ")\n",
    "\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0: return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0: return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days): break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "print(f\"[{_now()}] built {len(folds_by_day)} folds\")\n",
    "\n",
    "# ========= 1) 按“折训练期上界”进行标准化并落盘（fold-aware） =========\n",
    "def clean_and_standardize_for_fold(lf_base: pl.LazyFrame,\n",
    "                                   train_days: np.ndarray,\n",
    "                                   *,\n",
    "                                   feature_cols: List[str],\n",
    "                                   out_dir: str,\n",
    "                                   g_sym: str, g_date: str,\n",
    "                                   eps: float = 1e-6) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    以本折训练期的最大日期为 stats_hi，按“组内优先、全局回退”的 z-score 标准化；\n",
    "    输出到 out_dir/fold=K/{g_date}=YYYYMMDD/*.parquet\n",
    "    返回：z_cols, namark_cols\n",
    "    \"\"\"\n",
    "    stats_hi = int(train_days[-1])\n",
    "    print(f\"[{_now()}] fold stats_hi={stats_hi}\")\n",
    "\n",
    "    # 连续特征：inf->null / 缺失标记 / 组内ffill / 0兜底\n",
    "    inf2null_exprs = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
    "                      for c in feature_cols]\n",
    "    flags_exprs    = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\")\n",
    "                      for c in feature_cols]\n",
    "    fill_exprs     = [pl.col(c).forward_fill().over(g_sym).fill_null(0.0).alias(c)\n",
    "                      for c in feature_cols]\n",
    "\n",
    "    lf_clean = (\n",
    "        lf_base.with_columns(inf2null_exprs)\n",
    "               .with_columns(flags_exprs)\n",
    "               .with_columns(fill_exprs)\n",
    "    )\n",
    "\n",
    "    # 训练期统计\n",
    "    lf_stats_sym = (\n",
    "        lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "                .group_by(g_sym)\n",
    "                .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in feature_cols] +\n",
    "                     [pl.col(c).std().alias(f\"std_{c}\") for c in feature_cols])\n",
    "    )\n",
    "    lf_stats_glb = (\n",
    "        lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "                .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in feature_cols] +\n",
    "                        [pl.col(c).std().alias(f\"std_{c}_glb\") for c in feature_cols])\n",
    "    )\n",
    "\n",
    "    # 合并并做 z-score（组内优先，全局回退；std=0 回退全局）\n",
    "    lf_z = lf_clean.join(lf_stats_glb, how=\"cross\").join(lf_stats_sym, on=g_sym, how=\"left\")\n",
    "\n",
    "    z_cols = []\n",
    "    for c in feature_cols:\n",
    "        mu_c_sym, std_c_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "        mu_c_glb, std_c_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "        c_z = f\"{c}_z\"\n",
    "        lf_z = lf_z.with_columns(\n",
    "            pl.when(pl.col(mu_c_sym).is_null()).then(pl.col(mu_c_glb)).otherwise(pl.col(mu_c_sym)).alias(f\"{c}_mu_use\"),\n",
    "            pl.when(pl.col(std_c_sym).is_null() | (pl.col(std_c_sym) == 0)).then(pl.col(std_c_glb)).otherwise(pl.col(std_c_sym)).alias(f\"{c}_std_use\"),\n",
    "        ).with_columns(\n",
    "            ((pl.col(c) - pl.col(f\"{c}_mu_use\")) / (pl.col(f\"{c}_std_use\") + eps)).alias(c_z)\n",
    "        ).drop([mu_c_glb, std_c_glb, mu_c_sym, std_c_sym, f\"{c}_mu_use\", f\"{c}_std_use\"])\n",
    "        z_cols.append(c_z)\n",
    "\n",
    "    namark_cols = [f\"{c}__isna\" for c in feature_cols]\n",
    "    out_cols = [g_sym, g_date, g_time, \"time_idx\", weight_col, target_col] + time_features + z_cols + namark_cols\n",
    "    lf_out = lf_z.select(out_cols).sort([g_date, g_time, g_sym])\n",
    "\n",
    "    # 按天分区写出到 fold 子目录\n",
    "    out_root = Path(out_dir)\n",
    "    ensure_dir_local(out_root.as_posix())\n",
    "\n",
    "    # 分块写\n",
    "    all_fold_days = list(map(int, lf_out.select(g_date).unique().collect(streaming=True)[g_date].to_list()))\n",
    "    CHUNK_DAYS_LOCAL = 20\n",
    "    day_chunks = [all_fold_days[i:i+CHUNK_DAYS_LOCAL] for i in range(0, len(all_fold_days), CHUNK_DAYS_LOCAL)]\n",
    "\n",
    "    for ci, chunk in enumerate(day_chunks, 1):\n",
    "        df_chunk = (\n",
    "            lf_out.filter(pl.col(g_date).is_in(chunk))\n",
    "                  .collect(streaming=True)\n",
    "        )\n",
    "        try:\n",
    "            df_chunk.write_parquet(\n",
    "                out_root.as_posix(),\n",
    "                compression=\"zstd\",\n",
    "                partition_by=[g_date],\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback：与读取保持一致的目录层级\n",
    "            for d in chunk:\n",
    "                day_dir = out_root / f\"{g_date}={d}\"\n",
    "                day_dir.mkdir(parents=True, exist_ok=True)\n",
    "                df_chunk.filter(pl.col(g_date) == d)\\\n",
    "                        .write_parquet((day_dir / \"part-000.parquet\").as_posix(), compression=\"zstd\")\n",
    "        print(f\"[{_now()}] fold-chunk {ci}/{len(day_chunks)} -> days {chunk[0]}..{chunk[-1]} written\")\n",
    "\n",
    "    return z_cols, namark_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca49d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_template_and_val(clean_dir_fold: str,\n",
    "                           train_days: np.ndarray,\n",
    "                           val_days: np.ndarray,\n",
    "                           *,\n",
    "                           g_sym: str, g_date: str) -> Tuple[TimeSeriesDataSet, torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    用多天训练片段构建 template，生成验证 DataLoader\n",
    "    \"\"\"\n",
    "    # 选择若干训练日用于 template（覆盖更多类别）\n",
    "    k = min(5, len(train_days))\n",
    "    tmpl_days = [int(d) for d in train_days[:k]]\n",
    "\n",
    "    tmpl_paths = []\n",
    "    for d in tmpl_days:\n",
    "        tmpl_paths.extend(glob.glob(f\"{clean_dir_fold}/{g_date}={d}/*.parquet\"))\n",
    "    assert len(tmpl_paths) > 0, f\"no template files under {clean_dir_fold}\"\n",
    "\n",
    "    pdf_tmpl = pl.scan_parquet(tmpl_paths).collect(streaming=True).to_pandas()\n",
    "    pdf_tmpl[g_sym] = pdf_tmpl[g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "    unknown_reals = time_features + z_cols_global + namark_cols_global  # 将在主流程里覆盖\n",
    "    identity_scalers = {c: FunctionTransformer(validate=False) for c in unknown_reals}\n",
    "\n",
    "    template = TimeSeriesDataSet(\n",
    "        pdf_tmpl.sort_values([g_sym, \"time_idx\"]),\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_col,\n",
    "        group_ids=[g_sym],\n",
    "        weight=weight_col,\n",
    "        max_encoder_length=ENC_LEN,\n",
    "        max_prediction_length=PRED_LEN,\n",
    "        static_categoricals=[g_sym],\n",
    "        time_varying_unknown_reals=unknown_reals,\n",
    "        target_normalizer=None,   # 按需改成 TorchNormalizer()，若目标量纲很大\n",
    "        categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "        add_relative_time_idx=True, add_target_scales=True, add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "        scalers=identity_scalers,\n",
    "    )\n",
    "\n",
    "    # 验证集\n",
    "    val_paths = []\n",
    "    for d in val_days:\n",
    "        val_paths.extend(glob.glob(f\"{clean_dir_fold}/{g_date}={int(d)}/*.parquet\"))\n",
    "    assert len(val_paths) > 0, f\"no validation files under {clean_dir_fold}\"\n",
    "\n",
    "    pdf_val = pl.scan_parquet(val_paths).collect(streaming=True).to_pandas()\n",
    "    pdf_val[g_sym] = pdf_val[g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "    # 对齐类别以避免 unseen\n",
    "    pdf_val[g_sym] = pdf_val[g_sym].astype(\n",
    "        pd.CategoricalDtype(categories=list(pdf_tmpl[g_sym].cat.categories))\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        template, data=pdf_val.sort_values([g_sym, \"time_idx\"]), stop_randomization=True\n",
    "    )\n",
    "    val_loader = validation.to_dataloader(\n",
    "        train=False, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "    return template, val_loader\n",
    "\n",
    "\n",
    "class ShardedBatchStream(IterableDataset):\n",
    "    def __init__(self, template_tsd, shard_days, clean_dir: str, g_sym: str,\n",
    "                 batch_size: int = 1024, num_workers: int = 4, shuffle_within_shard: bool = True,\n",
    "                 buffer_batches: int = 16, seed: int = 42):\n",
    "        super().__init__()\n",
    "        self.template = template_tsd\n",
    "        self.days = list(map(int, shard_days))\n",
    "        self.clean_dir = clean_dir\n",
    "        self.g_sym = g_sym\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle_within_shard = shuffle_within_shard\n",
    "        self.buffer_batches = buffer_batches\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = random.Random(self.seed)\n",
    "        days = self.days[:]\n",
    "        rng.shuffle(days)\n",
    "\n",
    "        from collections import deque\n",
    "        buf = deque()\n",
    "\n",
    "        for d in days:\n",
    "            paths = glob.glob(f\"{self.clean_dir}/{g_date}={d}/*.parquet\")\n",
    "            if not paths:\n",
    "                continue\n",
    "            pdf = pl.scan_parquet(paths).collect(streaming=True).to_pandas()\n",
    "            if pdf.empty:\n",
    "                continue\n",
    "            pdf[self.g_sym] = pdf[self.g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "            tsds = TimeSeriesDataSet.from_dataset(\n",
    "                self.template,\n",
    "                data=pdf.sort_values([self.g_sym, \"time_idx\"]),\n",
    "                stop_randomization=True,\n",
    "            )\n",
    "            dl = tsds.to_dataloader(\n",
    "                train=True,\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "                shuffle=self.shuffle_within_shard,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=self.num_workers > 0,\n",
    "            )\n",
    "\n",
    "            for batch in dl:\n",
    "                if self.buffer_batches > 0:\n",
    "                    buf.append(batch)\n",
    "                    if len(buf) >= self.buffer_batches:\n",
    "                        k = rng.randrange(len(buf))\n",
    "                        if k:\n",
    "                            buf.rotate(-k)\n",
    "                        yield buf.popleft()\n",
    "                else:\n",
    "                    yield batch\n",
    "\n",
    "        while buf:\n",
    "            yield buf.popleft()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee175ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(fold_idx: int,\n",
    "                   train_days: np.ndarray,\n",
    "                   val_days: np.ndarray,\n",
    "                   *,\n",
    "                   lf_base: pl.LazyFrame,\n",
    "                   feature_cols: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    清洗&标准化本折 -> 构建 template/val -> 训练 -> 返回指标与路径\n",
    "    \"\"\"\n",
    "    print(f\"\\n========== FOLD {fold_idx} ==========\")\n",
    "    L.seed_everything(SEED, workers=True)\n",
    "\n",
    "    # 1) 本折清洗输出目录\n",
    "    clean_dir_fold = P(\"local\", f\"tft/clean_by_day/fold={fold_idx}\")\n",
    "    ensure_dir_local(clean_dir_fold)\n",
    "\n",
    "    # 2) 以本折训练上界做标准化并落盘\n",
    "    global z_cols_global, namark_cols_global\n",
    "    z_cols_global, namark_cols_global = clean_and_standardize_for_fold(\n",
    "        lf_base, np.sort(train_days), feature_cols=feature_cols,\n",
    "        out_dir=clean_dir_fold, g_sym=g_sym, g_date=g_date\n",
    "    )\n",
    "\n",
    "    # 3) 构建 template & val loader\n",
    "    template, val_loader = build_template_and_val(\n",
    "        clean_dir_fold=clean_dir_fold,\n",
    "        train_days=np.sort(train_days), val_days=val_days,\n",
    "        g_sym=g_sym, g_date=g_date\n",
    "    )\n",
    "\n",
    "    # 4) 训练流数据集\n",
    "    train_stream = ShardedBatchStream(\n",
    "        template_tsd=template,\n",
    "        shard_days=np.sort(train_days),\n",
    "        clean_dir=clean_dir_fold,\n",
    "        g_sym=g_sym,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=4,\n",
    "        shuffle_within_shard=True,\n",
    "        buffer_batches=16,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # 5) 模型与训练器\n",
    "    ckpt_dir = P(\"local\", f\"tft/ckpts/fold={fold_idx}\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10, min_delta=1e-4),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=ckpt_dir,\n",
    "            filename=\"tft-{epoch:02d}-{val_loss:.4f}\",\n",
    "            monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ]\n",
    "    logger = TensorBoardLogger(save_dir=P(\"local\",\"tft/tblogs\"), name=f\"fold{fold_idx}\")\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        template,\n",
    "        loss=RMSE(),                              # 训练目标\n",
    "        logging_metrics=[MAE(), RMSE()],          # 记录更直观的指标\n",
    "        learning_rate=LR,\n",
    "        hidden_size=HIDDEN,\n",
    "        attention_head_size=HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"gpu\", devices=1, precision=32,\n",
    "        max_epochs=1,                             # 你可改成更多 epoch\n",
    "        val_check_interval=VAL_EVERY_STEPS,\n",
    "        num_sanity_val_steps=0,\n",
    "        gradient_clip_val=0.5,\n",
    "        log_every_n_steps=50,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        default_root_dir=ckpt_dir,\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloaders=train_stream, val_dataloaders=val_loader)\n",
    "\n",
    "    # 6) 记录本折最佳指标\n",
    "    best_ckpt = callbacks[1].best_model_path\n",
    "    best_val = callbacks[1].best_model_score.item() if callbacks[1].best_model_score is not None else float(\"nan\")\n",
    "    print(f\"[{_now()}] FOLD {fold_idx} best val_loss={best_val:.6f} @ {best_ckpt}\")\n",
    "    return {\"fold\": fold_idx, \"best_val_loss\": best_val, \"ckpt\": best_ckpt, \"clean_dir\": clean_dir_fold}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b5db10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== FOLD 1 ==========\n",
      "[2025-09-26 11:12:23] fold stats_hi=1640\n",
      "[2025-09-26 11:12:30] fold-chunk 1/5 -> days 1605..1624 written\n",
      "[2025-09-26 11:12:34] fold-chunk 2/5 -> days 1625..1644 written\n",
      "[2025-09-26 11:12:38] fold-chunk 3/5 -> days 1645..1664 written\n",
      "[2025-09-26 11:12:42] fold-chunk 4/5 -> days 1665..1684 written\n",
      "[2025-09-26 11:12:45] fold-chunk 5/5 -> days 1685..1698 written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 520    | train\n",
      "3  | prescalers                         | ModuleDict                      | 352    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 65.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "20 | output_layer                       | Linear                          | 129    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "898 K     Trainable params\n",
      "0         Non-trainable params\n",
      "898 K     Total params\n",
      "3.595     Total estimated model params size (MB)\n",
      "493       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1228/? [21:17<00:00,  0.96it/s, v_num=0, train_loss_step=1.460, val_loss=0.899, train_loss_epoch=1.270]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1228/? [21:18<00:00,  0.96it/s, v_num=0, train_loss_step=1.460, val_loss=0.899, train_loss_epoch=1.270]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-26 11:34:09] FOLD 1 best val_loss=0.898728 @ /mnt/data/js/exp/v1/tft/ckpts/fold=1/tft-epoch=00-val_loss=0.8987.ckpt\n",
      "\n",
      "========== FOLD 2 ==========\n",
      "[2025-09-26 11:34:09] fold stats_hi=1650\n",
      "[2025-09-26 11:34:16] fold-chunk 1/5 -> days 1605..1624 written\n",
      "[2025-09-26 11:34:20] fold-chunk 2/5 -> days 1625..1644 written\n",
      "[2025-09-26 11:34:24] fold-chunk 3/5 -> days 1645..1664 written\n",
      "[2025-09-26 11:34:27] fold-chunk 4/5 -> days 1665..1684 written\n",
      "[2025-09-26 11:34:32] fold-chunk 5/5 -> days 1685..1698 written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 520    | train\n",
      "3  | prescalers                         | ModuleDict                      | 352    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 65.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "20 | output_layer                       | Linear                          | 129    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "898 K     Trainable params\n",
      "0         Non-trainable params\n",
      "898 K     Total params\n",
      "3.595     Total estimated model params size (MB)\n",
      "493       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1229/? [21:30<00:00,  0.95it/s, v_num=0, train_loss_step=1.360, val_loss=1.130, train_loss_epoch=1.130]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1229/? [21:30<00:00,  0.95it/s, v_num=0, train_loss_step=1.360, val_loss=1.130, train_loss_epoch=1.130]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-26 11:56:07] FOLD 2 best val_loss=1.126468 @ /mnt/data/js/exp/v1/tft/ckpts/fold=2/tft-epoch=00-val_loss=1.1265.ckpt\n",
      "\n",
      "========== FOLD 3 ==========\n",
      "[2025-09-26 11:56:07] fold stats_hi=1660\n",
      "[2025-09-26 11:56:15] fold-chunk 1/5 -> days 1605..1624 written\n",
      "[2025-09-26 11:56:18] fold-chunk 2/5 -> days 1625..1644 written\n",
      "[2025-09-26 11:56:23] fold-chunk 3/5 -> days 1645..1664 written\n",
      "[2025-09-26 11:56:27] fold-chunk 4/5 -> days 1665..1684 written\n",
      "[2025-09-26 11:56:30] fold-chunk 5/5 -> days 1685..1698 written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 520    | train\n",
      "3  | prescalers                         | ModuleDict                      | 352    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 65.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "20 | output_layer                       | Linear                          | 129    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "898 K     Trainable params\n",
      "0         Non-trainable params\n",
      "898 K     Total params\n",
      "3.595     Total estimated model params size (MB)\n",
      "493       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1236/? [21:39<00:00,  0.95it/s, v_num=0, train_loss_step=1.120, val_loss=1.020, train_loss_epoch=1.050]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1236/? [21:40<00:00,  0.95it/s, v_num=0, train_loss_step=1.120, val_loss=1.020, train_loss_epoch=1.050]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-26 12:18:18] FOLD 3 best val_loss=1.016473 @ /mnt/data/js/exp/v1/tft/ckpts/fold=3/tft-epoch=00-val_loss=1.0165.ckpt\n",
      "\n",
      "========== FOLD 4 ==========\n",
      "[2025-09-26 12:18:18] fold stats_hi=1670\n",
      "[2025-09-26 12:18:25] fold-chunk 1/5 -> days 1605..1624 written\n",
      "[2025-09-26 12:18:29] fold-chunk 2/5 -> days 1625..1644 written\n",
      "[2025-09-26 12:18:33] fold-chunk 3/5 -> days 1645..1664 written\n",
      "[2025-09-26 12:18:37] fold-chunk 4/5 -> days 1665..1684 written\n",
      "[2025-09-26 12:18:41] fold-chunk 5/5 -> days 1685..1698 written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 520    | train\n",
      "3  | prescalers                         | ModuleDict                      | 352    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 65.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "20 | output_layer                       | Linear                          | 129    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "898 K     Trainable params\n",
      "0         Non-trainable params\n",
      "898 K     Total params\n",
      "3.595     Total estimated model params size (MB)\n",
      "493       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1231/? [21:42<00:00,  0.95it/s, v_num=0, train_loss_step=1.010, val_loss=0.959, train_loss_epoch=1.030]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1231/? [21:43<00:00,  0.94it/s, v_num=0, train_loss_step=1.010, val_loss=0.959, train_loss_epoch=1.030]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-26 12:40:32] FOLD 4 best val_loss=0.959434 @ /mnt/data/js/exp/v1/tft/ckpts/fold=4/tft-epoch=00-val_loss=0.9594.ckpt\n",
      "\n",
      "========== FOLD 5 ==========\n",
      "[2025-09-26 12:40:32] fold stats_hi=1680\n",
      "[2025-09-26 12:40:39] fold-chunk 1/5 -> days 1605..1624 written\n",
      "[2025-09-26 12:40:43] fold-chunk 2/5 -> days 1625..1644 written\n",
      "[2025-09-26 12:40:47] fold-chunk 3/5 -> days 1645..1664 written\n",
      "[2025-09-26 12:40:51] fold-chunk 4/5 -> days 1665..1684 written\n",
      "[2025-09-26 12:40:55] fold-chunk 5/5 -> days 1685..1698 written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 520    | train\n",
      "3  | prescalers                         | ModuleDict                      | 352    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 65.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "20 | output_layer                       | Linear                          | 129    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "898 K     Trainable params\n",
      "0         Non-trainable params\n",
      "898 K     Total params\n",
      "3.595     Total estimated model params size (MB)\n",
      "493       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1245/? [21:57<00:00,  0.95it/s, v_num=0, train_loss_step=1.180, val_loss=0.966, train_loss_epoch=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1245/? [21:58<00:00,  0.94it/s, v_num=0, train_loss_step=1.180, val_loss=0.966, train_loss_epoch=1.000]\n",
      "[2025-09-26 13:02:59] FOLD 5 best val_loss=0.966133 @ /mnt/data/js/exp/v1/tft/ckpts/fold=5/tft-epoch=00-val_loss=0.9661.ckpt\n",
      "\n",
      "===== CV Summary =====\n",
      "Folds: 5  mean(val_loss)=0.993447  std=0.076288\n",
      "fold=1  val_loss=0.898728  ckpt=/mnt/data/js/exp/v1/tft/ckpts/fold=1/tft-epoch=00-val_loss=0.8987.ckpt\n",
      "fold=2  val_loss=1.126468  ckpt=/mnt/data/js/exp/v1/tft/ckpts/fold=2/tft-epoch=00-val_loss=1.1265.ckpt\n",
      "fold=3  val_loss=1.016473  ckpt=/mnt/data/js/exp/v1/tft/ckpts/fold=3/tft-epoch=00-val_loss=1.0165.ckpt\n",
      "fold=4  val_loss=0.959434  ckpt=/mnt/data/js/exp/v1/tft/ckpts/fold=4/tft-epoch=00-val_loss=0.9594.ckpt\n",
      "fold=5  val_loss=0.966133  ckpt=/mnt/data/js/exp/v1/tft/ckpts/fold=5/tft-epoch=00-val_loss=0.9661.ckpt\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for k, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "    res = train_one_fold(k, train_days, val_days, lf_base=lf0, feature_cols=feature_cols)\n",
    "    results.append(res)\n",
    "\n",
    "# 汇总打印\n",
    "print(\"\\n===== CV Summary =====\")\n",
    "vals = [r[\"best_val_loss\"] for r in results if np.isfinite(r[\"best_val_loss\"])]\n",
    "if len(vals) > 0:\n",
    "    print(f\"Folds: {len(vals)}  mean(val_loss)={np.mean(vals):.6f}  std={np.std(vals):.6f}\")\n",
    "for r in results:\n",
    "    print(f\"fold={r['fold']}  val_loss={r['best_val_loss']:.6f}  ckpt={r['ckpt']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
