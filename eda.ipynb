{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6e401b",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926fa521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gc, re\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pipeline.io import cfg, fs, storage_options, P, ensure_dir_az\n",
    "from pipeline.features import run_staged_engineering, StageA, StageB, StageC\n",
    "\n",
    "def azify(p: str) -> str:\n",
    "    return p if p.startswith(\"az://\") else f\"az://{p}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cda0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 常量/列名 ----\n",
    "FEATURE_ALL = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "RESP_COLS   = [f\"responder_{i}\" for i in range(9)]\n",
    "KEYS        = tuple(cfg[\"keys\"])\n",
    "g_sym, g_date, g_time = KEYS\n",
    "TB = cfg['time_bucket']\n",
    "# ---- I/O ----\n",
    "clean_root = azify(P(\"az\", cfg[\"paths\"][\"clean_shards\"]))\n",
    "fe_root    = azify(P(\"az\", cfg[\"paths\"][\"fe_shards\"]))\n",
    "ensure_dir_az(fe_root)\n",
    "\n",
    "clean_paths = [azify(p) for p in sorted(fs.glob(f\"{clean_root}/*.parquet\"))]\n",
    "if not clean_paths:\n",
    "    raise FileNotFoundError(f\"No clean shards under {clean_root}\")\n",
    "\n",
    "\n",
    "lc = pl.scan_parquet(clean_paths, storage_options=storage_options)\n",
    "lc = (\n",
    "    lc.filter(pl.col(g_date).is_between(830, 900, closed=\"both\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f610",
   "metadata": {},
   "source": [
    "# 找出组内几乎不变的常量列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99c8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lc.sort([\"symbol_id\", \"date_id\", \"time_id\"]).collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1140a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示全部\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2b1827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>feature_06</th>\n",
       "      <th>feature_07</th>\n",
       "      <th>feature_08</th>\n",
       "      <th>feature_09</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>weight</th>\n",
       "      <th>time_bucket</th>\n",
       "      <th>responder_0</th>\n",
       "      <th>responder_1</th>\n",
       "      <th>responder_2</th>\n",
       "      <th>responder_3</th>\n",
       "      <th>responder_4</th>\n",
       "      <th>responder_5</th>\n",
       "      <th>responder_6</th>\n",
       "      <th>responder_7</th>\n",
       "      <th>responder_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>830</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.732551</td>\n",
       "      <td>2.838249</td>\n",
       "      <td>-1.910233</td>\n",
       "      <td>-1.909973</td>\n",
       "      <td>-2.921678</td>\n",
       "      <td>2.130022</td>\n",
       "      <td>-0.288255</td>\n",
       "      <td>1.025684</td>\n",
       "      <td>-0.617220</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.568132</td>\n",
       "      <td>2.894893</td>\n",
       "      <td>0.541611</td>\n",
       "      <td>-0.179338</td>\n",
       "      <td>-0.071614</td>\n",
       "      <td>-0.228823</td>\n",
       "      <td>-1.216636</td>\n",
       "      <td>-1.019639</td>\n",
       "      <td>0.900644</td>\n",
       "      <td>-0.163989</td>\n",
       "      <td>1.02089</td>\n",
       "      <td>0.901643</td>\n",
       "      <td>-0.210650</td>\n",
       "      <td>-0.904227</td>\n",
       "      <td>1.03609</td>\n",
       "      <td>1.474487</td>\n",
       "      <td>1.036584</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>-0.282585</td>\n",
       "      <td>0.627613</td>\n",
       "      <td>-0.268317</td>\n",
       "      <td>-0.572415</td>\n",
       "      <td>0.412958</td>\n",
       "      <td>1.680631</td>\n",
       "      <td>-0.007865</td>\n",
       "      <td>-0.439949</td>\n",
       "      <td>0.964614</td>\n",
       "      <td>1.017879</td>\n",
       "      <td>-0.202907</td>\n",
       "      <td>-0.068881</td>\n",
       "      <td>-1.104081</td>\n",
       "      <td>-0.398352</td>\n",
       "      <td>-1.286100</td>\n",
       "      <td>0.951201</td>\n",
       "      <td>1.049090</td>\n",
       "      <td>1.382032</td>\n",
       "      <td>1.430444</td>\n",
       "      <td>-0.340676</td>\n",
       "      <td>1.536353</td>\n",
       "      <td>0.606949</td>\n",
       "      <td>0.847232</td>\n",
       "      <td>-0.832851</td>\n",
       "      <td>-0.099722</td>\n",
       "      <td>-1.252360</td>\n",
       "      <td>1.961124</td>\n",
       "      <td>0.340672</td>\n",
       "      <td>1.298574</td>\n",
       "      <td>0.502728</td>\n",
       "      <td>0.638373</td>\n",
       "      <td>-0.224157</td>\n",
       "      <td>-0.168616</td>\n",
       "      <td>-0.280784</td>\n",
       "      <td>-1.933092</td>\n",
       "      <td>-1.503250</td>\n",
       "      <td>-0.685505</td>\n",
       "      <td>2.619909</td>\n",
       "      <td>0.378071</td>\n",
       "      <td>-0.621479</td>\n",
       "      <td>3.117842</td>\n",
       "      <td>0.578580</td>\n",
       "      <td>-0.376818</td>\n",
       "      <td>-0.387218</td>\n",
       "      <td>-0.139912</td>\n",
       "      <td>-0.081436</td>\n",
       "      <td>-0.293182</td>\n",
       "      <td>-0.226534</td>\n",
       "      <td>1.613274</td>\n",
       "      <td>0</td>\n",
       "      <td>0.551002</td>\n",
       "      <td>0.112173</td>\n",
       "      <td>0.257809</td>\n",
       "      <td>-0.117991</td>\n",
       "      <td>-0.506580</td>\n",
       "      <td>0.107359</td>\n",
       "      <td>-0.234335</td>\n",
       "      <td>-0.961939</td>\n",
       "      <td>-0.088541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>830</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.619766</td>\n",
       "      <td>2.088053</td>\n",
       "      <td>-1.671484</td>\n",
       "      <td>-1.764021</td>\n",
       "      <td>-2.196926</td>\n",
       "      <td>1.041264</td>\n",
       "      <td>-0.922567</td>\n",
       "      <td>0.430553</td>\n",
       "      <td>-0.627672</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.608984</td>\n",
       "      <td>2.943188</td>\n",
       "      <td>0.420189</td>\n",
       "      <td>-0.179338</td>\n",
       "      <td>-0.053845</td>\n",
       "      <td>-0.228823</td>\n",
       "      <td>-1.747756</td>\n",
       "      <td>-1.482374</td>\n",
       "      <td>0.900644</td>\n",
       "      <td>-0.163989</td>\n",
       "      <td>1.02089</td>\n",
       "      <td>0.901643</td>\n",
       "      <td>-0.213363</td>\n",
       "      <td>-0.904227</td>\n",
       "      <td>1.03609</td>\n",
       "      <td>1.474487</td>\n",
       "      <td>1.036584</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>-0.282585</td>\n",
       "      <td>0.627613</td>\n",
       "      <td>-0.268317</td>\n",
       "      <td>0.075327</td>\n",
       "      <td>0.147362</td>\n",
       "      <td>2.137070</td>\n",
       "      <td>-0.008614</td>\n",
       "      <td>-0.450105</td>\n",
       "      <td>0.964614</td>\n",
       "      <td>1.038494</td>\n",
       "      <td>-0.202907</td>\n",
       "      <td>-0.068881</td>\n",
       "      <td>0.056437</td>\n",
       "      <td>-0.398352</td>\n",
       "      <td>-1.381087</td>\n",
       "      <td>0.965580</td>\n",
       "      <td>1.643109</td>\n",
       "      <td>1.254152</td>\n",
       "      <td>0.898919</td>\n",
       "      <td>-0.340676</td>\n",
       "      <td>-0.066505</td>\n",
       "      <td>0.606949</td>\n",
       "      <td>0.847232</td>\n",
       "      <td>-0.377195</td>\n",
       "      <td>-0.099722</td>\n",
       "      <td>-1.195951</td>\n",
       "      <td>1.314484</td>\n",
       "      <td>0.340672</td>\n",
       "      <td>0.919209</td>\n",
       "      <td>0.657180</td>\n",
       "      <td>0.638373</td>\n",
       "      <td>-0.356323</td>\n",
       "      <td>-0.164762</td>\n",
       "      <td>-0.213675</td>\n",
       "      <td>-1.344059</td>\n",
       "      <td>-1.772867</td>\n",
       "      <td>-0.836668</td>\n",
       "      <td>2.685382</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>-0.547803</td>\n",
       "      <td>3.160747</td>\n",
       "      <td>1.029321</td>\n",
       "      <td>-0.376818</td>\n",
       "      <td>-0.387218</td>\n",
       "      <td>-0.086060</td>\n",
       "      <td>-0.039380</td>\n",
       "      <td>-0.120607</td>\n",
       "      <td>-0.161126</td>\n",
       "      <td>1.613274</td>\n",
       "      <td>0</td>\n",
       "      <td>0.479581</td>\n",
       "      <td>0.162743</td>\n",
       "      <td>0.230476</td>\n",
       "      <td>-0.148894</td>\n",
       "      <td>-0.622764</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>-0.489949</td>\n",
       "      <td>-0.781161</td>\n",
       "      <td>-0.179710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.759395</td>\n",
       "      <td>2.369348</td>\n",
       "      <td>-1.696708</td>\n",
       "      <td>-1.932428</td>\n",
       "      <td>-2.460622</td>\n",
       "      <td>1.866504</td>\n",
       "      <td>-2.459820</td>\n",
       "      <td>-0.156537</td>\n",
       "      <td>-0.646958</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.630282</td>\n",
       "      <td>3.015451</td>\n",
       "      <td>0.956868</td>\n",
       "      <td>-0.179338</td>\n",
       "      <td>-0.011268</td>\n",
       "      <td>-0.228823</td>\n",
       "      <td>-1.642332</td>\n",
       "      <td>-1.335051</td>\n",
       "      <td>0.900644</td>\n",
       "      <td>-0.163989</td>\n",
       "      <td>1.02089</td>\n",
       "      <td>0.901643</td>\n",
       "      <td>-0.216021</td>\n",
       "      <td>-0.904227</td>\n",
       "      <td>1.03609</td>\n",
       "      <td>1.474487</td>\n",
       "      <td>1.036584</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>-0.282585</td>\n",
       "      <td>0.627613</td>\n",
       "      <td>-0.268317</td>\n",
       "      <td>0.013497</td>\n",
       "      <td>0.137450</td>\n",
       "      <td>1.761290</td>\n",
       "      <td>0.037328</td>\n",
       "      <td>-0.402884</td>\n",
       "      <td>0.964614</td>\n",
       "      <td>0.772581</td>\n",
       "      <td>-0.202907</td>\n",
       "      <td>-0.068881</td>\n",
       "      <td>-0.473211</td>\n",
       "      <td>-0.398352</td>\n",
       "      <td>-1.467465</td>\n",
       "      <td>1.256983</td>\n",
       "      <td>1.685913</td>\n",
       "      <td>-0.156870</td>\n",
       "      <td>0.668008</td>\n",
       "      <td>-0.340676</td>\n",
       "      <td>-1.046606</td>\n",
       "      <td>0.606949</td>\n",
       "      <td>0.847232</td>\n",
       "      <td>-1.386103</td>\n",
       "      <td>-0.099722</td>\n",
       "      <td>-1.136616</td>\n",
       "      <td>1.467468</td>\n",
       "      <td>0.340672</td>\n",
       "      <td>-0.376697</td>\n",
       "      <td>0.254106</td>\n",
       "      <td>0.638373</td>\n",
       "      <td>-0.304117</td>\n",
       "      <td>-0.234047</td>\n",
       "      <td>-0.190594</td>\n",
       "      <td>-1.307520</td>\n",
       "      <td>-1.767718</td>\n",
       "      <td>-0.404848</td>\n",
       "      <td>2.248114</td>\n",
       "      <td>0.594394</td>\n",
       "      <td>-0.483131</td>\n",
       "      <td>3.223826</td>\n",
       "      <td>0.874155</td>\n",
       "      <td>-0.376818</td>\n",
       "      <td>-0.387218</td>\n",
       "      <td>-0.036471</td>\n",
       "      <td>-0.031966</td>\n",
       "      <td>-0.212657</td>\n",
       "      <td>-0.121167</td>\n",
       "      <td>1.613274</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602188</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.138208</td>\n",
       "      <td>-0.098100</td>\n",
       "      <td>-0.919861</td>\n",
       "      <td>0.097814</td>\n",
       "      <td>-0.367869</td>\n",
       "      <td>-0.843593</td>\n",
       "      <td>0.031717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>830</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.118403</td>\n",
       "      <td>2.481385</td>\n",
       "      <td>-1.941222</td>\n",
       "      <td>-1.943709</td>\n",
       "      <td>-2.032991</td>\n",
       "      <td>1.674263</td>\n",
       "      <td>-2.987439</td>\n",
       "      <td>-0.442469</td>\n",
       "      <td>-0.667290</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.633998</td>\n",
       "      <td>3.025571</td>\n",
       "      <td>1.065891</td>\n",
       "      <td>-0.179338</td>\n",
       "      <td>0.064825</td>\n",
       "      <td>-0.228823</td>\n",
       "      <td>-1.138194</td>\n",
       "      <td>-1.231092</td>\n",
       "      <td>0.900644</td>\n",
       "      <td>-0.163989</td>\n",
       "      <td>1.02089</td>\n",
       "      <td>0.901643</td>\n",
       "      <td>-0.218629</td>\n",
       "      <td>-0.904227</td>\n",
       "      <td>1.03609</td>\n",
       "      <td>1.474487</td>\n",
       "      <td>1.036584</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>-0.282585</td>\n",
       "      <td>0.627613</td>\n",
       "      <td>-0.268317</td>\n",
       "      <td>-0.523783</td>\n",
       "      <td>0.052489</td>\n",
       "      <td>2.720790</td>\n",
       "      <td>0.004992</td>\n",
       "      <td>-0.324539</td>\n",
       "      <td>0.964614</td>\n",
       "      <td>0.831047</td>\n",
       "      <td>-0.202907</td>\n",
       "      <td>-0.068881</td>\n",
       "      <td>-0.087708</td>\n",
       "      <td>-0.398352</td>\n",
       "      <td>-1.647498</td>\n",
       "      <td>0.693677</td>\n",
       "      <td>1.145990</td>\n",
       "      <td>0.256506</td>\n",
       "      <td>0.839130</td>\n",
       "      <td>-0.340676</td>\n",
       "      <td>1.137633</td>\n",
       "      <td>0.606949</td>\n",
       "      <td>0.847232</td>\n",
       "      <td>-1.375746</td>\n",
       "      <td>-0.099722</td>\n",
       "      <td>-1.328762</td>\n",
       "      <td>0.669375</td>\n",
       "      <td>0.340672</td>\n",
       "      <td>0.373901</td>\n",
       "      <td>0.412801</td>\n",
       "      <td>0.638373</td>\n",
       "      <td>-0.361982</td>\n",
       "      <td>-0.090786</td>\n",
       "      <td>-0.252602</td>\n",
       "      <td>-0.685047</td>\n",
       "      <td>-2.119442</td>\n",
       "      <td>-0.470560</td>\n",
       "      <td>2.583262</td>\n",
       "      <td>0.707315</td>\n",
       "      <td>-0.421351</td>\n",
       "      <td>3.233614</td>\n",
       "      <td>1.567712</td>\n",
       "      <td>-0.376818</td>\n",
       "      <td>-0.387218</td>\n",
       "      <td>-0.033219</td>\n",
       "      <td>-0.028263</td>\n",
       "      <td>-0.098273</td>\n",
       "      <td>-0.091303</td>\n",
       "      <td>1.613274</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272396</td>\n",
       "      <td>-0.035213</td>\n",
       "      <td>-0.031412</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>-0.616780</td>\n",
       "      <td>0.703515</td>\n",
       "      <td>-0.234365</td>\n",
       "      <td>-0.710919</td>\n",
       "      <td>1.267214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>830</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.601780</td>\n",
       "      <td>2.347465</td>\n",
       "      <td>-1.442185</td>\n",
       "      <td>-1.702303</td>\n",
       "      <td>-1.472809</td>\n",
       "      <td>1.725644</td>\n",
       "      <td>-1.938621</td>\n",
       "      <td>-0.309586</td>\n",
       "      <td>-0.684043</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.630944</td>\n",
       "      <td>3.051734</td>\n",
       "      <td>1.467152</td>\n",
       "      <td>-0.179338</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>-0.059567</td>\n",
       "      <td>-0.580090</td>\n",
       "      <td>-1.242958</td>\n",
       "      <td>0.900644</td>\n",
       "      <td>-0.163989</td>\n",
       "      <td>1.02089</td>\n",
       "      <td>0.901643</td>\n",
       "      <td>-0.221187</td>\n",
       "      <td>-0.904227</td>\n",
       "      <td>1.03609</td>\n",
       "      <td>1.474487</td>\n",
       "      <td>1.036584</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>-0.282585</td>\n",
       "      <td>0.627613</td>\n",
       "      <td>-0.268317</td>\n",
       "      <td>-0.458802</td>\n",
       "      <td>0.147759</td>\n",
       "      <td>1.670382</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>-0.282672</td>\n",
       "      <td>0.964614</td>\n",
       "      <td>0.893771</td>\n",
       "      <td>-0.202907</td>\n",
       "      <td>-0.068881</td>\n",
       "      <td>-0.009640</td>\n",
       "      <td>-0.398352</td>\n",
       "      <td>-0.751949</td>\n",
       "      <td>0.926668</td>\n",
       "      <td>1.772647</td>\n",
       "      <td>0.584011</td>\n",
       "      <td>1.117206</td>\n",
       "      <td>-0.340676</td>\n",
       "      <td>1.045445</td>\n",
       "      <td>0.606949</td>\n",
       "      <td>0.847232</td>\n",
       "      <td>-0.898895</td>\n",
       "      <td>-0.099722</td>\n",
       "      <td>-0.895798</td>\n",
       "      <td>1.384581</td>\n",
       "      <td>0.340672</td>\n",
       "      <td>0.452311</td>\n",
       "      <td>0.334856</td>\n",
       "      <td>0.638373</td>\n",
       "      <td>-0.345169</td>\n",
       "      <td>-0.075059</td>\n",
       "      <td>-0.230152</td>\n",
       "      <td>-1.632946</td>\n",
       "      <td>-1.828838</td>\n",
       "      <td>-0.395274</td>\n",
       "      <td>1.896746</td>\n",
       "      <td>0.941926</td>\n",
       "      <td>-0.275199</td>\n",
       "      <td>1.724052</td>\n",
       "      <td>1.310066</td>\n",
       "      <td>-0.376818</td>\n",
       "      <td>-0.387218</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>-0.095110</td>\n",
       "      <td>-0.087271</td>\n",
       "      <td>1.613274</td>\n",
       "      <td>0</td>\n",
       "      <td>0.361236</td>\n",
       "      <td>0.030810</td>\n",
       "      <td>0.270003</td>\n",
       "      <td>-0.034661</td>\n",
       "      <td>-0.712328</td>\n",
       "      <td>-0.165719</td>\n",
       "      <td>-0.486264</td>\n",
       "      <td>-0.955219</td>\n",
       "      <td>-0.703522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol_id  date_id  time_id  feature_00  feature_01  feature_02  \\\n",
       "0          0      830        0   -1.732551    2.838249   -1.910233   \n",
       "1          0      830        1   -1.619766    2.088053   -1.671484   \n",
       "2          0      830        2   -1.759395    2.369348   -1.696708   \n",
       "3          0      830        3   -1.118403    2.481385   -1.941222   \n",
       "4          0      830        4   -1.601780    2.347465   -1.442185   \n",
       "\n",
       "   feature_03  feature_04  feature_05  feature_06  feature_07  feature_08  \\\n",
       "0   -1.909973   -2.921678    2.130022   -0.288255    1.025684   -0.617220   \n",
       "1   -1.764021   -2.196926    1.041264   -0.922567    0.430553   -0.627672   \n",
       "2   -1.932428   -2.460622    1.866504   -2.459820   -0.156537   -0.646958   \n",
       "3   -1.943709   -2.032991    1.674263   -2.987439   -0.442469   -0.667290   \n",
       "4   -1.702303   -1.472809    1.725644   -1.938621   -0.309586   -0.684043   \n",
       "\n",
       "   feature_09  feature_10  feature_11  feature_12  feature_13  feature_14  \\\n",
       "0        11.0         7.0        76.0   -0.568132    2.894893    0.541611   \n",
       "1        11.0         7.0        76.0   -0.608984    2.943188    0.420189   \n",
       "2        11.0         7.0        76.0   -0.630282    3.015451    0.956868   \n",
       "3        11.0         7.0        76.0   -0.633998    3.025571    1.065891   \n",
       "4        11.0         7.0        76.0   -0.630944    3.051734    1.467152   \n",
       "\n",
       "   feature_15  feature_16  feature_17  feature_18  feature_19  feature_20  \\\n",
       "0   -0.179338   -0.071614   -0.228823   -1.216636   -1.019639    0.900644   \n",
       "1   -0.179338   -0.053845   -0.228823   -1.747756   -1.482374    0.900644   \n",
       "2   -0.179338   -0.011268   -0.228823   -1.642332   -1.335051    0.900644   \n",
       "3   -0.179338    0.064825   -0.228823   -1.138194   -1.231092    0.900644   \n",
       "4   -0.179338    0.014355   -0.059567   -0.580090   -1.242958    0.900644   \n",
       "\n",
       "   feature_21  feature_22  feature_23  feature_24  feature_25  feature_26  \\\n",
       "0   -0.163989     1.02089    0.901643   -0.210650   -0.904227     1.03609   \n",
       "1   -0.163989     1.02089    0.901643   -0.213363   -0.904227     1.03609   \n",
       "2   -0.163989     1.02089    0.901643   -0.216021   -0.904227     1.03609   \n",
       "3   -0.163989     1.02089    0.901643   -0.218629   -0.904227     1.03609   \n",
       "4   -0.163989     1.02089    0.901643   -0.221187   -0.904227     1.03609   \n",
       "\n",
       "   feature_27  feature_28  feature_29  feature_30  feature_31  feature_32  \\\n",
       "0    1.474487    1.036584    0.012248    0.006823   -0.282585    0.627613   \n",
       "1    1.474487    1.036584    0.012248    0.007117   -0.282585    0.627613   \n",
       "2    1.474487    1.036584    0.012248    0.007407   -0.282585    0.627613   \n",
       "3    1.474487    1.036584    0.012248    0.007693   -0.282585    0.627613   \n",
       "4    1.474487    1.036584    0.012248    0.007975   -0.282585    0.627613   \n",
       "\n",
       "   feature_33  feature_34  feature_35  feature_36  feature_37  feature_38  \\\n",
       "0   -0.268317   -0.572415    0.412958    1.680631   -0.007865   -0.439949   \n",
       "1   -0.268317    0.075327    0.147362    2.137070   -0.008614   -0.450105   \n",
       "2   -0.268317    0.013497    0.137450    1.761290    0.037328   -0.402884   \n",
       "3   -0.268317   -0.523783    0.052489    2.720790    0.004992   -0.324539   \n",
       "4   -0.268317   -0.458802    0.147759    1.670382    0.003621   -0.282672   \n",
       "\n",
       "   feature_39  feature_40  feature_41  feature_42  feature_43  feature_44  \\\n",
       "0    0.964614    1.017879   -0.202907   -0.068881   -1.104081   -0.398352   \n",
       "1    0.964614    1.038494   -0.202907   -0.068881    0.056437   -0.398352   \n",
       "2    0.964614    0.772581   -0.202907   -0.068881   -0.473211   -0.398352   \n",
       "3    0.964614    0.831047   -0.202907   -0.068881   -0.087708   -0.398352   \n",
       "4    0.964614    0.893771   -0.202907   -0.068881   -0.009640   -0.398352   \n",
       "\n",
       "   feature_45  feature_46  feature_47  feature_48  feature_49  feature_50  \\\n",
       "0   -1.286100    0.951201    1.049090    1.382032    1.430444   -0.340676   \n",
       "1   -1.381087    0.965580    1.643109    1.254152    0.898919   -0.340676   \n",
       "2   -1.467465    1.256983    1.685913   -0.156870    0.668008   -0.340676   \n",
       "3   -1.647498    0.693677    1.145990    0.256506    0.839130   -0.340676   \n",
       "4   -0.751949    0.926668    1.772647    0.584011    1.117206   -0.340676   \n",
       "\n",
       "   feature_51  feature_52  feature_53  feature_54  feature_55  feature_56  \\\n",
       "0    1.536353    0.606949    0.847232   -0.832851   -0.099722   -1.252360   \n",
       "1   -0.066505    0.606949    0.847232   -0.377195   -0.099722   -1.195951   \n",
       "2   -1.046606    0.606949    0.847232   -1.386103   -0.099722   -1.136616   \n",
       "3    1.137633    0.606949    0.847232   -1.375746   -0.099722   -1.328762   \n",
       "4    1.045445    0.606949    0.847232   -0.898895   -0.099722   -0.895798   \n",
       "\n",
       "   feature_57  feature_58  feature_59  feature_60  feature_61  feature_62  \\\n",
       "0    1.961124    0.340672    1.298574    0.502728    0.638373   -0.224157   \n",
       "1    1.314484    0.340672    0.919209    0.657180    0.638373   -0.356323   \n",
       "2    1.467468    0.340672   -0.376697    0.254106    0.638373   -0.304117   \n",
       "3    0.669375    0.340672    0.373901    0.412801    0.638373   -0.361982   \n",
       "4    1.384581    0.340672    0.452311    0.334856    0.638373   -0.345169   \n",
       "\n",
       "   feature_63  feature_64  feature_65  feature_66  feature_67  feature_68  \\\n",
       "0   -0.168616   -0.280784   -1.933092   -1.503250   -0.685505    2.619909   \n",
       "1   -0.164762   -0.213675   -1.344059   -1.772867   -0.836668    2.685382   \n",
       "2   -0.234047   -0.190594   -1.307520   -1.767718   -0.404848    2.248114   \n",
       "3   -0.090786   -0.252602   -0.685047   -2.119442   -0.470560    2.583262   \n",
       "4   -0.075059   -0.230152   -1.632946   -1.828838   -0.395274    1.896746   \n",
       "\n",
       "   feature_69  feature_70  feature_71  feature_72  feature_73  feature_74  \\\n",
       "0    0.378071   -0.621479    3.117842    0.578580   -0.376818   -0.387218   \n",
       "1    0.463456   -0.547803    3.160747    1.029321   -0.376818   -0.387218   \n",
       "2    0.594394   -0.483131    3.223826    0.874155   -0.376818   -0.387218   \n",
       "3    0.707315   -0.421351    3.233614    1.567712   -0.376818   -0.387218   \n",
       "4    0.941926   -0.275199    1.724052    1.310066   -0.376818   -0.387218   \n",
       "\n",
       "   feature_75  feature_76  feature_77  feature_78    weight  time_bucket  \\\n",
       "0   -0.139912   -0.081436   -0.293182   -0.226534  1.613274            0   \n",
       "1   -0.086060   -0.039380   -0.120607   -0.161126  1.613274            0   \n",
       "2   -0.036471   -0.031966   -0.212657   -0.121167  1.613274            0   \n",
       "3   -0.033219   -0.028263   -0.098273   -0.091303  1.613274            0   \n",
       "4    0.007926    0.011083   -0.095110   -0.087271  1.613274            0   \n",
       "\n",
       "   responder_0  responder_1  responder_2  responder_3  responder_4  \\\n",
       "0     0.551002     0.112173     0.257809    -0.117991    -0.506580   \n",
       "1     0.479581     0.162743     0.230476    -0.148894    -0.622764   \n",
       "2     0.602188     0.000249     0.138208    -0.098100    -0.919861   \n",
       "3     0.272396    -0.035213    -0.031412     0.032020    -0.616780   \n",
       "4     0.361236     0.030810     0.270003    -0.034661    -0.712328   \n",
       "\n",
       "   responder_5  responder_6  responder_7  responder_8  \n",
       "0     0.107359    -0.234335    -0.961939    -0.088541  \n",
       "1     0.005573    -0.489949    -0.781161    -0.179710  \n",
       "2     0.097814    -0.367869    -0.843593     0.031717  \n",
       "3     0.703515    -0.234365    -0.710919     1.267214  \n",
       "4    -0.165719    -0.486264    -0.955219    -0.703522  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f520cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = lc.collect_schema().names()\n",
    "cols = [c for c in cols if c not in KEYS]\n",
    "\n",
    "# 按 symbol 判断是否存在段内波动（可按 __streak_id 更细）\n",
    "df_val = lc.group_by(g_sym).agg([\n",
    "    pl.col(c).var().alias(f\"{c}_var\") for c in cols\n",
    "]).collect().to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f53d1fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>feature_00_var</th>\n",
       "      <th>feature_01_var</th>\n",
       "      <th>feature_02_var</th>\n",
       "      <th>feature_03_var</th>\n",
       "      <th>feature_04_var</th>\n",
       "      <th>feature_05_var</th>\n",
       "      <th>feature_06_var</th>\n",
       "      <th>feature_07_var</th>\n",
       "      <th>feature_08_var</th>\n",
       "      <th>feature_09_var</th>\n",
       "      <th>feature_10_var</th>\n",
       "      <th>feature_11_var</th>\n",
       "      <th>feature_12_var</th>\n",
       "      <th>feature_13_var</th>\n",
       "      <th>feature_14_var</th>\n",
       "      <th>feature_15_var</th>\n",
       "      <th>feature_16_var</th>\n",
       "      <th>feature_17_var</th>\n",
       "      <th>feature_18_var</th>\n",
       "      <th>feature_19_var</th>\n",
       "      <th>feature_20_var</th>\n",
       "      <th>feature_21_var</th>\n",
       "      <th>feature_22_var</th>\n",
       "      <th>feature_23_var</th>\n",
       "      <th>feature_24_var</th>\n",
       "      <th>feature_25_var</th>\n",
       "      <th>feature_26_var</th>\n",
       "      <th>feature_27_var</th>\n",
       "      <th>feature_28_var</th>\n",
       "      <th>feature_29_var</th>\n",
       "      <th>feature_30_var</th>\n",
       "      <th>feature_31_var</th>\n",
       "      <th>feature_32_var</th>\n",
       "      <th>feature_33_var</th>\n",
       "      <th>feature_34_var</th>\n",
       "      <th>feature_35_var</th>\n",
       "      <th>feature_36_var</th>\n",
       "      <th>feature_37_var</th>\n",
       "      <th>feature_38_var</th>\n",
       "      <th>feature_39_var</th>\n",
       "      <th>feature_40_var</th>\n",
       "      <th>feature_41_var</th>\n",
       "      <th>feature_42_var</th>\n",
       "      <th>feature_43_var</th>\n",
       "      <th>feature_44_var</th>\n",
       "      <th>feature_45_var</th>\n",
       "      <th>feature_46_var</th>\n",
       "      <th>feature_47_var</th>\n",
       "      <th>feature_48_var</th>\n",
       "      <th>feature_49_var</th>\n",
       "      <th>feature_50_var</th>\n",
       "      <th>feature_51_var</th>\n",
       "      <th>feature_52_var</th>\n",
       "      <th>feature_53_var</th>\n",
       "      <th>feature_54_var</th>\n",
       "      <th>feature_55_var</th>\n",
       "      <th>feature_56_var</th>\n",
       "      <th>feature_57_var</th>\n",
       "      <th>feature_58_var</th>\n",
       "      <th>feature_59_var</th>\n",
       "      <th>feature_60_var</th>\n",
       "      <th>feature_61_var</th>\n",
       "      <th>feature_62_var</th>\n",
       "      <th>feature_63_var</th>\n",
       "      <th>feature_64_var</th>\n",
       "      <th>feature_65_var</th>\n",
       "      <th>feature_66_var</th>\n",
       "      <th>feature_67_var</th>\n",
       "      <th>feature_68_var</th>\n",
       "      <th>feature_69_var</th>\n",
       "      <th>feature_70_var</th>\n",
       "      <th>feature_71_var</th>\n",
       "      <th>feature_72_var</th>\n",
       "      <th>feature_73_var</th>\n",
       "      <th>feature_74_var</th>\n",
       "      <th>feature_75_var</th>\n",
       "      <th>feature_76_var</th>\n",
       "      <th>feature_77_var</th>\n",
       "      <th>feature_78_var</th>\n",
       "      <th>weight_var</th>\n",
       "      <th>time_bucket_var</th>\n",
       "      <th>responder_0_var</th>\n",
       "      <th>responder_1_var</th>\n",
       "      <th>responder_2_var</th>\n",
       "      <th>responder_3_var</th>\n",
       "      <th>responder_4_var</th>\n",
       "      <th>responder_5_var</th>\n",
       "      <th>responder_6_var</th>\n",
       "      <th>responder_7_var</th>\n",
       "      <th>responder_8_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.552755</td>\n",
       "      <td>1.365154</td>\n",
       "      <td>0.566029</td>\n",
       "      <td>0.561250</td>\n",
       "      <td>1.262979</td>\n",
       "      <td>0.556862</td>\n",
       "      <td>0.565860</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.524954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.387970</td>\n",
       "      <td>0.594805</td>\n",
       "      <td>1.107079</td>\n",
       "      <td>0.337759</td>\n",
       "      <td>0.239664</td>\n",
       "      <td>0.356054</td>\n",
       "      <td>0.969259</td>\n",
       "      <td>1.117848</td>\n",
       "      <td>0.522853</td>\n",
       "      <td>0.028281</td>\n",
       "      <td>0.461378</td>\n",
       "      <td>0.099372</td>\n",
       "      <td>3.379281</td>\n",
       "      <td>0.992878</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.080140</td>\n",
       "      <td>0.068642</td>\n",
       "      <td>0.016971</td>\n",
       "      <td>0.038842</td>\n",
       "      <td>0.032799</td>\n",
       "      <td>0.448151</td>\n",
       "      <td>1.215907</td>\n",
       "      <td>0.363069</td>\n",
       "      <td>0.364058</td>\n",
       "      <td>1.244993</td>\n",
       "      <td>0.631386</td>\n",
       "      <td>0.673492</td>\n",
       "      <td>0.720899</td>\n",
       "      <td>0.287284</td>\n",
       "      <td>0.431883</td>\n",
       "      <td>0.654224</td>\n",
       "      <td>0.285330</td>\n",
       "      <td>0.423189</td>\n",
       "      <td>0.876377</td>\n",
       "      <td>0.769831</td>\n",
       "      <td>0.510614</td>\n",
       "      <td>0.397635</td>\n",
       "      <td>0.430001</td>\n",
       "      <td>1.174583</td>\n",
       "      <td>0.449479</td>\n",
       "      <td>0.920802</td>\n",
       "      <td>1.114798</td>\n",
       "      <td>0.405241</td>\n",
       "      <td>0.851618</td>\n",
       "      <td>0.841864</td>\n",
       "      <td>1.185929</td>\n",
       "      <td>0.756390</td>\n",
       "      <td>0.696308</td>\n",
       "      <td>0.687349</td>\n",
       "      <td>1.318248</td>\n",
       "      <td>0.014069</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.012386</td>\n",
       "      <td>0.890098</td>\n",
       "      <td>0.984169</td>\n",
       "      <td>1.197771</td>\n",
       "      <td>0.430643</td>\n",
       "      <td>0.882722</td>\n",
       "      <td>1.503357</td>\n",
       "      <td>0.547872</td>\n",
       "      <td>1.121680</td>\n",
       "      <td>0.803579</td>\n",
       "      <td>0.772007</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.867212</td>\n",
       "      <td>0.739379</td>\n",
       "      <td>0.711066</td>\n",
       "      <td>0.119436</td>\n",
       "      <td>2.917395</td>\n",
       "      <td>0.616058</td>\n",
       "      <td>0.531589</td>\n",
       "      <td>0.495747</td>\n",
       "      <td>0.412397</td>\n",
       "      <td>0.544061</td>\n",
       "      <td>0.277634</td>\n",
       "      <td>0.718410</td>\n",
       "      <td>0.791492</td>\n",
       "      <td>0.655386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.548992</td>\n",
       "      <td>1.382129</td>\n",
       "      <td>0.561656</td>\n",
       "      <td>0.557211</td>\n",
       "      <td>1.282489</td>\n",
       "      <td>1.266256</td>\n",
       "      <td>1.196652</td>\n",
       "      <td>1.170471</td>\n",
       "      <td>1.404676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.672015</td>\n",
       "      <td>0.446237</td>\n",
       "      <td>0.571783</td>\n",
       "      <td>0.039601</td>\n",
       "      <td>0.115734</td>\n",
       "      <td>0.052979</td>\n",
       "      <td>1.107934</td>\n",
       "      <td>0.904983</td>\n",
       "      <td>0.107175</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.099870</td>\n",
       "      <td>0.055856</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.053868</td>\n",
       "      <td>0.040503</td>\n",
       "      <td>0.087358</td>\n",
       "      <td>0.047418</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>0.040073</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.524180</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.535646</td>\n",
       "      <td>0.531792</td>\n",
       "      <td>0.513215</td>\n",
       "      <td>0.817112</td>\n",
       "      <td>0.901747</td>\n",
       "      <td>0.635815</td>\n",
       "      <td>0.656796</td>\n",
       "      <td>0.576189</td>\n",
       "      <td>0.618453</td>\n",
       "      <td>0.673807</td>\n",
       "      <td>0.594721</td>\n",
       "      <td>0.808187</td>\n",
       "      <td>0.651088</td>\n",
       "      <td>0.473595</td>\n",
       "      <td>0.330866</td>\n",
       "      <td>0.397667</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.524928</td>\n",
       "      <td>0.620066</td>\n",
       "      <td>0.745573</td>\n",
       "      <td>0.710783</td>\n",
       "      <td>0.565665</td>\n",
       "      <td>0.945226</td>\n",
       "      <td>0.770273</td>\n",
       "      <td>0.688124</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.560879</td>\n",
       "      <td>1.354523</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.045185</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.819833</td>\n",
       "      <td>0.913698</td>\n",
       "      <td>0.649664</td>\n",
       "      <td>0.359704</td>\n",
       "      <td>0.504921</td>\n",
       "      <td>0.740665</td>\n",
       "      <td>0.464555</td>\n",
       "      <td>0.641880</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>0.226378</td>\n",
       "      <td>2.917394</td>\n",
       "      <td>0.087697</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.289837</td>\n",
       "      <td>0.354238</td>\n",
       "      <td>0.248936</td>\n",
       "      <td>0.504041</td>\n",
       "      <td>0.505460</td>\n",
       "      <td>0.619265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.554910</td>\n",
       "      <td>1.382055</td>\n",
       "      <td>0.567323</td>\n",
       "      <td>0.561602</td>\n",
       "      <td>1.280338</td>\n",
       "      <td>0.792015</td>\n",
       "      <td>0.708159</td>\n",
       "      <td>0.708745</td>\n",
       "      <td>0.898291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957395</td>\n",
       "      <td>0.562188</td>\n",
       "      <td>0.819856</td>\n",
       "      <td>0.163199</td>\n",
       "      <td>0.224330</td>\n",
       "      <td>0.173441</td>\n",
       "      <td>0.801165</td>\n",
       "      <td>0.956839</td>\n",
       "      <td>0.312479</td>\n",
       "      <td>0.041273</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>0.056459</td>\n",
       "      <td>1.022089</td>\n",
       "      <td>0.635058</td>\n",
       "      <td>1.005747</td>\n",
       "      <td>0.179896</td>\n",
       "      <td>0.064578</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.181036</td>\n",
       "      <td>0.048149</td>\n",
       "      <td>0.480815</td>\n",
       "      <td>0.729741</td>\n",
       "      <td>0.462582</td>\n",
       "      <td>0.460121</td>\n",
       "      <td>0.608341</td>\n",
       "      <td>0.549104</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.670257</td>\n",
       "      <td>0.591085</td>\n",
       "      <td>0.579791</td>\n",
       "      <td>0.664771</td>\n",
       "      <td>0.602460</td>\n",
       "      <td>0.603417</td>\n",
       "      <td>0.707444</td>\n",
       "      <td>0.735617</td>\n",
       "      <td>0.495377</td>\n",
       "      <td>0.345699</td>\n",
       "      <td>0.378226</td>\n",
       "      <td>0.907092</td>\n",
       "      <td>0.770608</td>\n",
       "      <td>0.808978</td>\n",
       "      <td>0.877194</td>\n",
       "      <td>0.838816</td>\n",
       "      <td>0.724038</td>\n",
       "      <td>0.822038</td>\n",
       "      <td>0.868243</td>\n",
       "      <td>0.608072</td>\n",
       "      <td>0.487991</td>\n",
       "      <td>0.539289</td>\n",
       "      <td>1.372391</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.048345</td>\n",
       "      <td>0.021081</td>\n",
       "      <td>0.894643</td>\n",
       "      <td>1.013949</td>\n",
       "      <td>0.863872</td>\n",
       "      <td>0.427929</td>\n",
       "      <td>0.694537</td>\n",
       "      <td>1.070250</td>\n",
       "      <td>0.550201</td>\n",
       "      <td>0.885370</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.013178</td>\n",
       "      <td>0.007697</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.011320</td>\n",
       "      <td>0.252883</td>\n",
       "      <td>2.917394</td>\n",
       "      <td>0.126117</td>\n",
       "      <td>0.092158</td>\n",
       "      <td>0.162265</td>\n",
       "      <td>0.366780</td>\n",
       "      <td>0.466114</td>\n",
       "      <td>0.270763</td>\n",
       "      <td>0.627109</td>\n",
       "      <td>0.623044</td>\n",
       "      <td>0.643914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.544929</td>\n",
       "      <td>1.381215</td>\n",
       "      <td>0.566360</td>\n",
       "      <td>0.555322</td>\n",
       "      <td>1.281383</td>\n",
       "      <td>0.909352</td>\n",
       "      <td>0.896445</td>\n",
       "      <td>0.874527</td>\n",
       "      <td>0.921214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.561792</td>\n",
       "      <td>0.350904</td>\n",
       "      <td>0.491883</td>\n",
       "      <td>0.056532</td>\n",
       "      <td>0.082865</td>\n",
       "      <td>0.064485</td>\n",
       "      <td>1.039179</td>\n",
       "      <td>1.094883</td>\n",
       "      <td>0.312703</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.081323</td>\n",
       "      <td>0.046669</td>\n",
       "      <td>0.419925</td>\n",
       "      <td>0.205901</td>\n",
       "      <td>0.057822</td>\n",
       "      <td>0.055831</td>\n",
       "      <td>0.048448</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>0.022199</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.606517</td>\n",
       "      <td>1.033365</td>\n",
       "      <td>0.620611</td>\n",
       "      <td>0.615893</td>\n",
       "      <td>1.241831</td>\n",
       "      <td>1.172710</td>\n",
       "      <td>1.031463</td>\n",
       "      <td>0.887168</td>\n",
       "      <td>0.465441</td>\n",
       "      <td>0.611708</td>\n",
       "      <td>0.857941</td>\n",
       "      <td>0.462305</td>\n",
       "      <td>0.609903</td>\n",
       "      <td>1.050166</td>\n",
       "      <td>0.935664</td>\n",
       "      <td>0.865137</td>\n",
       "      <td>0.665295</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>1.289184</td>\n",
       "      <td>0.642040</td>\n",
       "      <td>1.106642</td>\n",
       "      <td>1.259622</td>\n",
       "      <td>0.611706</td>\n",
       "      <td>1.106358</td>\n",
       "      <td>1.130183</td>\n",
       "      <td>0.983077</td>\n",
       "      <td>1.199619</td>\n",
       "      <td>1.042449</td>\n",
       "      <td>1.061715</td>\n",
       "      <td>1.354523</td>\n",
       "      <td>0.048719</td>\n",
       "      <td>0.032136</td>\n",
       "      <td>0.028412</td>\n",
       "      <td>0.966119</td>\n",
       "      <td>1.133813</td>\n",
       "      <td>0.504825</td>\n",
       "      <td>0.259509</td>\n",
       "      <td>0.421536</td>\n",
       "      <td>0.609912</td>\n",
       "      <td>0.335555</td>\n",
       "      <td>0.540843</td>\n",
       "      <td>0.937813</td>\n",
       "      <td>0.926799</td>\n",
       "      <td>1.243093</td>\n",
       "      <td>1.236686</td>\n",
       "      <td>0.894295</td>\n",
       "      <td>0.887631</td>\n",
       "      <td>0.039136</td>\n",
       "      <td>2.917394</td>\n",
       "      <td>0.640581</td>\n",
       "      <td>0.531188</td>\n",
       "      <td>0.606989</td>\n",
       "      <td>0.693196</td>\n",
       "      <td>0.959543</td>\n",
       "      <td>0.496605</td>\n",
       "      <td>1.076024</td>\n",
       "      <td>1.204980</td>\n",
       "      <td>1.090948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0.480486</td>\n",
       "      <td>1.341183</td>\n",
       "      <td>0.489112</td>\n",
       "      <td>0.487549</td>\n",
       "      <td>1.256398</td>\n",
       "      <td>0.252141</td>\n",
       "      <td>0.244330</td>\n",
       "      <td>0.245144</td>\n",
       "      <td>0.288890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.565315</td>\n",
       "      <td>1.321725</td>\n",
       "      <td>1.569553</td>\n",
       "      <td>0.574343</td>\n",
       "      <td>0.649250</td>\n",
       "      <td>0.590675</td>\n",
       "      <td>0.837791</td>\n",
       "      <td>0.979682</td>\n",
       "      <td>1.145622</td>\n",
       "      <td>0.396290</td>\n",
       "      <td>0.776229</td>\n",
       "      <td>0.123759</td>\n",
       "      <td>0.826143</td>\n",
       "      <td>0.577025</td>\n",
       "      <td>0.009512</td>\n",
       "      <td>0.040777</td>\n",
       "      <td>0.035051</td>\n",
       "      <td>0.534702</td>\n",
       "      <td>0.624036</td>\n",
       "      <td>0.452346</td>\n",
       "      <td>0.522916</td>\n",
       "      <td>1.092504</td>\n",
       "      <td>0.529362</td>\n",
       "      <td>0.517149</td>\n",
       "      <td>1.228774</td>\n",
       "      <td>0.584121</td>\n",
       "      <td>0.620062</td>\n",
       "      <td>0.553654</td>\n",
       "      <td>0.863332</td>\n",
       "      <td>0.573639</td>\n",
       "      <td>0.600963</td>\n",
       "      <td>0.930218</td>\n",
       "      <td>0.619869</td>\n",
       "      <td>0.561321</td>\n",
       "      <td>0.734626</td>\n",
       "      <td>0.339067</td>\n",
       "      <td>0.215602</td>\n",
       "      <td>0.328815</td>\n",
       "      <td>0.749016</td>\n",
       "      <td>0.494618</td>\n",
       "      <td>0.593022</td>\n",
       "      <td>0.744752</td>\n",
       "      <td>0.829775</td>\n",
       "      <td>0.592210</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.836390</td>\n",
       "      <td>0.545412</td>\n",
       "      <td>0.409078</td>\n",
       "      <td>0.541881</td>\n",
       "      <td>1.361752</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.113724</td>\n",
       "      <td>0.048085</td>\n",
       "      <td>0.971390</td>\n",
       "      <td>1.050737</td>\n",
       "      <td>1.993703</td>\n",
       "      <td>1.293977</td>\n",
       "      <td>1.877849</td>\n",
       "      <td>1.149141</td>\n",
       "      <td>0.982393</td>\n",
       "      <td>1.251839</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.014939</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>1.453611</td>\n",
       "      <td>2.917395</td>\n",
       "      <td>0.505271</td>\n",
       "      <td>0.292066</td>\n",
       "      <td>0.505539</td>\n",
       "      <td>0.374949</td>\n",
       "      <td>0.458194</td>\n",
       "      <td>0.239406</td>\n",
       "      <td>0.761277</td>\n",
       "      <td>0.717705</td>\n",
       "      <td>0.738836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol_id  feature_00_var  feature_01_var  feature_02_var  feature_03_var  \\\n",
       "0          3        0.552755        1.365154        0.566029        0.561250   \n",
       "1          0        0.548992        1.382129        0.561656        0.557211   \n",
       "2          9        0.554910        1.382055        0.567323        0.561602   \n",
       "3         15        0.544929        1.381215        0.566360        0.555322   \n",
       "4         12        0.480486        1.341183        0.489112        0.487549   \n",
       "\n",
       "   feature_04_var  feature_05_var  feature_06_var  feature_07_var  \\\n",
       "0        1.262979        0.556862        0.565860        0.548872   \n",
       "1        1.282489        1.266256        1.196652        1.170471   \n",
       "2        1.280338        0.792015        0.708159        0.708745   \n",
       "3        1.281383        0.909352        0.896445        0.874527   \n",
       "4        1.256398        0.252141        0.244330        0.245144   \n",
       "\n",
       "   feature_08_var  feature_09_var  feature_10_var  feature_11_var  \\\n",
       "0        0.524954             0.0             0.0             0.0   \n",
       "1        1.404676             0.0             0.0             0.0   \n",
       "2        0.898291             0.0             0.0             0.0   \n",
       "3        0.921214             0.0             0.0             0.0   \n",
       "4        0.288890             0.0             0.0             0.0   \n",
       "\n",
       "   feature_12_var  feature_13_var  feature_14_var  feature_15_var  \\\n",
       "0        1.387970        0.594805        1.107079        0.337759   \n",
       "1        0.672015        0.446237        0.571783        0.039601   \n",
       "2        0.957395        0.562188        0.819856        0.163199   \n",
       "3        0.561792        0.350904        0.491883        0.056532   \n",
       "4        1.565315        1.321725        1.569553        0.574343   \n",
       "\n",
       "   feature_16_var  feature_17_var  feature_18_var  feature_19_var  \\\n",
       "0        0.239664        0.356054        0.969259        1.117848   \n",
       "1        0.115734        0.052979        1.107934        0.904983   \n",
       "2        0.224330        0.173441        0.801165        0.956839   \n",
       "3        0.082865        0.064485        1.039179        1.094883   \n",
       "4        0.649250        0.590675        0.837791        0.979682   \n",
       "\n",
       "   feature_20_var  feature_21_var  feature_22_var  feature_23_var  \\\n",
       "0        0.522853        0.028281        0.461378        0.099372   \n",
       "1        0.107175        0.003992        0.099870        0.055856   \n",
       "2        0.312479        0.041273        0.190669        0.056459   \n",
       "3        0.312703        0.002599        0.081323        0.046669   \n",
       "4        1.145622        0.396290        0.776229        0.123759   \n",
       "\n",
       "   feature_24_var  feature_25_var  feature_26_var  feature_27_var  \\\n",
       "0        3.379281        0.992878        0.005527        0.080140   \n",
       "1        0.279330        0.053868        0.040503        0.087358   \n",
       "2        1.022089        0.635058        1.005747        0.179896   \n",
       "3        0.419925        0.205901        0.057822        0.055831   \n",
       "4        0.826143        0.577025        0.009512        0.040777   \n",
       "\n",
       "   feature_28_var  feature_29_var  feature_30_var  feature_31_var  \\\n",
       "0        0.068642        0.016971        0.038842        0.032799   \n",
       "1        0.047418        0.007202        0.040073        0.004311   \n",
       "2        0.064578        0.005952        0.181036        0.048149   \n",
       "3        0.048448        0.011935        0.022199        0.002624   \n",
       "4        0.035051        0.534702        0.624036        0.452346   \n",
       "\n",
       "   feature_32_var  feature_33_var  feature_34_var  feature_35_var  \\\n",
       "0        0.448151        1.215907        0.363069        0.364058   \n",
       "1        0.524180        0.625698        0.535646        0.531792   \n",
       "2        0.480815        0.729741        0.462582        0.460121   \n",
       "3        0.606517        1.033365        0.620611        0.615893   \n",
       "4        0.522916        1.092504        0.529362        0.517149   \n",
       "\n",
       "   feature_36_var  feature_37_var  feature_38_var  feature_39_var  \\\n",
       "0        1.244993        0.631386        0.673492        0.720899   \n",
       "1        0.513215        0.817112        0.901747        0.635815   \n",
       "2        0.608341        0.549104        0.834108        0.670257   \n",
       "3        1.241831        1.172710        1.031463        0.887168   \n",
       "4        1.228774        0.584121        0.620062        0.553654   \n",
       "\n",
       "   feature_40_var  feature_41_var  feature_42_var  feature_43_var  \\\n",
       "0        0.287284        0.431883        0.654224        0.285330   \n",
       "1        0.656796        0.576189        0.618453        0.673807   \n",
       "2        0.591085        0.579791        0.664771        0.602460   \n",
       "3        0.465441        0.611708        0.857941        0.462305   \n",
       "4        0.863332        0.573639        0.600963        0.930218   \n",
       "\n",
       "   feature_44_var  feature_45_var  feature_46_var  feature_47_var  \\\n",
       "0        0.423189        0.876377        0.769831        0.510614   \n",
       "1        0.594721        0.808187        0.651088        0.473595   \n",
       "2        0.603417        0.707444        0.735617        0.495377   \n",
       "3        0.609903        1.050166        0.935664        0.865137   \n",
       "4        0.619869        0.561321        0.734626        0.339067   \n",
       "\n",
       "   feature_48_var  feature_49_var  feature_50_var  feature_51_var  \\\n",
       "0        0.397635        0.430001        1.174583        0.449479   \n",
       "1        0.330866        0.397667        0.784188        0.524928   \n",
       "2        0.345699        0.378226        0.907092        0.770608   \n",
       "3        0.665295        0.810526        1.289184        0.642040   \n",
       "4        0.215602        0.328815        0.749016        0.494618   \n",
       "\n",
       "   feature_52_var  feature_53_var  feature_54_var  feature_55_var  \\\n",
       "0        0.920802        1.114798        0.405241        0.851618   \n",
       "1        0.620066        0.745573        0.710783        0.565665   \n",
       "2        0.808978        0.877194        0.838816        0.724038   \n",
       "3        1.106642        1.259622        0.611706        1.106358   \n",
       "4        0.593022        0.744752        0.829775        0.592210   \n",
       "\n",
       "   feature_56_var  feature_57_var  feature_58_var  feature_59_var  \\\n",
       "0        0.841864        1.185929        0.756390        0.696308   \n",
       "1        0.945226        0.770273        0.688124        0.495845   \n",
       "2        0.822038        0.868243        0.608072        0.487991   \n",
       "3        1.130183        0.983077        1.199619        1.042449   \n",
       "4        0.670886        0.836390        0.545412        0.409078   \n",
       "\n",
       "   feature_60_var  feature_61_var  feature_62_var  feature_63_var  \\\n",
       "0        0.687349        1.318248        0.014069        0.013905   \n",
       "1        0.560879        1.354523        0.005005        0.045185   \n",
       "2        0.539289        1.372391        0.012662        0.048345   \n",
       "3        1.061715        1.354523        0.048719        0.032136   \n",
       "4        0.541881        1.361752        0.025641        0.113724   \n",
       "\n",
       "   feature_64_var  feature_65_var  feature_66_var  feature_67_var  \\\n",
       "0        0.012386        0.890098        0.984169        1.197771   \n",
       "1        0.011303        0.819833        0.913698        0.649664   \n",
       "2        0.021081        0.894643        1.013949        0.863872   \n",
       "3        0.028412        0.966119        1.133813        0.504825   \n",
       "4        0.048085        0.971390        1.050737        1.993703   \n",
       "\n",
       "   feature_68_var  feature_69_var  feature_70_var  feature_71_var  \\\n",
       "0        0.430643        0.882722        1.503357        0.547872   \n",
       "1        0.359704        0.504921        0.740665        0.464555   \n",
       "2        0.427929        0.694537        1.070250        0.550201   \n",
       "3        0.259509        0.421536        0.609912        0.335555   \n",
       "4        1.293977        1.877849        1.149141        0.982393   \n",
       "\n",
       "   feature_72_var  feature_73_var  feature_74_var  feature_75_var  \\\n",
       "0        1.121680        0.803579        0.772007        0.893878   \n",
       "1        0.641880        0.005279        0.008474        0.003289   \n",
       "2        0.885370        0.006811        0.013178        0.007697   \n",
       "3        0.540843        0.937813        0.926799        1.243093   \n",
       "4        1.251839        0.005265        0.014939        0.002925   \n",
       "\n",
       "   feature_76_var  feature_77_var  feature_78_var  weight_var  \\\n",
       "0        0.867212        0.739379        0.711066    0.119436   \n",
       "1        0.005716        0.003751        0.006334    0.226378   \n",
       "2        0.012359        0.005860        0.011320    0.252883   \n",
       "3        1.236686        0.894295        0.887631    0.039136   \n",
       "4        0.009967        0.003702        0.011298    1.453611   \n",
       "\n",
       "   time_bucket_var  responder_0_var  responder_1_var  responder_2_var  \\\n",
       "0         2.917395         0.616058         0.531589         0.495747   \n",
       "1         2.917394         0.087697         0.050360         0.098249   \n",
       "2         2.917394         0.126117         0.092158         0.162265   \n",
       "3         2.917394         0.640581         0.531188         0.606989   \n",
       "4         2.917395         0.505271         0.292066         0.505539   \n",
       "\n",
       "   responder_3_var  responder_4_var  responder_5_var  responder_6_var  \\\n",
       "0         0.412397         0.544061         0.277634         0.718410   \n",
       "1         0.289837         0.354238         0.248936         0.504041   \n",
       "2         0.366780         0.466114         0.270763         0.627109   \n",
       "3         0.693196         0.959543         0.496605         1.076024   \n",
       "4         0.374949         0.458194         0.239406         0.761277   \n",
       "\n",
       "   responder_7_var  responder_8_var  \n",
       "0         0.791492         0.655386  \n",
       "1         0.505460         0.619265  \n",
       "2         0.623044         0.643914  \n",
       "3         1.204980         1.090948  \n",
       "4         0.717705         0.738836  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5667b5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_00_var     0.547226\n",
       "feature_01_var     1.375401\n",
       "feature_02_var     0.560970\n",
       "feature_03_var     0.555015\n",
       "feature_04_var     1.273144\n",
       "feature_05_var     0.828063\n",
       "feature_06_var     0.798979\n",
       "feature_07_var     0.784331\n",
       "feature_08_var     0.876229\n",
       "feature_09_var     0.000000\n",
       "feature_10_var     0.000000\n",
       "feature_11_var     0.000000\n",
       "feature_12_var     1.071622\n",
       "feature_13_var     0.602902\n",
       "feature_14_var     0.930499\n",
       "feature_15_var     0.171979\n",
       "feature_16_var     0.276320\n",
       "feature_17_var     0.181769\n",
       "feature_18_var     1.000998\n",
       "feature_19_var     1.028265\n",
       "feature_20_var     0.400267\n",
       "feature_21_var     0.056166\n",
       "feature_22_var     0.240312\n",
       "feature_23_var     0.066352\n",
       "feature_24_var     0.661134\n",
       "feature_25_var     0.254777\n",
       "feature_26_var     0.093861\n",
       "feature_27_var     0.071554\n",
       "feature_28_var     0.055915\n",
       "feature_29_var     0.065506\n",
       "feature_30_var     0.090546\n",
       "feature_31_var     0.058873\n",
       "feature_32_var     0.514277\n",
       "feature_33_var     0.858567\n",
       "feature_34_var     0.519349\n",
       "feature_35_var     0.516355\n",
       "feature_36_var     0.853943\n",
       "feature_37_var     0.787425\n",
       "feature_38_var     0.812481\n",
       "feature_39_var     0.705985\n",
       "feature_40_var     0.585164\n",
       "feature_41_var     0.595504\n",
       "feature_42_var     0.716894\n",
       "feature_43_var     0.592900\n",
       "feature_44_var     0.616055\n",
       "feature_45_var     0.794048\n",
       "feature_46_var     0.823152\n",
       "feature_47_var     0.538329\n",
       "feature_48_var     0.415972\n",
       "feature_49_var     0.490695\n",
       "feature_50_var     0.988527\n",
       "feature_51_var     0.673659\n",
       "feature_52_var     0.822472\n",
       "feature_53_var     1.000416\n",
       "feature_54_var     0.788138\n",
       "feature_55_var     0.810112\n",
       "feature_56_var     0.896594\n",
       "feature_57_var     1.001353\n",
       "feature_58_var     0.811710\n",
       "feature_59_var     0.664488\n",
       "feature_60_var     0.729234\n",
       "feature_61_var     1.360677\n",
       "feature_62_var     0.016626\n",
       "feature_63_var     0.064079\n",
       "feature_64_var     0.026349\n",
       "feature_65_var     1.005885\n",
       "feature_66_var     1.014518\n",
       "feature_67_var     1.072707\n",
       "feature_68_var     0.504382\n",
       "feature_69_var     0.882573\n",
       "feature_70_var     1.023735\n",
       "feature_71_var     0.511137\n",
       "feature_72_var     0.877436\n",
       "feature_73_var     0.445522\n",
       "feature_74_var     0.442895\n",
       "feature_75_var     0.515421\n",
       "feature_76_var     0.510246\n",
       "feature_77_var     0.419122\n",
       "feature_78_var     0.413096\n",
       "weight_var         0.240844\n",
       "time_bucket_var    2.917394\n",
       "responder_0_var    0.302060\n",
       "responder_1_var    0.255383\n",
       "responder_2_var    0.303123\n",
       "responder_3_var    0.457497\n",
       "responder_4_var    0.601421\n",
       "responder_5_var    0.323820\n",
       "responder_6_var    0.797263\n",
       "responder_7_var    0.850519\n",
       "responder_8_var    0.775510\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_cols = [c for c in df_val.columns if c.endswith(\"_var\")]\n",
    "df_val[var_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "near_constant_cols = (\n",
    "    df_val[var_cols].mean(axis=0, skipna=True).loc[lambda s: s < 1e-10].index.str.replace(\"_var\", \"\").tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c51e005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_09', 'feature_10', 'feature_11']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "near_constant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7751f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_09', 'feature_10', 'feature_11']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_val[var_cols].mean(axis=0, skipna=True).loc[lambda s: s < 1e-4].index.str.replace(\"_var\", \"\").tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fad71c",
   "metadata": {},
   "source": [
    "Streak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = lc.collect_schema().names()\n",
    "cols = [c for c in cols if c not in KEYS]\n",
    "\n",
    "df_u = lc.group_by(g_sym).agg([\n",
    "    pl.col(c).n_unique().alias(f\"{c}__nunq_in_streak\") for c in cols\n",
    "]).collect().to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f017f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 显示pandas 所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7849d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc.select(pl.col([\"symbol_id\", \"date_id\", \"time_id\", \"feature_09\", \"feature_10\", \"feature_11\"])).sort([\"symbol_id\", \"date_id\", \"time_id\"]).slice(0, 200).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df4cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc.select(pl.col([\"symbol_id\", \"date_id\", \"time_id\", \"feature_09\", \"feature_10\", \"feature_11\"])).sort([\"symbol_id\", \"time_id\", \"date_id\"]).slice(0, 200).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14630e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_univalue = lc.group_by(\"symbol_id\").agg([\n",
    "    pl.col(\"feature_09\").n_unique().alias(\"feature_09__nunq_in_streak\"),\n",
    "    pl.col(\"feature_10\").n_unique().alias(\"feature_10__nunq_in_streak\"),\n",
    "    pl.col(\"feature_11\").n_unique().alias(\"feature_11__nunq_in_streak\"),\n",
    "]).sort(\"symbol_id\").collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_univalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols = [\"feature_09\", \"feature_10\", \"feature_11\"]\n",
    "by = [\"symbol_id\"]\n",
    "\n",
    "audit_const = lf_data.select([\n",
    "    *[pl.col(c).n_unique().over(by).alias(f\"{c}__nunq_in_streak\") for c in test_cols]\n",
    "]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境与依赖\n",
    "\n",
    "# 基础包\n",
    "import tempfile\n",
    "\n",
    "import os, gc, glob, json, yaml, time\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "import lightgbm as lgb\n",
    "from dataclasses import dataclass\n",
    "import pyarrow.parquet as pq\n",
    "from typing import Sequence, Optional, Union, List, Tuple, Iterable, Mapping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Azure & 文件系统\n",
    "import fsspec\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # 默认会加载当前目录下的 .env 文件\n",
    "\n",
    "\n",
    "# 连接云空间\n",
    "\n",
    "ACC = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "KEY = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "if not ACC or not KEY:\n",
    "    raise RuntimeError(\"Azure credentials not found. Please set them in .env\")\n",
    "storage_options = {\"account_name\": ACC, \"account_key\": KEY}\n",
    "fs = fsspec.filesystem(\"az\", **storage_options)\n",
    "\n",
    "\n",
    "\n",
    "# 定义路径辅助函数\n",
    "\n",
    "# 读取配置（唯一来源）\n",
    "cfg = yaml.safe_load(open(\"config/data.yaml\"))\n",
    "\n",
    "# 路径辅助函数\n",
    "def P(kind: str, subpath: str = \"\") -> str:\n",
    "    container  = str(cfg[\"blob\"][\"container\"]).strip(\"/\")\n",
    "    prefix     = str(cfg[\"blob\"][\"prefix\"]).strip(\"/\")\n",
    "    version    = str(cfg[\"exp_root\"]).strip(\"/\")\n",
    "    local_root = Path(cfg[\"local\"][\"root\"])\n",
    "\n",
    "    sub = str(subpath).strip(\"/\")  # 只做最小化处理；你也可以直接用 subpath\n",
    "\n",
    "    if kind == \"az\":\n",
    "        base = f\"az://{container}\" + (f\"/{prefix}\" if prefix else \"\") + f\"/{version}\"\n",
    "        return f\"{base}/{sub}\" if sub else base\n",
    "    if kind == \"np\":\n",
    "        base = f\"{container}\" + (f\"/{prefix}\" if prefix else \"\") + f\"/{version}\"\n",
    "        return f\"{base}/{sub}\" if sub else base\n",
    "    if kind == \"local\":\n",
    "        base = (local_root / version).as_posix()\n",
    "        return f\"{base}/{sub}\" if sub else base\n",
    "    raise ValueError(\"kind must be 'az', 'np', or 'local'\")\n",
    "\n",
    "\n",
    "# 全局变量\n",
    "KEYS = cfg['keys']\n",
    "WEIGHT = cfg['weight']\n",
    "TIME_SORT = cfg['sorts']['time_major']\n",
    "\n",
    "FEATURE_ALL = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "RESP_COLS   = [f\"responder_{i}\" for i in range(0, 9)]\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "\n",
    "\n",
    "np_paths = fs.glob(f\"az://jackson/js_exp/raw/train.parquet/partition_id=[0-9]/*.parquet\")\n",
    "\n",
    "paths=[]\n",
    "for p in np_paths:\n",
    "    paths.append(\"az://\"+p)\n",
    "lb = pl.scan_parquet(paths, storage_options=storage_options)\n",
    "DATE_LO = int(1000)\n",
    "DATE_HI = int(1698)\n",
    "lb = lb.filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI, closed=\"both\"))\n",
    "lb = lb.sort(KEYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c04c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.select(pl.col(\"weight\").min()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e28aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1300-1500__cv3-g7-r4__seed42__top1000__1760299442__range1000-1600__range1000-1600__cv2-g7-r4__1760347190.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_features.iloc[500:])['mean_gain'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bc8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1300-1500__cv3-g7-r4__seed42__top1000__1760299442__range1000-1600__range1000-1600__cv2-g7-r4__1760347190.csv\") as f:\n",
    "    features = f.read().splitlines()\n",
    "    \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_path = \"az://jackson/js_exp/exp/v1/panel_shards/panel_0815_0844.parquet\"\n",
    "\n",
    "lx = pl.scan_parquet(t_path, storage_options=storage_options)\n",
    "names = lx.collect_schema().names()\n",
    "print(f\"feat number: {len(names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32703529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计行数\n",
    "lb.select(pl.count()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfa13f",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9c07f",
   "metadata": {},
   "source": [
    "## times + clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79163dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time features\n",
    "# 添加时间特征 bucket，将一天T ticks分为B部分\n",
    "B = cfg['trading']['bucket_size']\n",
    "T = cfg['trading']['ticks']\n",
    "\n",
    "def clip_upper(expr: pl.Expr, ub: int) -> pl.Expr:\n",
    "    return pl.when(expr > pl.lit(ub)).then(pl.lit(ub)).otherwise(expr)\n",
    "lb = lb.with_columns(\n",
    "    bucket_raw = pl.col('time_id') * pl.lit(B) // pl.lit(T) # 这里T 我们假设为全局常数，不分组计算\n",
    ").with_columns(\n",
    "    time_bucket = clip_upper(pl.col('bucket_raw'), B - 1).cast(pl.UInt8)\n",
    ").drop(pl.col('bucket_raw'))\n",
    "\n",
    "\n",
    "\n",
    "# Clipping\n",
    "\n",
    "def rolling_sigma_clip(\n",
    "    lf: pl.LazyFrame,\n",
    "    clip_features: Sequence[str],\n",
    "    over_cols: Sequence[str],\n",
    "    *,\n",
    "    is_sorted: bool = False,\n",
    "    window: int = 50,\n",
    "    k: float = 3.0,\n",
    "    ddof: int = 1,\n",
    "    min_valid: int = 10,\n",
    "    cast_float32: bool = True,\n",
    "    sanitize: bool = True,\n",
    ") -> pl.LazyFrame:\n",
    "    if not is_sorted:\n",
    "        raise ValueError(\"Input LazyFrame must be pre-sorted by ['symbol_id','date_id','time_id']\")\n",
    "\n",
    "    required = {\"symbol_id\",\"date_id\",\"time_id\",\"time_bucket\"} | set(clip_features)\n",
    "    names = set(lf.collect_schema().names())\n",
    "    missing = list(required - names)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns: {missing}\")\n",
    "\n",
    "\n",
    "    base = lf.select(pl.col([\"symbol_id\",\"date_id\",\"time_id\",\"time_bucket\"] + list(clip_features)))\n",
    "    min_need = max(min_valid, ddof + 1)\n",
    "    min_samp = ddof + 1\n",
    "\n",
    "    exprs = []\n",
    "    for c in clip_features:\n",
    "        x = pl.col(c)\n",
    "        if cast_float32:\n",
    "            x = x.cast(pl.Float32)\n",
    "        if sanitize:\n",
    "            x = pl.when(x.is_finite()).then(x).otherwise(None)\n",
    "\n",
    "        # 注意：这里不要 over\n",
    "        xlag = x.shift(1)\n",
    "\n",
    "        # 只在 rolling 结果上 over（组内历史）\n",
    "        cnt = (\n",
    "            xlag.is_not_null()\n",
    "                .cast(pl.Int32)\n",
    "                .rolling_sum(window_size=window, min_samples=ddof + 1)\n",
    "        ).over(over_cols)\n",
    "\n",
    "        mu = (\n",
    "            xlag.rolling_mean(window_size=window, min_samples=ddof + 1)\n",
    "        ).over(over_cols)\n",
    "\n",
    "        sd = (\n",
    "            xlag.rolling_std(window_size=window, ddof=ddof, min_samples=ddof + 1)\n",
    "        ).over(over_cols)\n",
    "\n",
    "        lo, hi = mu - k * sd, mu + k * sd\n",
    "        exprs.append(\n",
    "            pl.when(cnt >= max(min_valid, ddof + 1))\n",
    "            .then(x.clip(lo, hi))\n",
    "            .otherwise(x)\n",
    "            .alias(c)\n",
    "        )\n",
    "\n",
    "    return base.with_columns(exprs)\n",
    "\n",
    "\n",
    "lb = lb.sort(KEYS)\n",
    "\n",
    "lf_clip = rolling_sigma_clip(\n",
    "    lf=lb,\n",
    "    clip_features=FEATURE_ALL,\n",
    "    over_cols=cfg['winsorization']['groupby'],\n",
    "    is_sorted=True,\n",
    "    window=cfg['winsorization']['window'],\n",
    "    k=cfg['winsorization']['z_k'],\n",
    "    ddof=cfg['winsorization']['ddof'],\n",
    "    min_valid=cfg['winsorization']['min_valid'],\n",
    "    cast_float32=cfg['winsorization']['cast_float32'],\n",
    "    sanitize=cfg['winsorization'].get('sanitize', True)\n",
    ")\n",
    "\n",
    "lf_clip.collect_schema().names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432615f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "clip_out = Path(P(\"local\", cfg[\"paths\"][\"cache\"])) / \"sample_clipped.parquet\"\n",
    "clip_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = lf_clip.collect()  # 非流式；会把计划完整执行后落到内存\n",
    "df.write_parquet(str(clip_out), compression=\"zstd\")  # 可加 use_pyarrow=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6200fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计lf_clip行数\n",
    "lf_clip.select(pl.count()).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing\n",
    "def causal_impute(\n",
    "    lf: pl.LazyFrame,\n",
    "    impute_cols: Sequence[str],\n",
    "    *,\n",
    "    open_tick_window: Tuple[int, int] = (0, 10),\n",
    "    ttl_days_open: int = 5,\n",
    "    intra_ffill_max_gap_ticks: Optional[int] = 100,\n",
    "    ttl_days_same_tick: Optional[int] = 5,\n",
    "    is_sorted: bool = False,\n",
    ") -> pl.LazyFrame:\n",
    "    if not is_sorted:\n",
    "        raise ValueError(\"Input LazyFrame must be pre-sorted by ['symbol_id','date_id','time_id']\")\n",
    "\n",
    "    # 参数合法性\n",
    "    assert intra_ffill_max_gap_ticks is None or intra_ffill_max_gap_ticks >= 0\n",
    "    assert ttl_days_same_tick is None or ttl_days_same_tick >= 0\n",
    "\n",
    "    # 统一 dtype（可选，但更稳）\n",
    "    lf = lf.with_columns([pl.col(c).cast(pl.Float32) for c in impute_cols])\n",
    "    \n",
    "    \n",
    "    t0, t1 = open_tick_window\n",
    "    is_open = pl.col(\"time_id\").is_between(t0, t1, closed=\"left\")  # [t0, t1)\n",
    "\n",
    "    # ---- 1) 开盘：跨日承接（TTL）----\n",
    "    open_exprs = []\n",
    "    for c in impute_cols:\n",
    "        last_date = (\n",
    "            pl.when(pl.col(c).is_not_null()).then(pl.col(\"date_id\"))\n",
    "            .forward_fill().over(\"symbol_id\")\n",
    "        )\n",
    "        cand = pl.col(c).forward_fill().over(\"symbol_id\")\n",
    "        gap  = (pl.col(\"date_id\") - last_date).cast(pl.Int32)\n",
    "        open_exprs.append(\n",
    "            pl.when(is_open \n",
    "                    & pl.col(c).is_null() \n",
    "                    & (gap.fill_null(ttl_days_open + 1) <= ttl_days_open))\n",
    "            .then(cand)\n",
    "            .otherwise(pl.col(c))\n",
    "            .alias(c)\n",
    "        )\n",
    "    lf1 = lf.with_columns(open_exprs)\n",
    "\n",
    "    # ---- 2) 日内 ffill（(symbol,date)），可限步数 ----\n",
    "    if intra_ffill_max_gap_ticks is None:\n",
    "        lf2 = lf1.with_columns([pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"]).alias(c) for c in impute_cols])\n",
    "    else:\n",
    "        k = intra_ffill_max_gap_ticks\n",
    "        exprs = []\n",
    "        for c in impute_cols:\n",
    "            last_t = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"time_id\"))\n",
    "                .forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            )\n",
    "            cand = pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            gap  = (pl.col(\"time_id\") - last_t).cast(pl.Int32)\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & (gap.fill_null(k + 1) <= k))\n",
    "                .then(cand)\n",
    "                .otherwise(pl.col(c))\n",
    "                .alias(c)\n",
    "            )\n",
    "        lf2 = lf1.with_columns(exprs)\n",
    "\n",
    "    # ---- 3) 同一 time_id 跨日承接（TTL，可选）----\n",
    "    lf3 = lf2\n",
    "    if ttl_days_same_tick is not None:\n",
    "        d = ttl_days_same_tick\n",
    "        exprs = []\n",
    "        for c in impute_cols:\n",
    "            last_date_same = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"date_id\"))\n",
    "                .forward_fill().over([\"symbol_id\",\"time_id\"])\n",
    "            )\n",
    "            cand_same = pl.col(c).forward_fill().over([\"symbol_id\",\"time_id\"])\n",
    "            gap2 = (pl.col(\"date_id\") - last_date_same).cast(pl.Int32)\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & (gap2.fill_null(d + 1) <= d))\n",
    "                .then(cand_same)\n",
    "                .otherwise(pl.col(c))\n",
    "                .alias(c)\n",
    "            )\n",
    "        lf3 = lf2.with_columns(exprs)\n",
    "\n",
    "    # ---- 4) 再日内 ffill 传播（与步骤2同逻辑）----\n",
    "    if intra_ffill_max_gap_ticks is None:\n",
    "        lf4 = lf3.with_columns([pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"]).alias(c) for c in impute_cols])\n",
    "    else:\n",
    "        k = intra_ffill_max_gap_ticks\n",
    "        exprs = []\n",
    "        for c in impute_cols:\n",
    "            last_t = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"time_id\"))\n",
    "                .forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            )\n",
    "            cand = pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            gap  = (pl.col(\"time_id\") - last_t).cast(pl.Int32)\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & (gap.fill_null(k + 1) <= k))\n",
    "                .then(cand)\n",
    "                .otherwise(pl.col(c))\n",
    "                .alias(c)\n",
    "            )\n",
    "        lf4 = lf3.with_columns(exprs)\n",
    "\n",
    "    KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "    return lf4.select([*KEYS, *impute_cols])\n",
    "\n",
    "\n",
    "clip_path = Path(P(\"local\", cfg[\"paths\"][\"cache\"])) / \"sample_clipped.parquet\"\n",
    "lf_clip = pl.scan_parquet(str(clip_path))\n",
    "\n",
    "lf_clip = lf_clip.sort(KEYS)\n",
    "lf_imp = causal_impute(\n",
    "    lf=lf_clip,\n",
    "    impute_cols=FEATURE_ALL,\n",
    "    open_tick_window=cfg['fill']['open_tick_window'],\n",
    "    ttl_days_open=cfg['fill']['ttl_days_open'],\n",
    "    intra_ffill_max_gap_ticks=cfg['fill']['intra_ffill_max_gap_ticks'],\n",
    "    ttl_days_same_tick=cfg['fill']['ttl_days_same_tick'],\n",
    "    is_sorted=True\n",
    ")\n",
    "\n",
    "assert lf_imp.select(pl.len()).collect().item() == lb.select(pl.len()).collect().item()\n",
    "assert lf_imp.group_by([\"symbol_id\",\"date_id\",\"time_id\"]).len().filter(pl.col(\"len\")>1).collect().height == 0\n",
    "\n",
    "lf_imp = lf_imp.with_columns([pl.col(c).fill_null(0.0).alias(c) for c in FEATURE_ALL])\n",
    "\n",
    "\n",
    "imp_path = Path(P(\"local\", cfg[\"paths\"][\"cache\"])) / \"sample_imputed.parquet\"\n",
    "\n",
    "lf_imp.collect(streaming=True).write_parquet(str(imp_path), compression=\"zstd\", use_pyarrow=True)  # 可加 use_pyarrow=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ece4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计lf_imp行数\n",
    "lf_imp.select(pl.count()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa634be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c935a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging\n",
    "\n",
    "batch_size = cfg['fill']['batch_size']\n",
    "\n",
    "# 右表：\n",
    "rhs = (\n",
    "    lb.select([*KEYS, WEIGHT, 'time_bucket', *RESP_COLS])\n",
    "    .with_columns([pl.col(k).cast(pl.Int32) for k in KEYS]).sort(TIME_SORT)\n",
    ")\n",
    "print(\"Right table schema:\", rhs.collect_schema())\n",
    "print(\"row count:\", rhs.select(pl.count()).collect())\n",
    "\n",
    "# 左表\n",
    "imp_path = Path(P(\"local\", cfg[\"paths\"][\"cache\"])) / \"sample_imputed.parquet\"\n",
    "lf_imp = pl.scan_parquet(str(imp_path)).with_columns([pl.col(k).cast(pl.Int32) for k in KEYS])\n",
    "lf_imp = lf_imp.sort(TIME_SORT)\n",
    "\n",
    "print(\"Left table schema:\", lf_imp.collect_schema())\n",
    "print(\"row count:\", lf_imp.select(pl.count()).collect())\n",
    "\n",
    "\n",
    "dmin, dmax = (\n",
    "    lf_imp.select(\n",
    "        pl.col('date_id').min().alias('dmin'),\n",
    "        pl.col('date_id').max().alias('dmax')\n",
    "        )\n",
    "    .collect()\n",
    "    .row(0)\n",
    ")\n",
    "print(f\"Date range: {dmin} to {dmax}, total {dmax - dmin + 1} days\")\n",
    "\n",
    "path = P('az', cfg['paths']['clean_shards'])\n",
    "fs.makedirs(path, exist_ok=True)\n",
    "print(f\"Processing date range: {dmin} to {dmax}\")\n",
    "\n",
    "for lo in range(dmin, dmax + 1, batch_size):\n",
    "    hi = min(lo + batch_size, dmax + 1)\n",
    "\n",
    "    left = (\n",
    "        lf_imp\n",
    "        .filter(pl.col('date_id').is_between(lo, hi, closed='left'))\n",
    "    )\n",
    "    \n",
    "    right = rhs.filter(pl.col('date_id').is_between(lo, hi, closed='left'))\n",
    "\n",
    "    part = (left.join(right, on=TIME_SORT, how='left')).sort(TIME_SORT)\n",
    "\n",
    "    # 命名时注意 hi 是排他的，所以文件名用 hi-1\n",
    "    out_lo, out_hi = lo, hi - 1\n",
    "    (\n",
    "        part.sink_parquet(\n",
    "            f\"{path}/clean_{out_lo:04d}_{out_hi:04d}.parquet\",\n",
    "            compression=\"zstd\",\n",
    "            statistics=True,                 # 写入页/列统计划出更快\n",
    "            storage_options=storage_options,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca767b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root = P(\"az\", cfg[\"paths\"][\"clean_shards\"])\n",
    "test_path = f\"{test_root}/clean_1000_1029.parquet\"\n",
    "\n",
    "lx = pl.scan_parquet(str(test_path), storage_options=storage_options)\n",
    "result = lx.select(pl.col(\"weight\").is_null().sum()).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.limit().collect()  # 试运行一下，看看数据长啥样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be099f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = lb.select(pl.col(\"date_id\").unique().sort()).collect(streaming=True)[\"date_id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3489bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff(days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10ad82",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA：每个交易日的symbol覆盖情况,是否覆盖全程\n",
    "lf_s_d = lb.select(['date_id', 'symbol_id'])\n",
    "\n",
    "per_date = (\n",
    "    lf_s_d.group_by(\"date_id\")\n",
    "      .agg(pl.col(\"symbol_id\").n_unique().alias(\"n_symbols\"))\n",
    "      .sort(\"date_id\")\n",
    ")\n",
    "\n",
    "max_n = per_date.select(pl.max(\"n_symbols\")).collect().item()\n",
    "summary = per_date.with_columns([\n",
    "    pl.lit(max_n).alias(\"max_n\"),\n",
    "    (pl.col(\"n_symbols\") == max_n).alias(\"is_full_universe\")\n",
    "])\n",
    "\n",
    "dates_missing = summary.filter(pl.col(\"is_full_universe\") == False).select(\"date_id\")\n",
    "# summary.collect(); dates_missing.collect()\n",
    "\n",
    "dates_missing.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先选一个横向，纵向都比较小的样本,按照data_id来选一小块快速试验\n",
    "\n",
    "ls = lb.filter(pl.col('symbol_id').is_in([1,2,3,4,5]) & pl.col('date_id').is_in([1400,1420]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120e6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a362c607",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013932fc",
   "metadata": {},
   "source": [
    "添加time bucket, 将日分片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加时间特征 bucket，将一天T ticks分为B部分\n",
    "B = cfg['trading']['bucket_size']\n",
    "T = cfg['trading']['ticks']\n",
    "\n",
    "def clip_upper(expr: pl.Expr, ub: int) -> pl.Expr:\n",
    "    return pl.when(expr > pl.lit(ub)).then(pl.lit(ub)).otherwise(expr)\n",
    "lb = lb.with_columns(\n",
    "    bucket_raw = pl.col('time_id') * pl.lit(B) // pl.lit(T) # 这里T 我们假设为全局常数，不分组计算\n",
    ").with_columns(\n",
    "    time_bucket = clip_upper(pl.col('bucket_raw'), B - 1).cast(pl.UInt8)\n",
    ").drop(pl.col('bucket_raw'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dc1b49",
   "metadata": {},
   "source": [
    "Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_sigma_clip(\n",
    "    lf: pl.LazyFrame,\n",
    "    clip_features: Sequence[str],\n",
    "    over_cols: Sequence[str],\n",
    "    *,\n",
    "    is_sorted: bool = False,\n",
    "    window: int = 50,\n",
    "    k: float = 3.0,\n",
    "    ddof: int = 1,\n",
    "    min_valid: int = 10,\n",
    "    cast_float32: bool = True,\n",
    "    sanitize: bool = True,\n",
    ") -> pl.LazyFrame:\n",
    "    if not is_sorted:\n",
    "        raise ValueError(\"Input LazyFrame must be pre-sorted by ['symbol_id','date_id','time_id']\")\n",
    "\n",
    "    required = {\"symbol_id\",\"date_id\",\"time_id\",\"time_bucket\"} | set(clip_features)\n",
    "    names = set(lf.collect_schema().names())\n",
    "    missing = list(required - names)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns: {missing}\")\n",
    "\n",
    "\n",
    "    base = lf.select(pl.col([\"symbol_id\",\"date_id\",\"time_id\",\"time_bucket\"] + list(clip_features)))\n",
    "    min_need = max(min_valid, ddof + 1)\n",
    "    min_samp = ddof + 1\n",
    "\n",
    "    exprs = []\n",
    "    for c in clip_features:\n",
    "        x = pl.col(c)\n",
    "        if cast_float32:\n",
    "            x = x.cast(pl.Float32)\n",
    "        if sanitize:\n",
    "            x = pl.when(x.is_finite()).then(x).otherwise(None)\n",
    "\n",
    "        # 注意：这里不要 over\n",
    "        xlag = x.shift(1)\n",
    "\n",
    "        # 只在 rolling 结果上 over（组内历史）\n",
    "        cnt = (\n",
    "            xlag.is_not_null()\n",
    "                .cast(pl.Int32)\n",
    "                .rolling_sum(window_size=window, min_samples=ddof + 1)\n",
    "        ).over(over_cols)\n",
    "\n",
    "        mu = (\n",
    "            xlag.rolling_mean(window_size=window, min_samples=ddof + 1)\n",
    "        ).over(over_cols)\n",
    "\n",
    "        sd = (\n",
    "            xlag.rolling_std(window_size=window, ddof=ddof, min_samples=ddof + 1)\n",
    "        ).over(over_cols)\n",
    "\n",
    "        lo, hi = mu - k * sd, mu + k * sd\n",
    "        exprs.append(\n",
    "            pl.when(cnt >= max(min_valid, ddof + 1))\n",
    "            .then(x.clip(lo, hi))\n",
    "            .otherwise(x)\n",
    "            .alias(c)\n",
    "        )\n",
    "\n",
    "    return base.with_columns(exprs)\n",
    "\n",
    "\n",
    "lb = lb.sort(KEYS)\n",
    "\n",
    "lf_clip = rolling_sigma_clip(\n",
    "    lf=lb,\n",
    "    clip_features=FEATURE_ALL,\n",
    "    over_cols=cfg['winsorization']['groupby'],\n",
    "    is_sorted=True,\n",
    "    window=cfg['winsorization']['window'],\n",
    "    k=cfg['winsorization']['z_k'],\n",
    "    ddof=cfg['winsorization']['ddof'],\n",
    "    min_valid=cfg['winsorization']['min_valid'],\n",
    "    cast_float32=cfg['winsorization']['cast_float32'],\n",
    "    sanitize=cfg['winsorization'].get('sanitize', True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00ec961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "clip_out = Path(P(\"local\", cfg[\"paths\"][\"cache\"])) / \"sample_clipped.parquet\"\n",
    "clip_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = lf_clip.collect()  # 非流式；会把计划完整执行后落到内存\n",
    "df.write_parquet(str(clip_out), compression=\"zstd\")  # 可加 use_pyarrow=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d510d",
   "metadata": {},
   "source": [
    "Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e616658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_impute(\n",
    "    lf: pl.LazyFrame,\n",
    "    impute_cols: Sequence[str],\n",
    "    *,\n",
    "    open_tick_window: Tuple[int, int] = (0, 10),\n",
    "    ttl_days_open: int = 5,\n",
    "    intra_ffill_max_gap_ticks: Optional[int] = 100,\n",
    "    ttl_days_same_tick: Optional[int] = 5,\n",
    "    is_sorted: bool = False,\n",
    ") -> pl.LazyFrame:\n",
    "    if not is_sorted:\n",
    "        raise ValueError(\"Input LazyFrame must be pre-sorted by ['symbol_id','date_id','time_id']\")\n",
    "\n",
    "    # 参数合法性\n",
    "    assert intra_ffill_max_gap_ticks is None or intra_ffill_max_gap_ticks >= 0\n",
    "    assert ttl_days_same_tick is None or ttl_days_same_tick >= 0\n",
    "\n",
    "    # 统一 dtype（可选，但更稳）\n",
    "    lf = lf.with_columns([pl.col(c).cast(pl.Float32) for c in impute_cols])\n",
    "    \n",
    "    \n",
    "    t0, t1 = open_tick_window\n",
    "    is_open = pl.col(\"time_id\").is_between(t0, t1, closed=\"left\")  # [t0, t1)\n",
    "\n",
    "    # ---- 1) 开盘：跨日承接（TTL）----\n",
    "    open_exprs = []\n",
    "    for c in impute_cols:\n",
    "        last_date = (\n",
    "            pl.when(pl.col(c).is_not_null()).then(pl.col(\"date_id\"))\n",
    "            .forward_fill().over(\"symbol_id\")\n",
    "        )\n",
    "        cand = pl.col(c).forward_fill().over(\"symbol_id\")\n",
    "        gap  = (pl.col(\"date_id\") - last_date).cast(pl.Int32)\n",
    "        open_exprs.append(\n",
    "            pl.when(is_open \n",
    "                    & pl.col(c).is_null() \n",
    "                    & (gap.fill_null(ttl_days_open + 1) <= ttl_days_open))\n",
    "            .then(cand)\n",
    "            .otherwise(pl.col(c))\n",
    "            .alias(c)\n",
    "        )\n",
    "    lf1 = lf.with_columns(open_exprs)\n",
    "\n",
    "    # ---- 2) 日内 ffill（(symbol,date)），可限步数 ----\n",
    "    if intra_ffill_max_gap_ticks is None:\n",
    "        lf2 = lf1.with_columns([pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"]).alias(c) for c in impute_cols])\n",
    "    else:\n",
    "        k = intra_ffill_max_gap_ticks\n",
    "        exprs = []\n",
    "        for c in impute_cols:\n",
    "            last_t = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"time_id\"))\n",
    "                .forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            )\n",
    "            cand = pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            gap  = (pl.col(\"time_id\") - last_t).cast(pl.Int32)\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & (gap.fill_null(k + 1) <= k))\n",
    "                .then(cand)\n",
    "                .otherwise(pl.col(c))\n",
    "                .alias(c)\n",
    "            )\n",
    "        lf2 = lf1.with_columns(exprs)\n",
    "\n",
    "    # ---- 3) 同一 time_id 跨日承接（TTL，可选）----\n",
    "    lf3 = lf2\n",
    "    if ttl_days_same_tick is not None:\n",
    "        d = ttl_days_same_tick\n",
    "        exprs = []\n",
    "        for c in impute_cols:\n",
    "            last_date_same = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"date_id\"))\n",
    "                .forward_fill().over([\"symbol_id\",\"time_id\"])\n",
    "            )\n",
    "            cand_same = pl.col(c).forward_fill().over([\"symbol_id\",\"time_id\"])\n",
    "            gap2 = (pl.col(\"date_id\") - last_date_same).cast(pl.Int32)\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & (gap2.fill_null(d + 1) <= d))\n",
    "                .then(cand_same)\n",
    "                .otherwise(pl.col(c))\n",
    "                .alias(c)\n",
    "            )\n",
    "        lf3 = lf2.with_columns(exprs)\n",
    "\n",
    "    # ---- 4) 再日内 ffill 传播（与步骤2同逻辑）----\n",
    "    if intra_ffill_max_gap_ticks is None:\n",
    "        lf4 = lf3.with_columns([pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"]).alias(c) for c in impute_cols])\n",
    "    else:\n",
    "        k = intra_ffill_max_gap_ticks\n",
    "        exprs = []\n",
    "        for c in impute_cols:\n",
    "            last_t = (\n",
    "                pl.when(pl.col(c).is_not_null()).then(pl.col(\"time_id\"))\n",
    "                .forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            )\n",
    "            cand = pl.col(c).forward_fill().over([\"symbol_id\",\"date_id\"])\n",
    "            gap  = (pl.col(\"time_id\") - last_t).cast(pl.Int32)\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(c).is_null() & (gap.fill_null(k + 1) <= k))\n",
    "                .then(cand)\n",
    "                .otherwise(pl.col(c))\n",
    "                .alias(c)\n",
    "            )\n",
    "        lf4 = lf3.with_columns(exprs)\n",
    "\n",
    "    KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "    return lf4.select([*KEYS, *impute_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41829327",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_path = Path(P(\"local\", cfg[\"paths\"][\"cache\"])) / \"sample_clipped.parquet\"\n",
    "lf_clip = pl.scan_parquet(str(clip_path))\n",
    "\n",
    "lf_clip = lf_clip.sort(KEYS)\n",
    "lf_imp = causal_impute(\n",
    "    lf=lf_clip,\n",
    "    impute_cols=FEATURE_ALL,\n",
    "    open_tick_window=cfg['fill']['open_tick_window'],\n",
    "    ttl_days_open=cfg['fill']['ttl_days_open'],\n",
    "    intra_ffill_max_gap_ticks=cfg['fill']['intra_ffill_max_gap_ticks'],\n",
    "    ttl_days_same_tick=cfg['fill']['ttl_days_same_tick'],\n",
    "    is_sorted=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac16091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看缺失情况\n",
    "\n",
    "pre_null = lf_clip.select([pl.col(c).is_null().mean().alias(c) for c in FEATURE_ALL]).collect()\n",
    "post_null = lf_imp.select([pl.col(c).is_null().mean().alias(c) for c in FEATURE_ALL]).collect()\n",
    "# melt 后拼起来看变化\n",
    "pre_m = pre_null.melt(variable_name=\"feature\", value_name=\"pre_null\")\n",
    "post_m = post_null.melt(variable_name=\"feature\", value_name=\"post_null\")\n",
    "summary = pre_m.join(post_m, on=\"feature\").with_columns(\n",
    "    (pl.col(\"pre_null\") - pl.col(\"post_null\")).alias(\"filled_delta\")\n",
    ").sort(\"post_null\", descending=True)\n",
    "summary  # post_null 高的基本就是那 ~20 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59feca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按日期看缺失是否“早期更高”（排除冷启动特征）\n",
    "cols = [\"feature_21\",\"feature_26\"]  # 换成你缺失最高的几列\n",
    "by_date = (\n",
    "    lf_imp.group_by(\"date_id\")\n",
    "          .agg([pl.col(c).is_null().mean().alias(c) for c in cols])\n",
    "          .sort(\"date_id\")\n",
    "          .collect()\n",
    ")\n",
    "by_date  # 冷启动型会在较早的 date_id 更高，然后趋于稳定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2805052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这天到底有多少 symbol 是“整天全空”？\n",
    "c = \"feature_21\"\n",
    "\n",
    "daily_all_null_share = (\n",
    "    lf_imp\n",
    "    .group_by([\"date_id\",\"symbol_id\"])\n",
    "    .agg(pl.col(c).is_null().mean().alias(\"null_rate\"))\n",
    "    .with_columns((pl.col(\"null_rate\") == 1).alias(\"all_null_day\"))\n",
    "    .group_by(\"date_id\")\n",
    "    .agg(pl.mean(\"all_null_day\").alias(\"share_symbols_all_null\"))\n",
    "    .sort(\"date_id\")\n",
    "    .collect()\n",
    ")\n",
    "daily_all_null_share.filter(pl.col(\"share_symbols_all_null\") != 0)\n",
    "\n",
    "#找到原因， 是在中间的一些日期， 有1-2个symbol_id的一些特征列缺失 1/32, 1/33. 1/34 ~0.029-0.032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41f59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lf_imp.select(pl.len()).collect().item() == lb.select(pl.len()).collect().item()\n",
    "assert lf_imp.group_by([\"symbol_id\",\"date_id\",\"time_id\"]).len().filter(pl.col(\"len\")>1).collect().height == 0\n",
    "\n",
    "lf_imp = lf_imp.with_columns([pl.col(c).fill_null(0.0).alias(c) for c in FEATURE_ALL])\n",
    "\n",
    "# 再次查看缺失情况\n",
    "post_null2 = lf_imp.select([pl.col(c).is_null().mean().alias(c) for c in FEATURE_ALL]).collect()\n",
    "post_null2  # 应该全0了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67551a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54211fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_path = Path(P(\"local\", cfg[\"paths\"][\"cache\"])) / \"sample_imputed.parquet\"\n",
    "\n",
    "lf_imp.collect(streaming=True).write_parquet(str(imp_path), compression=\"zstd\", use_pyarrow=True)  # 可加 use_pyarrow=True\n",
    "\n",
    "\n",
    "# 合并响应变量\n",
    "\n",
    "\n",
    "batch_size = cfg['fill']['batch_size']\n",
    "\n",
    "# 右表：去重 + 对齐类型\n",
    "rhs = (\n",
    "    lb.select([*KEYS, WEIGHT, 'time_bucket', *RESP_COLS])\n",
    "    .with_columns([pl.col(k).cast(pl.Int32) for k in KEYS])\n",
    ")\n",
    "\n",
    "\n",
    "# 左表\n",
    "imp_path = Path(P(\"local\", cfg[\"paths\"][\"cache\"])) / \"sample_imputed.parquet\"\n",
    "lf_imp = pl.scan_parquet(str(imp_path)).with_columns([pl.col(k).cast(pl.Int32) for k in KEYS])\n",
    "\n",
    "dmin, dmax = (\n",
    "    lf_imp.select(\n",
    "        pl.col('date_id').min().alias('dmin'),\n",
    "        pl.col('date_id').max().alias('dmax')\n",
    "        )\n",
    "    .collect()\n",
    "    .row(0)\n",
    ")\n",
    "\n",
    "path = P('az', cfg['paths']['clean_shards'])\n",
    "fs.makedirs(path, exist_ok=True)\n",
    "print(f\"Processing date range: {dmin} to {dmax}\")\n",
    "\n",
    "\n",
    "for lo in range(dmin, dmax + 1, batch_size):\n",
    "    hi = min(lo + batch_size, dmax + 1)\n",
    "\n",
    "    left = (\n",
    "        lf_imp\n",
    "        .filter(pl.col('date_id').is_between(lo, hi, closed='left'))\n",
    "    )\n",
    "    right = rhs.filter(pl.col('date_id').is_between(lo, hi, closed='left')).sort(TIME_SORT).unique(subset=KEYS, keep='last')\n",
    "\n",
    "    part = (left.join(right, on=KEYS, how='left')).sort(TIME_SORT)\n",
    "    \n",
    "    feature_cols = [c for c in part.collect_schema().names() if c not in set([*KEYS, WEIGHT, 'time_bucket', *RESP_COLS])]\n",
    "    part = part.select([*KEYS, WEIGHT, 'time_bucket', *feature_cols,  *RESP_COLS])\n",
    "\n",
    "\n",
    "    # 命名时注意 hi 是排他的，所以文件名用 hi-1\n",
    "    out_lo, out_hi = lo, hi - 1\n",
    "    (\n",
    "        part.sink_parquet(\n",
    "            f\"{path}/clean_{out_lo:04d}_{out_hi:04d}.parquet\",\n",
    "            compression=\"zstd\",\n",
    "            statistics=True,                 # 写入页/列统计划出更快\n",
    "            storage_options=storage_options,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_root = P('az', cfg['paths']['clean_shards'])\n",
    "clean_paths = fs.glob(f\"{clean_root}/clean_*.parquet\")\n",
    "print(f\"Found {len(clean_paths)} clean shards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0430c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b9eb33a",
   "metadata": {},
   "source": [
    "\n",
    "# 特征工程函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85363ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "\n",
    "# A：响应列的“上一日尾部/日度摘要”\n",
    "def fe_resp_daily(\n",
    "    lf: pl.LazyFrame,\n",
    "    *,\n",
    "    keys: Tuple[str, str, str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    rep_cols: Sequence[str],\n",
    "    is_sorted: bool = False,\n",
    "    prev_soft_days: Optional[int] = None,\n",
    "    cast_f32: bool = True,\n",
    "    tail_lags: Sequence[int] = (1,),\n",
    "    tail_diffs: Sequence[int] = (1,),\n",
    "    rolling_windows: Sequence[int] | None = (3,),\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"一次日频聚合得到昨日尾部与日级摘要 → 统一 TTL 到“对 d 生效的历史值” → 回拼到 tick 级。\"\"\"\n",
    "    g_symbol, g_date, g_time = keys\n",
    "\n",
    "    # 若未保证排序，这里补一次（只影响 lf；日频表仍会再按 (symbol,date) 排）\n",
    "    if not is_sorted:\n",
    "        lf = lf.sort([g_symbol, g_date, g_time])\n",
    "\n",
    "    # --- 一次性日频聚合 ---\n",
    "    need_L = sorted(set(tail_lags) | {k+1 for k in tail_diffs} | {1})\n",
    "    agg_exprs: list[pl.Expr] = []\n",
    "    for r in rep_cols:\n",
    "        # 尾部倒数第 L（长度不足 L → null）\n",
    "        for L in need_L:\n",
    "            agg_exprs.append(\n",
    "                pl.when(pl.len() >= L)\n",
    "                .then(pl.col(r).sort_by(pl.col(g_time)).tail(L).first())\n",
    "                .otherwise(None)\n",
    "                .alias(f\"{r}_prev_tail_lag{L}\")\n",
    "            )\n",
    "        # 当日统计（显式补上 prevday_close）\n",
    "        agg_exprs += [\n",
    "            pl.col(r).sort_by(pl.col(g_time)).last().alias(f\"{r}_prevday_close\"),\n",
    "            pl.col(r).mean().alias(f\"{r}_prevday_mean\"),\n",
    "            pl.col(r).std(ddof=1).alias(f\"{r}_prevday_std\"),\n",
    "        ]\n",
    "\n",
    "    daily = (\n",
    "        lf.group_by([g_symbol, g_date])\n",
    "        .agg(agg_exprs)\n",
    "        .sort([g_symbol, g_date])                # 供下面 shift/ffill 正确运行\n",
    "    )\n",
    "\n",
    "    # 派生（当日）dK：last - (K+1 from end)\n",
    "    daily = daily.with_columns([\n",
    "        (pl.col(f\"{r}_prev_tail_lag1\") - pl.col(f\"{r}_prev_tail_lag{K+1}\")).alias(f\"{r}_prev_tail_d{K}\")\n",
    "        for r in rep_cols for K in tail_diffs\n",
    "        if f\"{r}_prev_tail_lag{K+1}\" in daily.collect_schema().names()\n",
    "    ])\n",
    "\n",
    "    # prev2day/overnight/rolling（仍是“当日相对”的量）\n",
    "    daily = daily.with_columns([\n",
    "        pl.col(f\"{r}_prevday_close\").shift(1).over(g_symbol).alias(f\"{r}_prev2day_close\")\n",
    "        for r in rep_cols\n",
    "    ]).with_columns(\n",
    "        [\n",
    "            (pl.col(f\"{r}_prevday_close\") - pl.col(f\"{r}_prevday_mean\")).alias(f\"{r}_prevday_close_minus_mean\")\n",
    "            for r in rep_cols\n",
    "        ] + [\n",
    "            (pl.col(f\"{r}_prevday_close\") - pl.col(f\"{r}_prev2day_close\")).alias(f\"{r}_overnight_gap\")\n",
    "            for r in rep_cols\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if rolling_windows:\n",
    "        wins = sorted({int(w) for w in rolling_windows if int(w) > 1})\n",
    "        roll_exprs: list[pl.Expr] = []\n",
    "        for r in rep_cols:\n",
    "            base = pl.col(f\"{r}_prevday_close\")\n",
    "            for w in wins:\n",
    "                roll_exprs += [\n",
    "                    base.rolling_mean(window_size=w, min_samples=1).over(g_symbol)\n",
    "                        .alias(f\"{r}_close_roll{w}_mean\"),\n",
    "                    base.rolling_std(window_size=w, ddof=1, min_samples=2).over(g_symbol)\n",
    "                        .alias(f\"{r}_close_roll{w}_std\"),\n",
    "                ]\n",
    "        daily = daily.with_columns(roll_exprs)\n",
    "\n",
    "    # === 核心：将上面的“当日统计/尾部衍生列”转换为“对 d 生效的历史 TTL 值” ===\n",
    "    prev_cols = [c for c in daily.collect_schema().names() if c not in (g_symbol, g_date)]\n",
    "    exprs: list[pl.Expr] = []\n",
    "    for c in prev_cols:\n",
    "        # 最近一次（发生在当前日之前）的非空日期与值\n",
    "        last_non_null_day = (\n",
    "            pl.when(pl.col(c).is_not_null()).then(pl.col(g_date)).otherwise(None)\n",
    "            .forward_fill().over(g_symbol)\n",
    "            .shift(1)\n",
    "        )\n",
    "        last_non_null_val = pl.col(c).forward_fill().over(g_symbol).shift(1)\n",
    "\n",
    "        if prev_soft_days is None:\n",
    "            resolved = last_non_null_val  # 无限 TTL：总取最近一次历史非空\n",
    "        else:\n",
    "            gap_days = (pl.col(g_date) - last_non_null_day).cast(pl.Int32)\n",
    "            resolved = pl.when(gap_days.is_not_null() & (gap_days <= int(prev_soft_days))) \\\n",
    "                        .then(last_non_null_val) \\\n",
    "                        .otherwise(None)\n",
    "\n",
    "        if cast_f32:\n",
    "            resolved = resolved.cast(pl.Float32)\n",
    "        exprs.append(resolved.alias(c))    # 列名不变，语义已是“对 d 生效的历史值”\n",
    "\n",
    "    daily_prev = daily.with_columns(exprs)\n",
    "\n",
    "    # 回拼到 tick 级（左连），并固定顺序（可选）\n",
    "    out = lf.join(daily_prev, on=[g_symbol, g_date], how=\"left\")\n",
    "    out = out.sort([g_symbol, g_date, g_time])\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# B：同 time_id 跨日的 prev{k} + 统计\n",
    "def fe_resp_same_tick_xday(\n",
    "    lf: pl.LazyFrame,\n",
    "    *,\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    rep_cols: Sequence[str],\n",
    "    is_sorted: bool = False,\n",
    "    prev_soft_days: Optional[int] = None,   # None=严格d-k；整数=TTL\n",
    "    cast_f32: bool = True,\n",
    "    ndays: int = 5,\n",
    "    stats_rep_cols: Optional[Sequence[str]] = None,\n",
    "    add_prev1_multirep: bool = True,\n",
    "    batch_size: int = 5,\n",
    ") -> pl.LazyFrame:\n",
    "    \n",
    "    g_symbol, g_date, g_time = keys\n",
    "\n",
    "    # 保证 (symbol,time) 组内按 date 递增（shift(k).over([symbol,time]) 的因果顺序）\n",
    "    if not is_sorted:\n",
    "        lf = lf.sort([g_symbol, g_time, g_date]) # 注意不是date, time\n",
    "\n",
    "    if stats_rep_cols is None:\n",
    "        stats_rep_cols = list(rep_cols)\n",
    "\n",
    "    def _chunks(lst, k):\n",
    "        for i in range(0, len(lst), k):\n",
    "            yield lst[i:i+k]\n",
    "\n",
    "    lf_cur = lf\n",
    "\n",
    "    # 1) prev{k} with strict / TTL\n",
    "    for batch in _chunks(list(rep_cols), batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            for k in range(1, ndays + 1):\n",
    "                val_k  = pl.col(r).shift(k).over([g_symbol, g_time])\n",
    "                day_k  = pl.col(g_date).shift(k).over([g_symbol, g_time])\n",
    "                gap_k  = (pl.col(g_date) - day_k).cast(pl.Int32)\n",
    "\n",
    "                if prev_soft_days is None:\n",
    "                    # 严格 d-k：gap==k\n",
    "                    keep = gap_k.is_not_null() & (gap_k == k)\n",
    "                else:\n",
    "                    # TTL：只要在当前日之前，且 gap<=K\n",
    "                    keep = gap_k.is_not_null() & (gap_k > 0) & (gap_k <= int(prev_soft_days))\n",
    "\n",
    "                val_k = pl.when(keep).then(val_k).otherwise(None)\n",
    "                if cast_f32:\n",
    "                    val_k = val_k.cast(pl.Float32)\n",
    "                exprs.append(val_k.alias(f\"{r}_same_t_prev{k}\"))\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 2) mean/std（忽略 null）\n",
    "    for batch in _chunks([r for r in stats_rep_cols if r in rep_cols], batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            cols = [f\"{r}_same_t_prev{k}\" for k in range(1, ndays + 1)]\n",
    "            vals = pl.concat_list([pl.col(c) for c in cols]).list.drop_nulls()\n",
    "            m = vals.list.mean()\n",
    "            s = vals.list.std(ddof=1)   # 和全局统计一致\n",
    "            if cast_f32:\n",
    "                m = m.cast(pl.Float32); s = s.cast(pl.Float32)\n",
    "            exprs += [\n",
    "                m.alias(f\"{r}_same_t_last{ndays}_mean\"),\n",
    "                s.alias(f\"{r}_same_t_last{ndays}_std\"),\n",
    "            ]\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 3) slope：时间方向设为“最近为正、久远为负”（正=近期上升）\n",
    "    x = np.arange(ndays, 0, -1, dtype=np.float64)\n",
    "    x = (x - x.mean()) / (x.std() + 1e-9)\n",
    "    x_lits = [pl.lit(float(v)) for v in x]\n",
    "\n",
    "    for batch in _chunks([r for r in stats_rep_cols if r in rep_cols], batch_size):\n",
    "        exprs = []\n",
    "        for r in batch:\n",
    "            cols = [f\"{r}_same_t_prev{k}\" for k in range(1, ndays + 1)]\n",
    "            mean_ref = pl.col(f\"{r}_same_t_last{ndays}_mean\")\n",
    "            std_ref  = pl.col(f\"{r}_same_t_last{ndays}_std\")\n",
    "            terms = [((pl.col(c) - mean_ref) / (std_ref + 1e-9)) * x_lits[i]\n",
    "                    for i, c in enumerate(cols)]\n",
    "            # ——更稳：对 null 显式置 0，避免某些版本 sum_horizontal 因 null 变 null\n",
    "            terms = [pl.when(pl.col(c).is_not_null() & mean_ref.is_not_null() & std_ref.is_not_null())\n",
    "                    .then(t).otherwise(pl.lit(0.0)) for t, c in zip(terms, cols)]\n",
    "\n",
    "            n_eff = pl.sum_horizontal([pl.col(c).is_not_null().cast(pl.Int32) for c in cols]).cast(pl.Float32)\n",
    "            den   = pl.when(n_eff > 0).then(n_eff).otherwise(pl.lit(1.0))\n",
    "            slope = pl.sum_horizontal(terms) / den\n",
    "            if cast_f32:\n",
    "                slope = slope.cast(pl.Float32)\n",
    "            exprs.append(slope.alias(f\"{r}_same_t_last{ndays}_slope\"))\n",
    "        lf_cur = lf_cur.with_columns(exprs)\n",
    "\n",
    "    # 4) 跨 responder 的 prev1 行内统计（可选）\n",
    "    if add_prev1_multirep and len(rep_cols) > 0:\n",
    "        n_rep = len(rep_cols)  \n",
    "        prev1_cols = [f\"{r}_same_t_prev1\" for r in rep_cols]\n",
    "        prev1_list = pl.concat_list([pl.col(c) for c in prev1_cols]).list.drop_nulls()\n",
    "        m1 = prev1_list.list.mean()\n",
    "        s1 = prev1_list.list.std(ddof=1)\n",
    "        if cast_f32:\n",
    "            m1 = m1.cast(pl.Float32); s1 = s1.cast(pl.Float32)\n",
    "        lf_cur = lf_cur.with_columns([\n",
    "            m1.alias(f\"prev1_same_t_mean_{n_rep}rep\"),\n",
    "            s1.alias(f\"prev1_same_t_std_{n_rep}rep\"),\n",
    "        ])\n",
    "\n",
    "    # 出口保持有序，便于后续 C 阶段 shift/rolling\n",
    "    lf_cur = lf_cur.sort([g_symbol, g_date, g_time])\n",
    "    return lf_cur\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# C 系列：\n",
    "\n",
    "def fe_feat_history(\n",
    "    *,\n",
    "    lf: pl.LazyFrame,\n",
    "    keys: Tuple[str,str,str] = (\"symbol_id\",\"date_id\",\"time_id\"),\n",
    "    feature_cols: Sequence[str],\n",
    "    is_sorted: bool = False,\n",
    "    prev_soft_days: Optional[int] = None,\n",
    "    cast_f32: bool = True,\n",
    "    batch_size: int = 10,\n",
    "    lags: Iterable[int] = (1, 3),\n",
    "    ret_periods: Iterable[int] = (1,),\n",
    "    diff_periods: Iterable[int] = (1,),\n",
    "    rz_windows: Iterable[int] = (5,),\n",
    "    ewm_spans: Iterable[int] = (10,),\n",
    "    keep_rmean_rstd: bool = True,\n",
    "    cs_cols: Optional[Sequence[str]] = None,\n",
    ") -> pl.LazyFrame:\n",
    "    \n",
    "    g_sym, g_date, g_time = keys\n",
    "    \n",
    "    by_grp = [g_sym]\n",
    "    by_cs  = [g_date, g_time]\n",
    "\n",
    "    need_cols = [*keys, *feature_cols]\n",
    "    schema = lf.collect_schema().names()\n",
    "    miss = [c for c in need_cols if c not in schema]\n",
    "    if miss:\n",
    "        raise KeyError(f\"Columns not found: {miss}\")\n",
    "\n",
    "    lf_out = lf.select(need_cols)\n",
    "    if not is_sorted:\n",
    "        lf_out = lf_out.sort(list(keys))\n",
    "\n",
    "    def _chunks(lst, k):\n",
    "        for i in range(0, len(lst), k):\n",
    "            yield lst[i:i+k]\n",
    "\n",
    "    # ---- 规范化参数：None/[] -> 空元组；并去重/转 int/保正数 ----\n",
    "    def _clean_pos_sorted_unique(x):\n",
    "        if not x:\n",
    "            return tuple()\n",
    "        return tuple(sorted({int(v) for v in x if int(v) >= 1}))\n",
    "\n",
    "    LAGS   = _clean_pos_sorted_unique(lags)\n",
    "    K_RET  = _clean_pos_sorted_unique(ret_periods)\n",
    "    K_DIFF = _clean_pos_sorted_unique(diff_periods)\n",
    "    RZW    = _clean_pos_sorted_unique(rz_windows)\n",
    "    SPANS  = _clean_pos_sorted_unique(ewm_spans)\n",
    "\n",
    "    # C1 lags\n",
    "    if LAGS:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for L in LAGS:\n",
    "                last_date_L = pl.col(g_date).shift(L).over(by_grp)\n",
    "                gap_L = (pl.col(g_date) - last_date_L).cast(pl.Int32)\n",
    "                if prev_soft_days is not None:\n",
    "                    keep_L = gap_L.is_not_null() & (gap_L > 0) & (gap_L <= pl.lit(int(prev_soft_days)))\n",
    "                for c in batch:\n",
    "                    e = pl.col(c).shift(L).over(by_grp)\n",
    "                    if prev_soft_days is not None:\n",
    "                        e = pl.when(keep_L).then(e).otherwise(None)\n",
    "                    if cast_f32:\n",
    "                        e = e.cast(pl.Float32)\n",
    "                    exprs.append(e.alias(f\"{c}__lag{L}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "    # C2 returns（可选）\n",
    "    if K_RET:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for c in batch:\n",
    "                cur = pl.col(c)\n",
    "                for k in K_RET:\n",
    "                    if k in LAGS:\n",
    "                        prev = pl.col(f\"{c}__lag{k}\")  # 已含 TTL\n",
    "                    else:\n",
    "                        prev = pl.col(c).shift(k).over(by_grp)\n",
    "                        if prev_soft_days is not None:\n",
    "                            last_date_k = pl.col(g_date).shift(k).over(by_grp)\n",
    "                            gap_k = (pl.col(g_date) - last_date_k).cast(pl.Int32)\n",
    "                            keep_k = gap_k.is_not_null() & (gap_k > 0) & (gap_k <= pl.lit(int(prev_soft_days)))\n",
    "                            prev = pl.when(keep_k).then(prev).otherwise(None)\n",
    "                    ret = pl.when(prev.is_not_null() & (prev.abs() > 1e-12)).then(cur / prev - 1.0).otherwise(None)\n",
    "                    if cast_f32:\n",
    "                        ret = ret.cast(pl.Float32)\n",
    "                    exprs.append(ret.alias(f\"{c}__ret{k}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "\n",
    "    # C3 diffs（可选）\n",
    "    if K_DIFF:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs = []\n",
    "            for c in batch:\n",
    "                cur = pl.col(c)\n",
    "                for k in K_DIFF:\n",
    "                    if k in LAGS:\n",
    "                        prevk = pl.col(f\"{c}__lag{k}\")  # 已含 TTL\n",
    "                    else:\n",
    "                        prevk = pl.col(c).shift(k).over(by_grp)\n",
    "                        if prev_soft_days is not None:\n",
    "                            last_date_k = pl.col(g_date).shift(k).over(by_grp)\n",
    "                            gap_k = (pl.col(g_date) - last_date_k).cast(pl.Int32)\n",
    "                            keep_k = gap_k.is_not_null() & (gap_k > 0) & (gap_k <= pl.lit(int(prev_soft_days)))\n",
    "                            prevk = pl.when(keep_k).then(prevk).otherwise(None)\n",
    "                    d = pl.when(prevk.is_not_null()).then(cur - prevk).otherwise(None)\n",
    "                    if cast_f32:\n",
    "                        d = d.cast(pl.Float32)\n",
    "                    exprs.append(d.alias(f\"{c}__diff{k}\"))\n",
    "            lf_out = lf_out.with_columns(exprs)\n",
    "\n",
    "\n",
    "\n",
    "    # C4 rolling r-z\n",
    "    if RZW:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs_base = []\n",
    "            # 统一构造 t-1 的基准值（含 TTL 掩码）\n",
    "            if prev_soft_days is not None:\n",
    "                last_date_1 = pl.col(g_date).shift(1).over(by_grp)\n",
    "                gap_1 = (pl.col(g_date) - last_date_1).cast(pl.Int32)\n",
    "                keep_1 = gap_1.is_not_null() & (gap_1 > 0) & (gap_1 <= pl.lit(int(prev_soft_days)))\n",
    "\n",
    "            for c in batch:\n",
    "                # 若之前已在 C1 产出 __lag1，可直接用： base = pl.col(f\"{c}__lag1\")\n",
    "                base = pl.col(c).shift(1).over(by_grp)\n",
    "                if prev_soft_days is not None:\n",
    "                    base = pl.when(keep_1).then(base).otherwise(None)\n",
    "                exprs_base.append(base.alias(f\"{c}__tminus1_base\"))\n",
    "            lf_out = lf_out.with_columns(exprs_base)\n",
    "\n",
    "            # 真正的 rolling r-z\n",
    "            roll_exprs = []\n",
    "            for c in batch:\n",
    "                base = pl.col(f\"{c}__tminus1_base\")\n",
    "                for w in RZW:\n",
    "                    m  = base.rolling_mean(window_size=w, min_samples=1).over(by_grp)\n",
    "                    s  = base.rolling_std(window_size=w, ddof=1, min_samples=2).over(by_grp)  # 统一 ddof=1\n",
    "                    den = (s.fill_null(0.0) + 1e-9)\n",
    "                    rz = (base - m) / den\n",
    "                    if cast_f32:\n",
    "                        m = m.cast(pl.Float32); s = s.cast(pl.Float32); rz = rz.cast(pl.Float32)\n",
    "                    if keep_rmean_rstd:\n",
    "                        roll_exprs += [\n",
    "                            m.alias(f\"{c}__rmean{w}\"),\n",
    "                            s.alias(f\"{c}__rstd{w}\"),\n",
    "                            rz.alias(f\"{c}__rz{w}\"),\n",
    "                        ]\n",
    "                    else:\n",
    "                        roll_exprs.append(rz.alias(f\"{c}__rz{w}\"))\n",
    "            lf_out = lf_out.with_columns(roll_exprs)\n",
    "            lf_out = lf_out.drop([f\"{c}__tminus1_base\" for c in batch])\n",
    "\n",
    "\n",
    "    # C5 EWM（可选）\n",
    "    if SPANS:\n",
    "        for batch in _chunks(feature_cols, batch_size):\n",
    "            exprs_base = []\n",
    "\n",
    "            # TTL 掩码（t-1）\n",
    "            if prev_soft_days is not None:\n",
    "                last_date_1 = pl.col(g_date).shift(1).over(by_grp)\n",
    "                gap_1 = (pl.col(g_date) - last_date_1).cast(pl.Int32)\n",
    "                keep_1 = gap_1.is_not_null() & (gap_1 > 0) & (gap_1 <= pl.lit(int(prev_soft_days)))\n",
    "\n",
    "            # 构造 t-1 基准（若你已在 C1 产出 __lag1，可以直接用它替代下面两行）\n",
    "            for c in batch:\n",
    "                base = pl.col(c).shift(1).over(by_grp)\n",
    "                if prev_soft_days is not None:\n",
    "                    base = pl.when(keep_1).then(base).otherwise(None)\n",
    "                exprs_base.append(base.alias(f\"{c}__tminus1_base\"))\n",
    "            lf_out = lf_out.with_columns(exprs_base)\n",
    "\n",
    "            # 计算 EWM\n",
    "            ewm_exprs = []\n",
    "            for c in batch:\n",
    "                base = pl.col(f\"{c}__tminus1_base\")\n",
    "                for s in SPANS:\n",
    "                    ema = base.ewm_mean(span=int(s), adjust=False, ignore_nulls=True).over(by_grp)\n",
    "                    if cast_f32:\n",
    "                        ema = ema.cast(pl.Float32)\n",
    "                    ewm_exprs.append(ema.alias(f\"{c}__ewm{s}\"))\n",
    "            lf_out = lf_out.with_columns(ewm_exprs)\n",
    "\n",
    "            # 清理临时列\n",
    "            lf_out = lf_out.drop([f\"{c}__tminus1_base\" for c in batch])\n",
    "\n",
    "\n",
    "    # C6 cross-section rank（可选）\n",
    "    if cs_cols:\n",
    "        cs_cols = [c for c in cs_cols if c in feature_cols]\n",
    "        if cs_cols:\n",
    "\n",
    "            # TTL 掩码（t-1）\n",
    "            if prev_soft_days is not None:\n",
    "                last_date_1 = pl.col(g_date).shift(1).over(by_grp)\n",
    "                gap_1 = (pl.col(g_date) - last_date_1).cast(pl.Int32)\n",
    "                keep_1 = gap_1.is_not_null() & (gap_1 > 0) & (gap_1 <= pl.lit(int(prev_soft_days)))\n",
    "\n",
    "            # 先构造每列的 t-1 基准（含 TTL）\n",
    "            exprs_base = []\n",
    "            for c in cs_cols:\n",
    "                base = pl.col(c).shift(1).over(by_grp)\n",
    "                if prev_soft_days is not None:\n",
    "                    base = pl.when(keep_1).then(base).otherwise(None)\n",
    "                exprs_base.append(base.alias(f\"{c}__tminus1_base\"))\n",
    "            lf_out = lf_out.with_columns(exprs_base)\n",
    "\n",
    "            # 基于 t-1：截面 z 与 rank(0..1)\n",
    "            cs_exprs = []\n",
    "            for c in cs_cols:\n",
    "                base = pl.col(f\"{c}__tminus1_base\")\n",
    "\n",
    "                # 截面统计（只用该列的 t-1）\n",
    "                n_valid = base.is_not_null().cast(pl.Int32).sum().over(by_cs)\n",
    "                mu      = base.mean().over(by_cs)\n",
    "                sig     = base.std(ddof=1).over(by_cs)\n",
    "\n",
    "                # z-score（数值更稳：sig.fill_null(0)+eps）\n",
    "                z = ((base - mu) / (sig.fill_null(0.0) + 1e-9)) \\\n",
    "                        .cast(pl.Float32 if cast_f32 else pl.Float64)\n",
    "\n",
    "                # 百分位排名：空→None；n=1→0.5\n",
    "                rank_raw = base.rank(method=\"average\").over(by_cs)\n",
    "                csrank = pl.when(base.is_null()).then(None).otherwise(\n",
    "                    pl.when(n_valid > 1)\n",
    "                    .then((rank_raw - 0.5) / n_valid.cast(pl.Float32))\n",
    "                    .otherwise(pl.lit(0.5))\n",
    "                ).cast(pl.Float32 if cast_f32 else pl.Float64)\n",
    "\n",
    "                cs_exprs += [z.alias(f\"{c}__cs_z\"), csrank.alias(f\"{c}__csrank\")]\n",
    "\n",
    "            lf_out = lf_out.with_columns(cs_exprs)\n",
    "\n",
    "            # 清理临时列\n",
    "            lf_out = lf_out.drop([f\"{c}__tminus1_base\" for c in cs_cols])\n",
    "    return lf_out\n",
    "\n",
    "   \n",
    "@dataclass\n",
    "class StageA:\n",
    "    tail_lags: Sequence[int]\n",
    "    tail_diffs: Sequence[int]\n",
    "    rolling_windows: Optional[Sequence[int]]\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    is_sorted: bool = False\n",
    "    cast_f32: bool = True\n",
    "\n",
    "@dataclass\n",
    "class StageB:\n",
    "    ndays: int\n",
    "    stats_rep_cols: Optional[Sequence[str]] = None\n",
    "    add_prev1_multirep: bool = True\n",
    "    batch_size: int = 5\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    is_sorted: bool = False\n",
    "    cast_f32: bool = True\n",
    "\n",
    "# C 的每个操作可选；None / [] 表示跳过该操作\n",
    "@dataclass\n",
    "class StageC:\n",
    "    lags: Optional[Iterable[int]] = None\n",
    "    ret_periods: Optional[Iterable[int]] = None\n",
    "    diff_periods: Optional[Iterable[int]] = None\n",
    "    rz_windows: Optional[Iterable[int]] = None\n",
    "    ewm_spans: Optional[Iterable[int]] = None\n",
    "    cs_cols: Optional[Sequence[str]] = None\n",
    "    keep_rmean_rstd: bool = True\n",
    "    prev_soft_days: Optional[int] = None\n",
    "    batch_size: Optional[int] = 10\n",
    "    is_sorted: bool = False\n",
    "    cast_f32: bool = True\n",
    "    \n",
    "\n",
    "def assert_time_monotone(path, *, date_col=\"date_id\", time_col=\"time_id\"):\n",
    "    s = (pl.scan_parquet(path, storage_options=storage_options)\n",
    "           .select([\n",
    "               (pl.col(date_col).diff().fill_null(0) < 0).any().alias('date_drop'),\n",
    "               ((pl.col(date_col).diff().fill_null(0) == 0) &\n",
    "                (pl.col(time_col).diff().fill_null(0) < 0)).any().alias('time_drop')\n",
    "           ])\n",
    "           .collect(streaming=True))\n",
    "    assert not s['date_drop'][0] and not s['time_drop'][0]\n",
    "\n",
    "\n",
    "def run_staged_engineering(\n",
    "    lf_base: pl.LazyFrame,\n",
    "    *,\n",
    "    keys: Sequence[str],\n",
    "    rep_cols: Sequence[str],\n",
    "    feature_cols: Sequence[str],\n",
    "    out_dir: str,\n",
    "    A: StageA | None = None,\n",
    "    B: StageB | None = None,\n",
    "    C: StageC | None = None,\n",
    "    write_date_between: tuple[int, int] | None = None,   # 新增：只写核心区间\n",
    "):\n",
    "    g_symbol, g_date, g_time = keys\n",
    "\n",
    "    def _save(lf_out: pl.LazyFrame, path: str):\n",
    "        if write_date_between is None:\n",
    "            raise ValueError(\"write_date_between must be specified to avoid date overlap\")\n",
    "        lo, hi = write_date_between\n",
    "        \n",
    "        sk = [g_date, g_time, g_symbol]\n",
    "        \n",
    "        df = lf_out.filter(pl.col(g_date).is_between(lo, hi)).sort(sk).collect()\n",
    "        with fs.open(path, \"wb\") as f:   # 复用你上面构好的 fs (fsspec)\n",
    "            df.write_parquet(f, compression=\"zstd\")\n",
    "        if cfg.get(\"debug\", {}).get(\"check_time_monotone\", True):\n",
    "            assert_time_monotone(path, date_col=g_date, time_col=g_time)\n",
    "\n",
    "\n",
    "        \n",
    "    # ---------- A ----------\n",
    "    if A is not None:\n",
    "        lf_resp = lf_base.select([*keys, *rep_cols])\n",
    "        lf_a_full = fe_resp_daily(\n",
    "            lf_resp,\n",
    "            keys=tuple(keys),\n",
    "            rep_cols=rep_cols,\n",
    "            is_sorted=A.is_sorted,\n",
    "            prev_soft_days=A.prev_soft_days,\n",
    "            cast_f32=A.cast_f32,\n",
    "            tail_lags=A.tail_lags,\n",
    "            tail_diffs=A.tail_diffs,\n",
    "            rolling_windows=A.rolling_windows,\n",
    "        )\n",
    "        drop = set(keys) | set(rep_cols)\n",
    "        a_cols = [c for c in lf_a_full.collect_schema().names() if c not in drop]\n",
    "        _save(lf_a_full.select([*keys, *a_cols]), f\"{out_dir}/stage_a.parquet\")\n",
    "        \n",
    "\n",
    "    # ---------- B ----------\n",
    "    if B is not None:\n",
    "        lf_resp = lf_base.select([*keys, *rep_cols])\n",
    "        lf_b_full = fe_resp_same_tick_xday(\n",
    "            lf_resp,\n",
    "            keys=tuple(keys),\n",
    "            rep_cols=rep_cols,\n",
    "            is_sorted=B.is_sorted,\n",
    "            prev_soft_days=B.prev_soft_days,\n",
    "            cast_f32=B.cast_f32,\n",
    "            ndays=B.ndays,\n",
    "            stats_rep_cols=B.stats_rep_cols,\n",
    "            add_prev1_multirep=B.add_prev1_multirep,\n",
    "            batch_size=B.batch_size,\n",
    "        )\n",
    "        drop = set(keys) | set(rep_cols)\n",
    "        b_cols = [c for c in lf_b_full.collect_schema().names() if c not in drop]\n",
    "        _save(lf_b_full.select([*keys, *b_cols]), f\"{out_dir}/stage_b.parquet\")\n",
    "\n",
    "    # ---------- C（按操作分别输出） ----------\n",
    "    if C is not None:\n",
    "        def _do_op(op_name: str, **op_flags):\n",
    "            lf_src = lf_base.select([*keys, *feature_cols])\n",
    "            lf_c = fe_feat_history(\n",
    "                lf=lf_src,\n",
    "                keys=tuple(keys),\n",
    "                feature_cols=feature_cols,\n",
    "                is_sorted=C.is_sorted,\n",
    "                prev_soft_days=C.prev_soft_days,\n",
    "                cast_f32=C.cast_f32,\n",
    "                batch_size=C.batch_size,\n",
    "                lags=op_flags.get(\"lags\"),\n",
    "                ret_periods=op_flags.get(\"ret_periods\"),\n",
    "                diff_periods=op_flags.get(\"diff_periods\"),\n",
    "                rz_windows=op_flags.get(\"rz_windows\"),\n",
    "                ewm_spans=op_flags.get(\"ewm_spans\"),\n",
    "                keep_rmean_rstd=C.keep_rmean_rstd,\n",
    "                cs_cols=op_flags.get(\"cs_cols\"),\n",
    "            ).drop(feature_cols)\n",
    "            cols = [c for c in lf_c.collect_schema().names() if c not in keys]\n",
    "            _save(lf_c.select([*keys, *cols]), f\"{out_dir}/stage_c_{op_name}.parquet\")\n",
    "\n",
    "        if C.lags:         _do_op(\"lags\",   lags=C.lags)\n",
    "        if C.ret_periods:  _do_op(\"ret\",    ret_periods=C.ret_periods)\n",
    "        if C.diff_periods: _do_op(\"diff\",   diff_periods=C.diff_periods)\n",
    "        if C.rz_windows:   _do_op(\"rz\",     rz_windows=C.rz_windows)\n",
    "        if C.ewm_spans:    _do_op(\"ewm\",    ewm_spans=C.ewm_spans)\n",
    "        if C.cs_cols:      _do_op(\"csrank\", cs_cols=C.cs_cols)\n",
    "        \n",
    "\n",
    "def weighted_r2_zero_mean(y_true, y_pred, weight) -> float:\n",
    "    \"\"\"\n",
    "    Sample-weighted zero-mean R^2 used in Jane Street:\n",
    "        R^2 = 1 - sum_i w_i (y_i - yhat_i)^2 / sum_i w_i y_i^2\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "    weight = np.asarray(weight, dtype=np.float64).ravel()\n",
    "    assert y_true.shape == y_pred.shape == weight.shape\n",
    "\n",
    "    num = np.sum(weight * (y_true - y_pred) ** 2)\n",
    "    den = np.sum(weight * (y_true ** 2))\n",
    "    if den <= 0:\n",
    "        return 0.0  # safe fallback (shouldn't happen on the full JS eval)\n",
    "    return 1.0 - (num / den)\n",
    "\n",
    "def lgb_wr2_eval(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    w = train_data.get_weight()\n",
    "    if w is None:\n",
    "        w = np.ones_like(y)\n",
    "    score = weighted_r2_zero_mean(y, preds, w)\n",
    "    return ('wr2', score, True)  # higher is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c19477",
   "metadata": {},
   "source": [
    "# 特征选择- 初选 (选特征，省略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b61d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, glob\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = [\"/mnt/data/js/clean/final_clean.parquet\"]\n",
    "KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "TARGET = \"responder_6\"\n",
    "WEIGHT = 'weight'\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)]\n",
    "\n",
    "OUT_DIR = \"/mnt/data/js/cache/first_selection\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- A: prev-day tails + daily summaries ---\n",
    "A = StageA(\n",
    "    tail_lags=(1,),\n",
    "    tail_diffs=(1,),\n",
    "    rolling_windows=(5,),\n",
    "    prev_soft_days=3,          # allow fallback up to 3 calendar days\n",
    ")\n",
    "\n",
    "# --- B: same time_id cross-day ---\n",
    "B = StageB(\n",
    "    ndays=3,                   # prev{1..3} at same time_id\n",
    "    stats_rep_cols=None,       # default: use rep_cols\n",
    "    add_prev1_multirep=True,\n",
    "    batch_size=5,\n",
    "    prev_soft_days=3,          # TTL for gaps\n",
    "    strict_k=False,            # allow ≤K-day gaps instead of strict d-k\n",
    ")\n",
    "\n",
    "# --- C: history features (keep it tiny) ---\n",
    "C = StageC(\n",
    "    lags=(1, ),\n",
    "    ret_periods=(1,),\n",
    "    diff_periods=(1,),\n",
    "    rz_windows=(5,),\n",
    "    ewm_spans=(10,),\n",
    "    keep_rmean_rstd=True,\n",
    "    cs_cols=None,        # must be subset of feature_cols\n",
    "    cs_by=(\"date_id\",\"time_id\"),\n",
    "    prev_soft_days=3,\n",
    ")\n",
    "\n",
    "# example call\n",
    "paths = run_staged_engineering(\n",
    "    lf_base=lf_base,                # your base LazyFrame\n",
    "    keys=KEYS,\n",
    "    rep_cols=REP_COLS,         # updated to use REP_COLS\n",
    "    feature_cols=FEATURE_COLS, # updated to use FEATURE_COLS\n",
    "    out_dir=OUT_DIR,\n",
    "    A=A, B=B, C=C,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e1720",
   "metadata": {},
   "source": [
    "0. 准备与切分天数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4df6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STAGE_PATHS = [\n",
    "    \"/mnt/data/js/cache/first_selection/stage_a.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_b.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_lags.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_ret.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_diff.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_rz.parquet\",\n",
    "    \"/mnt/data/js/cache/first_selection/stage_c_ewm.parquet\",\n",
    "]\n",
    "\n",
    "DATE_LO, DATE_HI = 1200, 1400\n",
    "OUT_DIR = \"/mnt/data/js/cache/first_selection/run_full\"\n",
    "SHARD_DIR = f\"{OUT_DIR}/shards_all\"\n",
    "Path(SHARD_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lf_base = pl.scan_parquet(BASE_PATH)\n",
    "# 仅拿目标区间的base\n",
    "lf_range = lf_base.filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "\n",
    "# days & split\n",
    "days = (lf_range.select(pl.col(\"date_id\").unique().sort())\n",
    "                .collect(streaming=True)[\"date_id\"].to_list())\n",
    "cut = int(len(days) * 0.8)\n",
    "train_days, val_days = days[:cut], days[cut:]\n",
    "print(f\"[split] train {len(train_days)} days, val {len(val_days)} days, range={days[0]}..{days[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eddc42",
   "metadata": {},
   "source": [
    "1.收集全量特征列名（并集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 来自 base 的特征列\n",
    "feat_cols = set(FEATURE_COLS)\n",
    "\n",
    "# 各 stage 全部列（除 KEYS/TARGET/WEIGHT）\n",
    "for p in STAGE_PATHS:\n",
    "    if not os.path.exists(p):\n",
    "        print(f\"[skip] missing: {p}\")\n",
    "        continue\n",
    "    names = pl.scan_parquet(p).collect_schema().names()\n",
    "    for c in names:\n",
    "        if c not in KEYS and c not in (TARGET, WEIGHT):\n",
    "            feat_cols.add(c)\n",
    "\n",
    "feat_cols = sorted(feat_cols)\n",
    "print(f\"[cols] total feature columns = {len(feat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278b8fe",
   "metadata": {},
   "source": [
    "2. 写“天片”—把所有列拼上并立刻落盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01289383",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_PER_SHARD = 16\n",
    "\n",
    "# 左表（含 base 的 FEATURE_COLS）\n",
    "lf_left_base = (\n",
    "    lf_range\n",
    "    .select([*KEYS, TARGET, WEIGHT, *[pl.col(c) for c in FEATURE_COLS]])\n",
    ")\n",
    "\n",
    "# 为每个 stage 准备元信息（列名 + 文件大小，先拼小文件更省内存）\n",
    "stage_meta = []\n",
    "for p in STAGE_PATHS:\n",
    "    if not os.path.exists(p): \n",
    "        continue\n",
    "    scan = pl.scan_parquet(p).filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI))\n",
    "    cols = [c for c in scan.collect_schema().names() if c not in KEYS]\n",
    "    if cols:\n",
    "        stage_meta.append({\"path\": p, \"cols\": cols, \"size\": os.path.getsize(p)})\n",
    "stage_meta.sort(key=lambda d: d[\"size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637955db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_shards(tag, days_list):\n",
    "    ds = sorted(days_list)\n",
    "    for i in range(0, len(ds), DAYS_PER_SHARD):\n",
    "        batch = set(ds[i:i+DAYS_PER_SHARD])\n",
    "\n",
    "        # 当前片的左表\n",
    "        lf_chunk = lf_left_base.filter(pl.col(\"date_id\").is_in(batch))\n",
    "        already = set(lf_chunk.collect_schema().names())\n",
    "\n",
    "        # 逐 stage 拼接（右表只取该片天数 + 只取未存在列）\n",
    "        for m in stage_meta:\n",
    "            need = [c for c in m[\"cols\"] if c not in already]\n",
    "            if not need:\n",
    "                continue\n",
    "            lf_add = (pl.scan_parquet(m[\"path\"])\n",
    "                        .filter(pl.col(\"date_id\").is_in(batch))\n",
    "                        .select([*KEYS, *need]))\n",
    "            lf_chunk = lf_chunk.join(lf_add, on=KEYS, how=\"left\")\n",
    "            already.update(need)\n",
    "\n",
    "        # 统一 float32 并落盘（列按 feat_cols 顺序对齐；片内缺失的列自然是 null）\n",
    "        present = [c for c in feat_cols if c in already]\n",
    "        cast_feats = [pl.col(c).cast(pl.Float32).alias(c) for c in present]\n",
    "        lf_out = lf_chunk.select([\n",
    "            *KEYS,\n",
    "            pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "            pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "            *cast_feats,\n",
    "        ])\n",
    "        out_path = f\"{SHARD_DIR}/{tag}_shard_{i//DAYS_PER_SHARD:04d}.parquet\"\n",
    "        lf_out.sink_parquet(out_path, compression=\"zstd\")\n",
    "        print(f\"[{tag}] wrote {out_path}\")\n",
    "        gc.collect()\n",
    "\n",
    "write_shards(\"train\", train_days)\n",
    "write_shards(\"val\",   val_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173b9e",
   "metadata": {},
   "source": [
    "3. 从 shards 构建 memmap 数组 （恒定内存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memmap_from_shards(glob_pat, feat_cols, prefix):\n",
    "    paths = sorted(glob.glob(glob_pat))\n",
    "    counts = [pl.scan_parquet(p).select(pl.len()).collect(streaming=True).item() for p in paths]\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    print(f\"[memmap] {glob_pat}: {len(paths)} files, {n_rows} rows, {n_feat} features\")\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    i = 0\n",
    "    for p, k in zip(paths, counts):\n",
    "        lf = pl.scan_parquet(p)\n",
    "        names = set(lf.collect_schema().names())\n",
    "        exprs = [\n",
    "            (pl.col(c).cast(pl.Float32).alias(c) if c in names\n",
    "             else pl.lit(None, dtype=pl.Float32).alias(c))\n",
    "            for c in feat_cols\n",
    "        ]\n",
    "        df = lf.select([\n",
    "            pl.col(TARGET).cast(pl.Float32).alias(TARGET),\n",
    "            pl.col(WEIGHT).cast(pl.Float32).alias(WEIGHT),\n",
    "            *exprs\n",
    "        ]).collect(streaming=True)\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(TARGET)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(WEIGHT)).to_numpy().ravel()\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush()\n",
    "    return X, y, w\n",
    "\n",
    "train_X, train_y, train_w = memmap_from_shards(f\"{SHARD_DIR}/train_shard_*.parquet\", feat_cols, f\"{OUT_DIR}/train\")\n",
    "val_X,   val_y,   val_w   = memmap_from_shards(f\"{SHARD_DIR}/val_shard_*.parquet\",   feat_cols, f\"{OUT_DIR}/val\")\n",
    "\n",
    "print(\"train shapes:\", train_X.shape, train_y.shape, train_w.shape)\n",
    "print(\"val   shapes:\", val_X.shape,   val_y.shape,   val_w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101d7bd",
   "metadata": {},
   "source": [
    "4. LightGBM 训练 + 重要性 （一次性全列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76652e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(train_X, label=train_y, weight=train_w,\n",
    "                     feature_name=feat_cols, free_raw_data=True)\n",
    "dval   = lgb.Dataset(val_X,   label=val_y,   weight=val_w,\n",
    "                     feature_name=feat_cols, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\", metric=\"None\",\n",
    "    num_threads=16, seed=42, deterministic=True, first_metric_only=True,\n",
    "    learning_rate=0.05, num_leaves=31, max_depth=-1, min_data_in_leaf=20,\n",
    "    # 内存友好\n",
    "    max_bin=63, bin_construct_sample_cnt=100_000, min_data_in_bin=3,\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params, dtrain,\n",
    "    valid_sets=[dval, dtrain], valid_names=[\"val\",\"train\"],\n",
    "    num_boost_round=1000, callbacks=[lgb.early_stopping(50)],\n",
    "    feval=lgb_wr2_eval,   # 你的评估函数\n",
    ")\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": model.feature_name(),\n",
    "    \"gain\": model.feature_importance(\"gain\"),\n",
    "    \"split\": model.feature_importance(\"split\"),\n",
    "}).sort_values(\"gain\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(imp.head(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.to_csv(f\"{OUT_DIR}/imp_1r.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7305fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.read_csv(f\"{OUT_DIR}/imp_1r.csv\")\n",
    "top_feats = imp.loc[imp.gain > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864927ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = top_feats['feature'].str.extract(r'^(feature_\\d{2}|responder_\\d)', expand=False)\n",
    "top_feats['family'] = fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f31e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_feats = top_feats.groupby('family').agg(\n",
    "    n = ('feature', 'count'),\n",
    "    gain = ('gain', 'sum'),\n",
    "    split = ('split', 'sum'),\n",
    ").reset_index().sort_values('gain', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03415ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fam_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_feat = fam_feats['family'].str.startswith('feature_', na=False)\n",
    "mask_resp = fam_feats[\"family\"].str.startswith(\"responder_\", na=False)\n",
    "features_only   = fam_feats[mask_feat].sort_values(\"gain\", ascending=False)\n",
    "responders_only = fam_feats[mask_resp].sort_values(\"gain\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = features_only['family'][:79] # select all\n",
    "selected_resps = responders_only['family'][:9] # select all\n",
    "\n",
    "# save the Series (no index)\n",
    "selected_features.to_csv(f\"{OUT_DIR}/selected_features_1r.csv\", index=False, header=False)\n",
    "selected_resps.to_csv(f\"{OUT_DIR}/selected_responders_1r.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230fcef",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1acc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集参数\n",
    "paths = fs.glob(f\"{P('az', cfg['paths']['clean_shards'])}/*.parquet\")\n",
    "az_paths = []\n",
    "for p in paths:\n",
    "    az_paths.append(f\"az://{p}\")\n",
    "az_paths = sorted(az_paths)  # Use sorted() instead of sort() to create a new sorted list   \n",
    "lc = pl.scan_parquet(az_paths, storage_options=storage_options)\n",
    "\n",
    "days = lc.select(pl.col(\"date_id\").unique().sort()).collect(streaming=True)[\"date_id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- step 2: FE per clean shard (A+B once, C batched internally via C.batch_size) -------\n",
    "fea = cfg.get(\"feature_eng\", {})\n",
    "A_cfg = fea.get(\"A\", {})\n",
    "B_cfg = fea.get(\"B\", {})\n",
    "C_cfg = fea.get(\"C\", {})\n",
    "A_enabled = A_cfg.get(\"enabled\", True)\n",
    "B_enabled = B_cfg.get(\"enabled\", True)\n",
    "C_enabled = C_cfg.get(\"enabled\", True)\n",
    "\n",
    "A = (StageA(\n",
    "        tail_lags=A_cfg.get(\"tail_lags\", [1]),\n",
    "        tail_diffs=A_cfg.get(\"tail_diffs\", [1]),\n",
    "        rolling_windows=A_cfg.get(\"rolling_windows\", [3]),\n",
    "        prev_soft_days=A_cfg.get(\"prev_soft_days\", 7),\n",
    "        is_sorted=A_cfg.get(\"is_sorted\", False),\n",
    "        cast_f32=A_cfg.get(\"cast_f32\", True),\n",
    "    ) if A_enabled else None)\n",
    "\n",
    "B = (StageB(\n",
    "        ndays=B_cfg.get(\"ndays\", 5),\n",
    "        stats_rep_cols=B_cfg.get(\"stats_rep_cols\", None),\n",
    "        add_prev1_multirep=B_cfg.get(\"add_prev1_multirep\", True),\n",
    "        batch_size=B_cfg.get(\"batch_size\", 5),\n",
    "        prev_soft_days=B_cfg.get(\"prev_soft_days\", 7),\n",
    "        is_sorted=B_cfg.get(\"is_sorted\", False),\n",
    "        cast_f32=B_cfg.get(\"cast_f32\", True),\n",
    "    ) if B_enabled else None)\n",
    "\n",
    "C = (StageC(\n",
    "        lags=C_cfg.get(\"lags\", [1,3]),\n",
    "        ret_periods=C_cfg.get(\"ret_periods\", [1]),\n",
    "        diff_periods=C_cfg.get(\"diff_periods\", [1]),\n",
    "        rz_windows=C_cfg.get(\"rz_windows\", [5]),\n",
    "        ewm_spans=C_cfg.get(\"ewm_spans\", [10]),\n",
    "        keep_rmean_rstd=C_cfg.get(\"keep_rmean_rstd\", True),\n",
    "        cs_cols=C_cfg.get(\"cs_cols\", None),\n",
    "        prev_soft_days=C_cfg.get(\"prev_soft_days\", 7),\n",
    "        batch_size=C_cfg.get(\"batch_size\", 10),\n",
    "        is_sorted=C_cfg.get(\"is_sorted\", False),\n",
    "        cast_f32=C_cfg.get(\"cast_f32\", True),\n",
    "    ) if C_enabled else None)\n",
    "\n",
    "\n",
    "\n",
    "# 创建 FE 输出目录\n",
    "fe_dir = P(\"az\", cfg[\"paths\"][\"fe_shards\"])\n",
    "fs.mkdir(fe_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 分片循环：每片包含 [pad_lo .. core_hi] 的输入，但只写 [core_lo .. core_hi] --------\n",
    "PAD_DAYS = 30 # 后期可定义函数取最小有效日期\n",
    "CORE_DAYS = 30\n",
    "\n",
    "\n",
    "g_date= cfg['keys'][1]\n",
    "for start in range(PAD_DAYS, len(days), CORE_DAYS):\n",
    "    core_lo_idx = start\n",
    "    core_hi_idx = min(start + CORE_DAYS - 1, len(days) - 1) # 闭区间\n",
    "    pad_lo_idx = core_lo_idx - PAD_DAYS\n",
    "    \n",
    "    core_lo, core_hi = days[core_lo_idx], days[core_hi_idx]\n",
    "    pad_lo = days[pad_lo_idx]\n",
    "    \n",
    "    # 仅读本片+pad的输入 （懒加载 + 谓词下推）\n",
    "    lf_shard = (lc.filter(pl.col(g_date).is_between(pad_lo, core_hi))\n",
    "                .select([*cfg['keys'], cfg['weight'], 'time_bucket', *RESP_COLS, *FEATURE_ALL]))\n",
    "    out_path = f\"{fe_dir}/fe_{core_lo:04d}_{core_hi:04d}\"\n",
    "    fs.mkdir(out_path, exist_ok=True)\n",
    "    print(f\"[FE] days {core_lo}..{core_hi} (pad from {pad_lo}) -> {out_path}\")\n",
    "    run_staged_engineering(\n",
    "        lf_base = lf_shard,\n",
    "        keys = cfg['keys'],\n",
    "        rep_cols = RESP_COLS,\n",
    "        feature_cols = FEATURE_ALL,\n",
    "        out_dir = out_path,\n",
    "        A = A,\n",
    "        B = B,\n",
    "        C = C,\n",
    "        write_date_between=(core_lo, core_hi)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20563187",
   "metadata": {},
   "source": [
    "## 把同一分片内 A/B/C 拼成 Panel 分片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a2a43",
   "metadata": {},
   "source": [
    "1. 基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS = tuple(cfg[\"keys\"])\n",
    "g_sym, g_date, g_time = KEYS\n",
    "TARGET, WEIGHT = cfg[\"target\"], cfg[\"weight\"]\n",
    "TIME_SORT = cfg['sorts']['time_major']\n",
    "\n",
    "clean_root = P(\"az\", cfg[\"paths\"][\"clean_shards\"])\n",
    "fe_root    = P(\"az\", cfg[\"paths\"][\"fe_shards\"])\n",
    "panel_root = P(\"az\", cfg[\"paths\"][\"panel_shards\"])\n",
    "fs.mkdir(panel_root, exist_ok=True)\n",
    "\n",
    "DATE_LO, DATE_HI = 900, 1000\n",
    "\n",
    "paths = fs.glob(f\"{fe_root }/*\")\n",
    "sorted_paths = [f\"az://{p}\" for p in sorted(paths)]\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1788951",
   "metadata": {},
   "source": [
    "2.枚举窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = set()\n",
    "for p in sorted_paths:\n",
    "    base = p.split(\"/\")[-1]  # e.g. C_lags_1220_1249.parquet\n",
    "    lo = int(base.split(\"_\")[-2]); hi = int(base.split(\"_\")[-1])\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643f6de",
   "metadata": {},
   "source": [
    "3. 按窗口拼接 (A + B + 所有 C_*) → 直接写数据分片（无大表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import fsspec, gc\n",
    "\n",
    "T = np.float32(cfg[\"trading\"][\"ticks\"])\n",
    "TWOPI_over_T = np.float32(2.0*np.pi) / T     # 全是 float32\n",
    "twopi_over_T_lit = pl.lit(TWOPI_over_T, dtype=pl.Float32)\n",
    "cast_keys = [pl.col(k).cast(pl.Int32).alias(k) for k in KEYS]\n",
    "\n",
    "\n",
    "lc = pl.scan_parquet(f\"{clean_root}/*.parquet\", storage_options=storage_options).with_columns(cast_keys)\n",
    "\n",
    "\n",
    "def assert_panel_shard(path, lo, hi, *, date_col=\"date_id\", time_col=\"time_id\"):\n",
    "    s = (pl.scan_parquet(path, storage_options=storage_options)\n",
    "           .select([\n",
    "               pl.col(date_col).min().alias(\"dmin\"),\n",
    "               pl.col(date_col).max().alias(\"dmax\"),\n",
    "               (pl.col(date_col).diff().fill_null(0) < 0).any().alias(\"date_drop\"),\n",
    "               ((pl.col(date_col).diff().fill_null(0) == 0) &\n",
    "                (pl.col(time_col).diff().fill_null(0) < 0)).any().alias(\"time_drop\"),\n",
    "           ])\n",
    "           .collect(streaming=True))\n",
    "    dmin, dmax = int(s[\"dmin\"][0]), int(s[\"dmax\"][0])\n",
    "    assert dmin == lo and dmax == hi, f\"date range mismatch: got [{dmin},{dmax}] expect [{lo},{hi}]\"\n",
    "    assert not s[\"date_drop\"][0] and not s[\"time_drop\"][0], \"time not monotone\"\n",
    "\n",
    "\n",
    "\n",
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "    \n",
    "    shard_name = f\"fe_{lo:04d}_{hi:04d}\"\n",
    "    \n",
    "    ti_f = pl.col(\"time_id\").cast(pl.Float32)\n",
    "    # 基表（先筛行，再一次性加时间特征）\n",
    "    lf = (\n",
    "        lc.filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "          .select([*KEYS, \"time_bucket\", TARGET, WEIGHT, *FEATURE_ALL])\n",
    "        .with_columns([\n",
    "            ti_f.alias(\"time_pos\"),\n",
    "            (ti_f * twopi_over_T_lit).alias(\"_phase_\").cast(pl.Float32),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # 兼容旧版：对表达式调用 .sin() / .cos()\n",
    "            pl.col(\"_phase_\").sin().cast(pl.Float32).alias(\"time_sin\"),\n",
    "            pl.col(\"_phase_\").cos().cast(pl.Float32).alias(\"time_cos\"),\n",
    "        ])\n",
    "        .drop([\"_phase_\"])\n",
    "    )\n",
    "    \n",
    "    fe_dir = f\"{fe_root}/{shard_name}\"\n",
    "    # A/B\n",
    "    A = pl.scan_parquet(f\"{fe_dir}/stage_a.parquet\", storage_options=storage_options).with_columns(cast_keys)\n",
    "    B = pl.scan_parquet(f\"{fe_dir}/stage_b.parquet\", storage_options=storage_options).with_columns(cast_keys)\n",
    "    \n",
    "    # C\n",
    "    C_paths = sorted(fs.glob(f\"{fe_dir}/stage_c_*.parquet\"))\n",
    "    C_paths = [f\"az://{p}\" for p in C_paths]\n",
    "    C_scans = [pl.scan_parquet(p, storage_options=storage_options).with_columns(cast_keys) for p in C_paths]\n",
    "    \n",
    "    # 逐个 join \n",
    "    panel = lf.join(A, on=list(KEYS), how=\"left\", suffix=\"_A\")\n",
    "    panel = panel.join(B, on=list(KEYS), how=\"left\", suffix=\"_B\")\n",
    "    for C in C_scans:\n",
    "        panel = panel.join(C, on=list(KEYS), how=\"left\", suffix=\"_C\")\n",
    "        \n",
    "    panel = panel.sort(TIME_SORT)\n",
    "    \n",
    "    df_out = panel.collect(streaming=True)\n",
    "    out_path = f\"{panel_root}/panel_{w_lo:04d}_{w_hi:04d}.parquet\"\n",
    "    \n",
    "    with fs.open(out_path, \"wb\") as f:\n",
    "        df_out.write_parquet(f, compression=\"zstd\")\n",
    "    print(f\"[panel] wrote {out_path} with {df_out.shape[0]} rows\")\n",
    "    \n",
    "    if cfg.get(\"debug\", {}).get(\"check_time_monotone\", True):\n",
    "        assert_panel_shard(out_path, w_lo, w_hi, date_col=g_date, time_col=g_time)\n",
    "    del df_out\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95fc78",
   "metadata": {},
   "source": [
    "4-5.构建memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路径\n",
    "mm_root = P(\"local\",  cfg[\"paths\"][\"sample_mm\"])\n",
    "os.makedirs(mm_root, exist_ok=True)\n",
    "\n",
    "# 选定区间的panel分片\n",
    "panel_paths = sorted([\n",
    "    f\"az://{p}\" for p in fs.glob(f\"{panel_root}/panel_*.parquet\")\n",
    "    if int(p.split(\"_\")[-2]) <= DATE_HI and int(p.split(\"_\")[-1].split(\".\")[0]) >= DATE_LO\n",
    "])\n",
    "\n",
    "assert panel_paths, \"no panel shards matched DATE_LO/DATE_HI\"\n",
    "\n",
    "# 任选一个训练分片当“列模板”\n",
    "sample_path = panel_paths[0]\n",
    "names = pl.scan_parquet(sample_path, storage_options=storage_options).collect_schema().names()\n",
    "\n",
    "# 直接从这个分片拿特征列（已包含 base + engineered）\n",
    "\n",
    "feat_cols = [c for c in names if c not in (*cfg['keys'], cfg['target'], cfg['weight'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "def shard2memmap(sorted_paths: list[str], feat_cols: list[str], prefix: str):\n",
    "    date_col   = cfg[\"keys\"][1]\n",
    "    target_col = cfg[\"target\"]\n",
    "    weight_col = cfg[\"weight\"]\n",
    "\n",
    "    # 统计每片行数\n",
    "    counts = []\n",
    "    for p in sorted_paths:\n",
    "        k = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "            .select(pl.len())\n",
    "            .collect(streaming=True)\n",
    "            .item())\n",
    "        counts.append(int(k))\n",
    "\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "    os.makedirs(os.path.dirname(prefix), exist_ok=True)\n",
    "\n",
    "    # 创建 memmap\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=np.float32, mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=np.float32, mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=np.float32, mode=\"w+\", shape=(n_rows,))\n",
    "    d = np.memmap(f\"{prefix}_date.int32.mmap\", dtype=np.int32,   mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    need_cols = [date_col, target_col, weight_col, *feat_cols]\n",
    "    ofs = 0\n",
    "    for p, k in zip(sorted_paths, counts):\n",
    "        df = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "                .select(need_cols)\n",
    "                .collect(streaming=True))\n",
    "\n",
    "        # 先 to_numpy，再 astype 保证为 float32（兼容老版本 Polars）\n",
    "        X_block = df.select(feat_cols).to_numpy().astype(np.float32, copy=False)\n",
    "        y_block = df.get_column(target_col).to_numpy().astype(np.float32, copy=False).ravel()\n",
    "        w_block = df.get_column(weight_col).to_numpy().astype(np.float32, copy=False).ravel()\n",
    "        d_block = df.get_column(date_col).to_numpy().astype(np.int32,   copy=False).ravel()\n",
    "\n",
    "        X[ofs:ofs+k, :] = X_block\n",
    "        y[ofs:ofs+k]    = y_block\n",
    "        w[ofs:ofs+k]    = w_block\n",
    "        d[ofs:ofs+k]    = d_block\n",
    "\n",
    "        ofs += k\n",
    "        del df, X_block, y_block, w_block, d_block\n",
    "        gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush(); d.flush()\n",
    "\n",
    "    meta = {\n",
    "        \"n_rows\": int(n_rows),\n",
    "        \"n_feat\": int(n_feat),\n",
    "        \"dtype\": {\"X\":\"float32\",\"y\":\"float32\",\"w\":\"float32\",\"date_id\":\"int32\"},\n",
    "        \"features\": list(feat_cols),\n",
    "        \"target\": target_col,\n",
    "        \"weight\": weight_col,\n",
    "        \"date_col\": date_col,\n",
    "        \"files\": sorted_paths,\n",
    "        \"ts\": time.time(),\n",
    "    }\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return {\n",
    "        \"X\": f\"{prefix}_X.float32.mmap\",\n",
    "        \"y\": f\"{prefix}_y.float32.mmap\",\n",
    "        \"w\": f\"{prefix}_w.float32.mmap\",\n",
    "        \"date\": f\"{prefix}_date.int32.mmap\",\n",
    "        \"meta\": f\"{prefix}.meta.json\",\n",
    "    }\n",
    "\n",
    "# —— 调用 —— #\n",
    "prefix = os.path.join(mm_root, \"full_sample_v1\")\n",
    "mm_paths = shard2memmap(sorted_paths=panel_paths, feat_cols=feat_cols, prefix=prefix)\n",
    "print(mm_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6a9c1",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cd427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 加载 memmap ----------\n",
    "with open(mm_paths[\"meta\"]) as f:\n",
    "    meta = json.load(f)\n",
    "N, F = meta[\"n_rows\"], meta[\"n_feat\"]\n",
    "X = np.memmap(mm_paths[\"X\"], dtype=np.float32, mode=\"r\", shape=(N, F))\n",
    "y = np.memmap(mm_paths[\"y\"], dtype=np.float32, mode=\"r\", shape=(N,))\n",
    "w = np.memmap(mm_paths[\"w\"], dtype=np.float32, mode=\"r\", shape=(N,))\n",
    "d = np.memmap(mm_paths[\"date\"], dtype=np.int32,   mode=\"r\", shape=(N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建并加载 d 后\n",
    "assert np.all(np.diff(d) >= 0), \"memmap d 不是非降序；请检查 panel 分片或排序\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bdc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _day_ptrs_from_sorted_dates(d: np.ndarray):\n",
    "    # 假设 d 非降序\n",
    "    d = d.ravel()\n",
    "    starts = np.r_[0, np.flatnonzero(d[1:] != d[:-1]) + 1]\n",
    "    days   = d[starts]\n",
    "    ends   = np.r_[starts[1:], d.size]     # 每个 day 的 [start,end)\n",
    "    return days, starts, ends\n",
    "\n",
    "def make_sliding_cv_fast(date_ids: np.ndarray, *, n_splits: int, gap_days: int = 5, train_to_val: int = 9):\n",
    "    days, starts, ends = _day_ptrs_from_sorted_dates(date_ids)\n",
    "    N, R, K, G = len(days), int(train_to_val), int(n_splits), int(gap_days)\n",
    "    usable = N - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    v_lo = T + G\n",
    "    folds = []\n",
    "    for V_i in v_lens:\n",
    "        v_hi  = v_lo + V_i\n",
    "        tr_hi = v_lo - G\n",
    "        tr_lo = tr_hi - T\n",
    "        if tr_lo < 0 or v_hi > N:\n",
    "            break\n",
    "        # 由于 d 全局有序，每个区间对应“连续行切片”\n",
    "        tr_idx = np.arange(starts[tr_lo], ends[tr_hi-1])\n",
    "        va_idx = np.arange(starts[v_lo],   ends[v_hi-1])\n",
    "        folds.append((tr_idx, va_idx))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "# 用 fast 版\n",
    "folds = make_sliding_cv_fast(d, n_splits=2, gap_days=5, train_to_val=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 统计 N 天窗口的行数（按你真实筛选逻辑来）\n",
    "n_rows = (\n",
    "    lc.filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI)) \n",
    "      .select(pl.len())\n",
    "      .collect()\n",
    "      .item()  # -> int\n",
    ")\n",
    "\n",
    "# 2) 估算 GPU “transfer to GPU” 的大头（经验值）\n",
    "n_feat = len(feat_cols)\n",
    "dense_groups = int(n_feat)  # 按之前比例估\n",
    "bytes_est = n_rows * 0.8* dense_groups         \n",
    "gb_est = bytes_est / (1024**3)\n",
    "\n",
    "print(f\"rows≈{n_rows:,}, dense_groups≈{dense_groups}, est GPU load≈{gb_est:.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff74677",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_params = dict(\n",
    "    max_bin=63,                    # 降低直方图桶数，省显存/内存\n",
    "    bin_construct_sample_cnt=200000,# 构桶采样行数（默认是20万）\n",
    "    min_data_in_bin=3,\n",
    "    data_random_seed=42,\n",
    ")\n",
    "\n",
    "# 1) 全集 Dataset\n",
    "d_all = lgb.Dataset(\n",
    "    X, label=y, weight=w,\n",
    "    feature_name=feat_cols,\n",
    "    free_raw_data=True,\n",
    "    params=ds_params,               # 让子集也继承这些设置\n",
    ")\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"None\",\n",
    "    device_type=\"gpu\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    max_depth=8,\n",
    "    feature_fraction=0.80,\n",
    "    bagging_fraction=0.80,\n",
    "    bagging_freq=1,\n",
    "    min_data_in_leaf=200,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 2) 多折训练 + 每折 wr2 + 汇总 gain_share（仅一张表）\n",
    "\n",
    "fi = pd.DataFrame({\"feature\": feat_cols})\n",
    "scores = []\n",
    "\n",
    "for k, (tr, va) in enumerate(folds, 1):\n",
    "    dtrain = d_all.subset(tr, params=ds_params)    # 只构建本折的子集\n",
    "    dvalid = d_all.subset(va, params=ds_params)\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params, dtrain,\n",
    "        valid_sets=[dvalid, dtrain],\n",
    "        valid_names=[\"val\", \"train\"],\n",
    "        feval=lgb_wr2_eval,\n",
    "        num_boost_round=4000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "            lgb.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 每折分数\n",
    "    scores.append(bst.best_score[\"val\"][\"wr2\"])   # or bst.best_score[\"val\"][\"wr2\"]\n",
    "\n",
    "    # 每折 gain_share → 作为一列加入\n",
    "    g = bst.feature_importance(importance_type=\"gain\", iteration=bst.best_iteration).astype(float)\n",
    "    denom = g.sum()\n",
    "    fi[f\"fold{k}_gain_share\"] = (g / denom) if denom > 0 else np.zeros_like(g, dtype=float)\n",
    "    bst.free_dataset()                 # 释放 booster 里持有的 Dataset\n",
    "    del dtrain, dvalid, bst; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078611a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总均值 + 排序 + 保存\n",
    "fold_cols = [c for c in fi.columns if c.startswith(\"fold\")]\n",
    "fi[\"mean_gain_share\"] = fi[fold_cols].mean(axis=1)\n",
    "fi = fi.sort_values(\"mean_gain_share\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c083fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 汇总均值\n",
    "fi.to_csv(f\"/mnt/data/js/exp/v1/sample_mm//fe_v1_gain_share.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.read_csv(f\"/mnt/data/js/exp/v1/sample_mm//fe_v1_gain_share.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f195dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = [c for c in fi['feature'][:100] if c.startswith('responder_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb168564",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ea4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = cfg.get(\"white_list\", [])\n",
    "fi_normal = fi[~fi[\"feature\"].isin(whitelist)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823becc8",
   "metadata": {},
   "source": [
    "展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = fi_normal[[\"feature\", \"mean_gain_share\"]].copy()\n",
    "dfi.reset_index(drop=True, inplace=True)\n",
    "dfi['rank'] = dfi.index + 1\n",
    "\n",
    "cum_share = dfi[\"mean_gain_share\"].cumsum()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "ax1.plot(dfi[\"rank\"], dfi[\"mean_gain_share\"], color=\"tab:blue\")\n",
    "ax1.set_xlabel(\"Feature rank (desc)\")\n",
    "ax1.set_ylabel(\"Mean gain share\", color=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(dfi[\"rank\"], cum_share, color=\"tab:orange\")\n",
    "ax2.set_ylabel(\"Cumulative share\", color=\"tab:orange\")\n",
    "\n",
    "plt.title(\"Feature importance (gain share)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fe42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_share = dfi[\"mean_gain_share\"].cumsum()\n",
    "tot = cum_share.iloc[-1]\n",
    "for th in [0.8, 0.9, 0.95]:\n",
    "    k = (cum_share <= th*tot).sum()\n",
    "    print(f\"{th*100:.0f}% → Top-{k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf27467",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feats = list(dict.fromkeys(whitelist + dfi['feature'][:632].tolist()))  # 保持顺序且不重复\n",
    "final_feats = pd.Series(final_feats)\n",
    "\n",
    "final_feats.to_csv(\"/mnt/data/js/exp/v1/sample_mm/top_fi_0911.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(final_feats, name=\"feature\").to_csv(f\"{P('local', 'exp/v1', cfg['paths']['sample_mm'])}/top_fi_0911.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d25520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it\n",
    "df_check = pd.read_csv(f\"/mnt/data/js/exp/v1/sample_mm/top_fi_0911.csv\")\n",
    "df_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0cf82",
   "metadata": {},
   "source": [
    "# 去共线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATHS = [\"/mnt/data/js/final_clean.parquet\"]\n",
    "KEYS = [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "TARGET = \"responder_6\"\n",
    "FEATURE_COLS = pd.read_csv('/home/admin_ml/Jackson/projects/selected_features.csv')['family'].tolist()\n",
    "REP_COLS = pd.read_csv('/home/admin_ml/Jackson/projects/selected_resps.csv')['family'].tolist()\n",
    "\n",
    "lf_base = pl.scan_parquet(PARQUET_PATHS).select(KEYS+FEATURE_COLS+REP_COLS)\n",
    "\n",
    "\n",
    "lf_slice = lf_base.filter((pl.col(\"date_id\") >= 1200) & (pl.col(\"date_id\") <= 1400))\n",
    "\n",
    "PARAMS = dict(\n",
    "        prev_soft_days=7,\n",
    "        tail_lags=(2, 5, 20, 40, 100),\n",
    "        tail_diffs=(2, 5,),\n",
    "        rolling_windows=(5, 20),\n",
    "        same_time_ndays=5,\n",
    "        strict_k=False,\n",
    "        hist_lags=(1,2,5,10,20,100),\n",
    "        ret_periods=(1,5,20),\n",
    "        diff_periods=(1,5),\n",
    "        rz_windows=(5,20),\n",
    "        ewm_spans=(5,40,100),\n",
    "        cs_cols=None,       # <- keep this small to avoid blow-up\n",
    "    )\n",
    "\n",
    "lf_eng = run_engineering_on_slice(lf_slice, **PARAMS)\n",
    "\n",
    "feats = pd.read_csv(\"/home/admin_ml/Jackson/projects/final_fi_mean.csv\")[\"feature\"].tolist()\n",
    "\n",
    "lf_small = lf_eng.select(feats[:500])\n",
    "lf_small.collect(streaming=True).write_parquet(\"/mnt/data/js/X_ready.parquet\", compression=\"zstd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7dbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lf = pl.scan_parquet(\"/mnt/data/js/X_ready.parquet\")\n",
    "\n",
    "df = lf.collect(streaming=True).to_pandas()\n",
    "\n",
    "# Correlation (pairwise complete obs) + guard on min observations\n",
    "min_obs = max(50, int(0.3 * len(df)))  # tweak as you like\n",
    "C = df.corr(method=\"pearson\", min_periods=min_obs).abs().fillna(0.0)\n",
    "\n",
    "# Ensure to align rows & cols to the same (priority) order, fill any NaNs with 0\n",
    "order = feats\n",
    "C = C.reindex(index=order, columns=order).fillna(0.0).copy()\n",
    "\n",
    "\n",
    "# --- Prepare NumPy array for the greedy loop ---\n",
    "A = C.values\n",
    "np.fill_diagonal(A, 0.0)  # ensure the value is smaller than thresh, so the feature won't be dropped by value'1'\n",
    "m = len(order)\n",
    "\n",
    "# --- Greedy de-correlation (keep-by-priority, drop neighbors) ---\n",
    "THRESH = 0.97\n",
    "keep_mask = np.ones(m, dtype=bool)\n",
    "\n",
    "for i in range(m):\n",
    "    if not keep_mask[i]:\n",
    "        continue  # already removed by a higher-priority pick\n",
    "    # only check j > i (upper triangle) among still-active features\n",
    "    active_slice = keep_mask[i+1:]\n",
    "    drop = (A[i, i+1:] >= THRESH) & active_slice\n",
    "    active_slice[drop] = False  # marks into keep_mask[i+1:] via view\n",
    "keep = [order[i] for i in range(m) if keep_mask[i]]\n",
    "\n",
    "\n",
    "pd.DataFrame({'feature': keep}).to_csv(\"final_selected_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4ba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fca06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93ac3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5d57402",
   "metadata": {},
   "source": [
    "# 全数据训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67727de1",
   "metadata": {},
   "source": [
    "基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_LO, DATE_HI = 680, 1530 # 指定训练/验证的 date_id 范围, 后期转为全部训练集\n",
    "# 基本量\n",
    "FEATURE_COLS = [f\"feature_{i:02d}\" for i in range(79)] #FEATURE_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_features_1r.csv\", header=None).squeeze().tolist()\n",
    "REP_COLS = [f\"responder_{i}\" for i in range(9)] #REP_COLS = pd.read_csv(f\"{INPUT_DIR}/selected_responders_1r.csv\", header=None).squeeze().tolist()\n",
    "\n",
    "paths = fs.glob(f\"{P('az', 'exp/v1', cfg['paths']['clean'])}/*.parquet\")\n",
    "clean_files = []\n",
    "for p in paths:\n",
    "    bn = os.path.basename(p)\n",
    "    parts = bn.split('_')\n",
    "    start = int(parts[1])\n",
    "    clean_files.append((start, p))\n",
    "    \n",
    "clean_sorted_file_list = [f\"az://{f}\" for _, f in sorted(clean_files)]\n",
    "\n",
    "lc = pl.scan_parquet(clean_sorted_file_list, storage_options=storage_options)\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433ce72",
   "metadata": {},
   "source": [
    "枚举窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdca44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 Blob 列出全部 fe_shards 分片（返回不带协议的路径，要手动加 az://）\n",
    "\n",
    "fe_all = fs.glob(f\"{P('np', 'exp/v1',cfg['paths']['fe_shards'])}/*.parquet\")\n",
    "fe_all = [f\"az://{p}\" for p in fe_all]\n",
    "\n",
    "# 按日期范围筛选\n",
    "wins = set()\n",
    "for p in fe_all:\n",
    "    base = p.split(\"/\")[-1]  # e.g. C_lags_1220_1249.parquet\n",
    "    lo = int(base.split(\"_\")[-2]); hi = int(base.split(\"_\")[-1].split(\".\")[0])\n",
    "    if hi >= DATE_LO and lo <= DATE_HI:\n",
    "        wins.add((lo, hi))\n",
    "wins = sorted(wins)\n",
    "print(f\"windows in range: {wins[:5]} ... (total {len(wins)})\")\n",
    "\n",
    "# 取得该区间实际天\n",
    "days = [d for d in range(DATE_LO, DATE_HI+1)]\n",
    "#cut = int(len(days)*0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecf2eb",
   "metadata": {},
   "source": [
    "3. 按窗口拼接 (A + B + 所有 C_*) → 直接写数据分片（无大表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826625ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import gc\n",
    "\n",
    "T = int(cfg['ticks'])                 # 例如 968 \n",
    "K = int(cfg['bucket_size'])           # 例如 6\n",
    "open_n  = int(cfg.get('open_auction_ticks', 5))\n",
    "close_n = int(cfg.get('close_auction_ticks', 5))\n",
    "\n",
    "# 安全的“上界截断”工具（兼容旧版 Polars 无 clip_max）\n",
    "def clip_upper(expr: pl.Expr, ub: int) -> pl.Expr:\n",
    "    return pl.when(expr > pl.lit(ub)).then(pl.lit(ub)).otherwise(expr)\n",
    "\n",
    "for (lo, hi) in wins:\n",
    "    # 与全局区间取交集，防止边缘窗口越界\n",
    "    w_lo, w_hi = max(lo, DATE_LO), min(hi, DATE_HI)\n",
    "\n",
    "    # 基表（先筛行，再一次性加时间特征）\n",
    "    base = (\n",
    "        lc.filter(pl.col(\"date_id\").is_between(w_lo, w_hi))\n",
    "          .select([*cfg['keys'], cfg['target'], cfg['weight'], *FEATURE_COLS])\n",
    "        .with_columns([\n",
    "            # 复制一份 time_id 作为“位置特征”，避免与 key 列冲突\n",
    "            pl.col(\"time_id\").cast(pl.Float32).alias(\"time_pos\"),\n",
    "\n",
    "              # 周期相位：phase = 2π * time_id / T\n",
    "              (2*np.pi * pl.col(\"time_id\") / pl.lit(T, dtype=pl.Float32)).alias(\"_phase_\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # 兼容旧版：对表达式调用 .sin() / .cos()\n",
    "            pl.col(\"_phase_\").sin().cast(pl.Float32).alias(\"time_sin\"),\n",
    "            pl.col(\"_phase_\").cos().cast(pl.Float32).alias(\"time_cos\"),\n",
    "        ])\n",
    "        .drop([\"_phase_\"])\n",
    "        .with_columns([\n",
    "            # 开盘/收盘指示器（恰好 open_n / close_n 个 tick）\n",
    "            (pl.col(\"time_id\") <  pl.lit(open_n)).cast(pl.Int8).alias(\"is_open_auction\"),\n",
    "            (pl.col(\"time_id\") >= pl.lit(T - close_n)).cast(pl.Int8).alias(\"is_close_auction\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # 分桶：bucket = floor(time_id * K / T)，并防御性截到 [0..K-1]\n",
    "    bucket_raw = ( (pl.col('time_id') * pl.lit(K)) // pl.lit(T) )\n",
    "    bucket_capped = clip_upper(bucket_raw, K - 1)\n",
    "    base = base.with_columns([\n",
    "        bucket_capped.cast(pl.Int8).alias(f\"time_bucket_{K}\")\n",
    "    ])\n",
    "\n",
    "    lf = base  # 后面继续你的 join 逻辑\n",
    "\n",
    "    fe_files = []\n",
    "    for name in (f\"A_{lo}_{hi}.parquet\", f\"B_{lo}_{hi}.parquet\"):\n",
    "        p = f\"{P('az', 'exp/v1', cfg['paths']['fe_shards'])}/{name}\"\n",
    "        fe_files.append(p)\n",
    "\n",
    "    # 同窗口所有 C_* 分片\n",
    "    c_files = fs.glob(f\"{P('np', 'exp/v1', cfg['paths']['fe_shards'])}/C_*_{lo}_{hi}.parquet\")\n",
    "    c_files = [f\"az://{p}\" for p in c_files]\n",
    "    fe_files.extend(c_files)\n",
    "\n",
    "    # 逐个左连接\n",
    "    already = set(lf.collect_schema().names())\n",
    "    for fp in fe_files:\n",
    "        ds = pl.scan_parquet(fp, storage_options=storage_options)\n",
    "        names = ds.collect_schema().names()\n",
    "        add_cols = [c for c in names if c not in already]\n",
    "        if add_cols:\n",
    "            lf = lf.join(ds.select([*cfg['keys'], *add_cols]), on=cfg['keys'], how=\"left\")\n",
    "            already.update(add_cols)\n",
    "\n",
    "    # 选出特征并统一类型\n",
    "    feat_present = [c for c in already if c not in (*cfg['keys'], cfg['target'], cfg['weight'])]\n",
    "    select_exprs = [\n",
    "        *cfg['keys'],\n",
    "        pl.col(cfg['target']).cast(pl.Float32).alias(cfg['target']),\n",
    "        pl.col(cfg['weight']).cast(pl.Float32).alias(cfg['weight']),\n",
    "        *[pl.col(c).cast(pl.Float32).alias(c) for c in feat_present],\n",
    "    ]\n",
    "    lf_win = lf.select(select_exprs)\n",
    "\n",
    "    # 直接流式写分片\n",
    "    panel_path = P(\"az\", \"exp/v1\", cfg[\"paths\"][\"panel_shards\"])\n",
    "    fs.mkdir(panel_path, exist_ok=True)\n",
    "    out_path = f\"{panel_path}/panel_{w_lo}_{w_hi}.parquet\"\n",
    "    (\n",
    "        lf_win.sink_parquet(\n",
    "            out_path,\n",
    "            compression=\"zstd\",\n",
    "            storage_options=storage_options,\n",
    "            statistics=True,\n",
    "            maintain_order=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0369d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查关键分片/总表是否按 (symbol_id, date_id, time_id) 排序\n",
    "\n",
    "paths = sorted(fs.glob(f\"{P('az', 'exp/v1', cfg['paths']['clean_shards'])}/*.parquet\"))\n",
    "for p in paths:\n",
    "    df = pl.read_parquet(f\"az://{p}\", storage_options=storage_options).select([\"symbol_id\",\"date_id\",\"time_id\"])\n",
    "    n  = df.height\n",
    "    # 计算“按 key 的正确顺序”\n",
    "    sid = df[\"symbol_id\"].to_numpy()\n",
    "    did = df[\"date_id\"].to_numpy()\n",
    "    tid = df[\"time_id\"].to_numpy()\n",
    "    ord_keys = np.lexsort((tid, did, sid))    # 以 symbol->date->time 升序\n",
    "    is_sorted = np.all(ord_keys == np.arange(n))\n",
    "    print(os.path.basename(p), \"sorted_by_keys:\", is_sorted, \"rows:\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc69b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a744bb",
   "metadata": {},
   "source": [
    "4.导入最终特征清单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27270503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_cols = pd.read_csv(\"/mnt/data/js/exp/v1/sample_mm/top_fi_0911.csv\")\n",
    "feat_cols = feat_cols['feature'].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118550e",
   "metadata": {},
   "source": [
    "5.构建memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mm_dir = P(\"local\", \"exp/v1\", cfg[\"paths\"][\"panel_mm\"])\n",
    "os.makedirs(mm_dir, exist_ok=True)\n",
    "\n",
    "def full_shard_key(p: str):\n",
    "    bn = os.path.basename(p)          # e.g. panel_1200_1219.parquet\n",
    "    m = re.match(r\"panel_(\\d+)_(\\d+)\\.parquet$\", bn)\n",
    "    if not m:\n",
    "        return (10**12, 10**12, bn)\n",
    "    lo, hi = map(int, m.groups())\n",
    "    return (lo, hi)\n",
    "\n",
    "\n",
    "def shard2memmap(glob_paths: list[str], feat_cols: list[str], prefix: str):\n",
    "    date_col   = cfg[\"keys\"][1]\n",
    "    target_col = cfg[\"target\"]\n",
    "    weight_col = cfg[\"weight\"]\n",
    "\n",
    "    paths = sorted(glob_paths, key=full_shard_key)\n",
    "\n",
    "    counts = []\n",
    "    for p in paths:\n",
    "        k = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "               .select(pl.len()).collect(streaming=True).item())\n",
    "        counts.append(int(k))\n",
    "    n_rows, n_feat = int(sum(counts)), len(feat_cols)\n",
    "\n",
    "    os.makedirs(os.path.dirname(prefix), exist_ok=True)\n",
    "\n",
    "    X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows, n_feat))\n",
    "    y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"w+\", shape=(n_rows,))\n",
    "    d = np.memmap(f\"{prefix}.date.i32.mmap\",  dtype=\"int32\",   mode=\"w+\", shape=(n_rows,))\n",
    "\n",
    "    i = 0\n",
    "    need_cols = [date_col, target_col, weight_col, *feat_cols]\n",
    "    for p, k in zip(paths, counts):\n",
    "        df = (pl.scan_parquet(p, storage_options=storage_options)\n",
    "                .select(need_cols).collect(streaming=True))\n",
    "\n",
    "        X[i:i+k, :] = df.select(feat_cols).to_numpy()\n",
    "        y[i:i+k]    = df.select(pl.col(target_col)).to_numpy().ravel()\n",
    "        w[i:i+k]    = df.select(pl.col(weight_col)).to_numpy().ravel()\n",
    "        d[i:i+k]    = df.select(pl.col(date_col)).to_numpy().ravel().astype(\"int32\")\n",
    "        i += k\n",
    "        del df; gc.collect()\n",
    "\n",
    "    X.flush(); y.flush(); w.flush(); d.flush()\n",
    "\n",
    "    meta = {\n",
    "        \"n_rows\": int(n_rows), \"n_feat\": int(n_feat), \"dtype\": \"float32\", \"ts\": time.time(),\n",
    "        \"features\": list(feat_cols), \"target\": target_col, \"weight\": weight_col,\n",
    "        \"date_col\": date_col, \"files\": paths\n",
    "    }\n",
    "    with open(f\"{prefix}.meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "    return X, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_paths = fs.glob(f\"{P('np', 'exp/v1', cfg['paths']['panel_shards'])}/panel_*_*.parquet\")\n",
    "glob_paths = []\n",
    "for p in np_paths:\n",
    "    glob_paths.append(f\"az://{p}\")\n",
    "    \n",
    "X, y, w = shard2memmap(glob_paths= glob_paths, feat_cols=feat_cols, prefix=f\"{mm_dir}/full_panel_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑动式来分割训练集/验证集\n",
    "\n",
    "d = np.memmap(f\"/mnt/data/js/exp/v1/panel_mm/full_panel_v1.date.i32.mmap\", dtype=\"int32\", mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.memmap(\"/mnt/data/js/exp/v1/panel_mm/full_panel_v1.date.i32.mmap\", dtype=\"int32\", mode=\"r\")\n",
    "\n",
    "mono = np.all(d[1:] >= d[:-1])\n",
    "viol = np.flatnonzero(d[1:] < d[:-1])\n",
    "print(\"monotonic_non_decreasing:\", bool(mono), \"| violations:\", viol.size)\n",
    "\n",
    "# 看前几个“逆序”位置（如果有）\n",
    "for j in viol[:10]:\n",
    "    print(f\"idx {j}->{j+1}: {d[j]} -> {d[j+1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.memmap(\"/mnt/data/js/exp/v1/panel_mm/full_panel_v1.date.i32.mmap\", dtype=\"int32\", mode=\"r\")\n",
    "\n",
    "days_all = np.unique(d)\n",
    "print(\"rows:\", d.size, \"unique_days:\", days_all.size, \"min/max:\", days_all.min(), days_all.max())\n",
    "\n",
    "# 相邻唯一天的差\n",
    "gaps = np.diff(days_all)\n",
    "gap_pos = np.flatnonzero(gaps > 1)\n",
    "print(\"global missing-day blocks:\", gap_pos.size)\n",
    "\n",
    "# 看前几段缺口：[前一天, 后一天, 差值]\n",
    "if gap_pos.size:\n",
    "    preview = np.c_[days_all[gap_pos], days_all[gap_pos+1], gaps[gap_pos]]\n",
    "    print(preview[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05800ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_sliding_cv(date_ids: np.ndarray, *, n_splits: int, gap_days: int = 5, train_to_val: int = 9):\n",
    "    # ---- 构造唯一天轴 ----\n",
    "    days = np.unique(date_ids)                 # 关键：按天计算窗口\n",
    "    N, R, K, G = len(days), int(train_to_val), int(n_splits), int(gap_days)\n",
    "\n",
    "    usable = N - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    v_lo = T + G\n",
    "\n",
    "    folds = []\n",
    "    for V_i in v_lens:\n",
    "        v_hi  = v_lo + V_i\n",
    "        tr_hi = v_lo - G\n",
    "        tr_lo = tr_hi - T\n",
    "        if tr_lo < 0 or v_hi > N:\n",
    "            break\n",
    "\n",
    "        tr_days = days[tr_lo:tr_hi]\n",
    "        va_days = days[v_lo:v_hi]\n",
    "\n",
    "        tr_idx = np.flatnonzero(np.isin(date_ids, tr_days))\n",
    "        va_idx = np.flatnonzero(np.isin(date_ids, va_days))\n",
    "\n",
    "        # 保险丝（行不重叠、天不重叠、gap 按“天位置”生效）\n",
    "        assert np.intersect1d(tr_idx, va_idx).size == 0, \"row overlap\"\n",
    "        assert np.intersect1d(tr_days, va_days).size == 0, \"day overlap\"\n",
    "        gap_pos = (np.searchsorted(days, va_days.min())\n",
    "                   - np.searchsorted(days, tr_days.max()) - 1)\n",
    "        assert gap_pos >= G, f\"gap_days not enforced: {gap_pos} < {G}\"\n",
    "\n",
    "        folds.append((tr_idx, va_idx))\n",
    "        v_lo = v_hi\n",
    "\n",
    "    return folds\n",
    "\n",
    "# 用法\n",
    "d = np.memmap(\"/mnt/data/js/exp/v1/panel_mm/full_panel_v1.date.i32.mmap\", dtype=\"int32\", mode=\"r\")\n",
    "folds = make_sliding_cv(d, n_splits=3, gap_days=5, train_to_val=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_all = np.unique(d)\n",
    "for i,(tr,va) in enumerate(folds,1):\n",
    "    tr_days = np.unique(d[tr]); va_days = np.unique(d[va])\n",
    "    row_ovl = np.intersect1d(tr,va).size\n",
    "    day_ovl = np.intersect1d(tr_days,va_days).size\n",
    "    gap_pos = (np.searchsorted(days_all, va_days.min())\n",
    "               - np.searchsorted(days_all, tr_days.max()) - 1)\n",
    "    print(f\"fold{i}: row_ovl={row_ovl}, day_ovl={day_ovl}, gap_days={gap_pos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54805ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e48a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) 加载 memmap ----------\n",
    "import json, numpy as np, lightgbm as lgb\n",
    "prefix = f\"/mnt/data/js/exp/v1/panel_mm/full_panel_v1\"\n",
    "with open(f\"{prefix}.meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "n_rows, n_feat = meta[\"n_rows\"], meta[\"n_feat\"]\n",
    "feat_names = meta[\"features\"]\n",
    "\n",
    "X = np.memmap(f\"{prefix}_X.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows, n_feat))\n",
    "y = np.memmap(f\"{prefix}_y.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows,))\n",
    "w = np.memmap(f\"{prefix}_w.float32.mmap\", dtype=\"float32\", mode=\"r\", shape=(n_rows,))\n",
    "# 你之前已定义：weighted_r2_zero_mean、lgb_wr2_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19d338",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 估算 GPU “transfer to GPU” 的大头（经验值）\n",
    "\n",
    "n_rows = (\n",
    "    lc.filter(pl.col(\"date_id\").is_between(DATE_LO, DATE_HI)) \n",
    "      .select(pl.len())\n",
    "      .collect()\n",
    "      .item()  # -> int\n",
    ")\n",
    "\n",
    "n_feat = len(feat_names)\n",
    "dense_groups = int(n_feat)  # 按之前比例估\n",
    "bytes_est = n_rows * 0.8* dense_groups         \n",
    "gb_est = bytes_est / (1024**3)\n",
    "\n",
    "print(f\"rows≈{n_rows:,}, dense_groups≈{dense_groups}, est GPU load≈{gb_est:.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c600b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_params = dict(\n",
    "    max_bin=31,                    \n",
    "    bin_construct_sample_cnt=100000,\n",
    "    min_data_in_bin=3,\n",
    "    data_random_seed=42,\n",
    ")\n",
    "\n",
    "# 1) 全集 Dataset\n",
    "d_all = lgb.Dataset(\n",
    "    X, label=y, weight=w,\n",
    "    feature_name=feat_names,\n",
    "    free_raw_data=True,\n",
    "    params=ds_params,               # 让子集也继承这些设置\n",
    ")\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"None\",\n",
    "    device_type=\"gpu\",\n",
    "    num_threads=16,\n",
    "    learning_rate=0.08,\n",
    "    num_leaves=31,\n",
    "    max_depth=8,\n",
    "    feature_fraction=0.60,\n",
    "    bagging_fraction=0.60,\n",
    "    bagging_freq=1,\n",
    "    min_data_in_leaf=200,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 2) 多折训练 + 每折 wr2 + 汇总 gain_share（仅一张表）\n",
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "fi = pd.DataFrame({\"feature\": feat_names})\n",
    "scores = [] \n",
    "\n",
    "for k, (tr, va) in enumerate(folds, 1):\n",
    "    dtrain = d_all.subset(tr, params=ds_params)    # 只构建本折的子集\n",
    "    dvalid = d_all.subset(va, params=ds_params)\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params, dtrain,\n",
    "        valid_sets=[dvalid, dtrain],\n",
    "        valid_names=[\"val\", \"train\"],\n",
    "        feval=lgb_wr2_eval,\n",
    "        num_boost_round=4000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "            lgb.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 每折分数\n",
    "    scores.append(bst.best_score[\"val\"][\"wr2\"])   # or bst.best_score[\"val\"][\"wr2\"]\n",
    "\n",
    "    # 每折 gain_share → 作为一列加入\n",
    "    g = bst.feature_importance(importance_type=\"gain\", iteration=bst.best_iteration).astype(float)\n",
    "    denom = g.sum()\n",
    "    fi[f\"fold{k}_gain_share\"] = (g / denom) if denom > 0 else np.zeros_like(g, dtype=float)\n",
    "    bst.free_dataset()                 # 释放 booster 里持有的 Dataset\n",
    "    del dtrain, dvalid, bst; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8080b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总均值 + 排序 + 保存\n",
    "fold_cols = [c for c in fi.columns if c.startswith(\"fold\")]\n",
    "fi[\"mean_gain_share\"] = fi[fold_cols].mean(axis=1)\n",
    "fi = fi.sort_values(\"mean_gain_share\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1234101",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016786d",
   "metadata": {},
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4cfbf",
   "metadata": {},
   "source": [
    "## 1.数据清洗,预处理\n",
    "\n",
    "数据集：test + pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf833da3",
   "metadata": {},
   "source": [
    "## 2.特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c6aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
