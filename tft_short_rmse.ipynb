{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2317e57f",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "from pipeline.stream_input_local import ShardedBatchStream  # 使用下方给你的新版类\n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c956339",
   "metadata": {},
   "source": [
    "## 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cd586",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========== 1) 统一配置 ==========\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "# 已知时间特征（给 encoder+decoder）\n",
    "TIME_FEATURES = [\"time_bucket\"]\n",
    "\n",
    "# 连续特征（示例）\n",
    "#BASIC_FEATURES = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "BASIC_FEATURES = [\n",
    "    \"feature_01\",\n",
    "    \"feature_04\",\n",
    "    \"feature_05\",\n",
    "    \"feature_06\",\n",
    "    \"feature_07\",\n",
    "    \"feature_08\",\n",
    "    \"feature_16\",\n",
    "    \"feature_20\",\n",
    "    \"feature_22\",\n",
    "    \"feature_24\",\n",
    "    \"feature_27\",\n",
    "    \"feature_36\",\n",
    "    \"feature_37\",\n",
    "    \"feature_38\",\n",
    "    \"feature_58\",\n",
    "    \"feature_69\",\n",
    "]\n",
    "BASIC_RESPONDERS = [f\"responder_{i}\" for i in range(0, 9)]\n",
    "\n",
    "RAW_FEATURES = BASIC_FEATURES \n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 5\n",
    "TRAIN_TO_VAL = 8\n",
    "ENC_LEN      = 50\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 256\n",
    "LR           = 3e-4\n",
    "HIDDEN       = 32\n",
    "HEADS        = 2\n",
    "DROPOUT      = 0.2\n",
    "MAX_EPOCHS   = 30\n",
    "CHUNK_DAYS   = 30\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\")\n",
    "\n",
    "# 目录准备\n",
    "ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "print(\"[config] ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300bbba",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04763c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_paths = fs.glob(f\"{PANEL_DIR_AZ}/*.parquet\")\n",
    "#data_paths = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "data_start_date = 1500\n",
    "data_end_date = 1580\n",
    "\n",
    "\n",
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/clean_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = pl.scan_parquet(data_paths, storage_options=storage_options).filter(pl.col(G_DATE).is_between(data_start_date, data_end_date, closed=\"both\"))\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeee704",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_data.limit(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1aaf",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d516",
   "metadata": {},
   "source": [
    "### 添加全局时间序列号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_grid = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "\n",
    "NEED_COLS = list(dict.fromkeys([G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL] + TIME_FEATURES + RAW_FEATURES + BASIC_RESPONDERS))\n",
    "lf_with_time_idx = (\n",
    "    lf_data.join(lf_grid, on=[G_DATE, G_TIME], how=\"left\")\n",
    "        .select([\"time_idx\"] + NEED_COLS)\n",
    "        .sort([G_SYM, \"time_idx\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cf4cf",
   "metadata": {},
   "source": [
    "### 简单的特征工程\n",
    "\n",
    "*完成初步验证后就删掉，用回由LGBM选中的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ba19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加每天的时间正余弦\n",
    "\n",
    "T = 968\n",
    "twopi_over_T = np.float32(2.0*np.pi) / T\n",
    "ti_f = pl.col(G_TIME).cast(pl.Float32)\n",
    "\n",
    "# 基表筛列 + 时间特征\n",
    "lf_with_sincos = (\n",
    "    lf_with_time_idx\n",
    "    .with_columns([\n",
    "        ti_f.alias(\"time_pos\"),\n",
    "        (ti_f * pl.lit(twopi_over_T, dtype=pl.Float32)).alias(\"_phase_\"),\n",
    "        ])\n",
    "    .with_columns([\n",
    "        pl.col(\"_phase_\").sin().cast(pl.Float32).alias(\"time_sin\"),\n",
    "        pl.col(\"_phase_\").cos().cast(pl.Float32).alias(\"time_cos\"),\n",
    "        ])\n",
    "    .drop([\"_phase_\"])\n",
    ").sort([G_SYM, \"time_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822feea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_with_sincos.limit(5).collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1475df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#先做responders日级摘要\n",
    "\n",
    "# 日频统计聚合\n",
    "aggs = []\n",
    "for r in BASIC_RESPONDERS:\n",
    "    aggs.extend([\n",
    "        pl.col(r).min().alias(f\"{r}_prevday_min\"),\n",
    "        pl.col(r).max().alias(f\"{r}_prevday_max\"),\n",
    "        pl.col(r).mean().alias(f\"{r}_prevday_mean\"),\n",
    "        pl.col(r).std(ddof=0).alias(f\"{r}_prevday_std\"),\n",
    "        pl.col(r).first().alias(f\"{r}_prevday_open\"),\n",
    "        pl.col(r).last().alias(f\"{r}_prevday_close\"),\n",
    "        (pl.col(r).last() / (pl.col(r).first() + 1e-8) - 1).alias(f\"{r}_prevday_chg\"),\n",
    "    ])\n",
    "\n",
    "lf_daily = (\n",
    "    lf_with_sincos\n",
    "    .group_by([G_SYM, G_DATE])\n",
    "    .agg(aggs)\n",
    "    .sort([G_SYM, G_DATE])\n",
    ")\n",
    "lf_daily.limit(5).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34343954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# responders ema\n",
    "R_EMA_SPANS = [3, 5, 7]\n",
    "\n",
    "# 取列名（LazyFrame）\n",
    "schema_cols = lf_daily.collect_schema().names()  # 或 lf_daily.schema.names()\n",
    "r_cols = [c for c in schema_cols if c.startswith(\"responder_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ewm_exprs = []\n",
    "for c in r_cols:\n",
    "    for s in R_EMA_SPANS:\n",
    "        r_ewm_exprs.append(\n",
    "            pl.col(c)\n",
    "            .ewm_mean(span=int(s), adjust=False, ignore_nulls=True, min_periods=1)\n",
    "            .over(G_SYM)\n",
    "            .cast(pl.Float32)\n",
    "            .alias(f\"{c}__ewm{s}\")\n",
    "        )\n",
    "\n",
    "lf_daily = lf_daily.with_columns(r_ewm_exprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c537058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断日频表的每个组的日期是否连续\n",
    "lf_daily_with_diff =lf_daily.with_columns(\n",
    "    [pl.col(G_DATE).diff().over(G_SYM).alias(\"date_diff\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 关键，shift为前一日\n",
    "shifted_cols = [c for c in lf_daily.collect_schema().names() if c not in (G_SYM, G_DATE, \"date_diff\")]\n",
    "lf_prevday_daily = (\n",
    "    lf_daily_with_diff\n",
    "    .with_columns([\n",
    "        pl.when(pl.col(\"date_diff\") <= 5)\n",
    "        .then(pl.col(c).shift(1).over([G_SYM]))  # ← 按组上一天\n",
    "        .otherwise(pl.lit(None))                # 超过阈值置空\n",
    "        .alias(c)                               # 覆盖原列（你当前的语义）\n",
    "        for c in shifted_cols\n",
    "    ])\n",
    "    .drop(\"date_diff\")\n",
    ")\n",
    "\n",
    "# 回拼到 tick 级\n",
    "lf_extended = (\n",
    "    lf_with_sincos.join(lf_prevday_daily, on=[G_SYM, G_DATE], how=\"left\")\n",
    ").sort([G_SYM, \"time_idx\"])\n",
    "to_drop = [c for c in BASIC_RESPONDERS if c != TARGET_COL]\n",
    "lf_extended = lf_extended.drop(pl.col(to_drop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_cols(-1)      # 显示所有列\n",
    "pl.Config.set_tbl_rows(-1)      # 显示所有行\n",
    "lf_extended.select(pl.all().is_null().sum()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加特征量的ewa\n",
    "\n",
    "feature_ewm_exprs = []\n",
    "feature_ewm_cols = []\n",
    "for c in BASIC_FEATURES:\n",
    "    for s in R_EMA_SPANS:\n",
    "        col_name = f\"{c}__ewm{s}\"\n",
    "        feature_ewm_cols.append(col_name)\n",
    "        feature_ewm_exprs.append(\n",
    "            pl.col(c)\n",
    "            .ewm_mean(span=int(s), adjust=False, ignore_nulls=True, min_periods=1)\n",
    "            .over([G_SYM, G_DATE])\n",
    "            .cast(pl.Float32)\n",
    "            .alias(col_name)\n",
    "        )\n",
    "lf_feature_ewm = lf_with_sincos.with_columns(feature_ewm_exprs)\n",
    "lf_feature_ewm = lf_feature_ewm.drop(pl.col(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964617ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_combined = (\n",
    "    lf_extended\n",
    "    .join(lf_feature_ewm.select([G_SYM, G_DATE, G_TIME] + feature_ewm_cols), on=[G_SYM, G_DATE, G_TIME], how=\"left\")\n",
    ").sort([G_SYM, \"time_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dafbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤掉第一天的数据\n",
    "lf_ready = lf_combined.filter(pl.col(G_DATE)>1500) #  去掉全是null 的第一天数据\n",
    "print(f\"[{_now()}] lazyframe ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04416ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_r_cols = [c for c in lf_ready.collect_schema().names() if c.startswith(\"responder_\")]\n",
    "fill_exprs = [pl.col(c).fill_null(0).cast(pl.Float32).alias(c) for c in missing_r_cols]\n",
    "    \n",
    "lf_ready = lf_ready.with_columns(fill_exprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_cols(-1)      # 显示所有列\n",
    "pl.Config.set_tbl_rows(-1)      # 显示所有行\n",
    "lf_ready.select(pl.all().is_null().sum()).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e04edc",
   "metadata": {},
   "source": [
    "## CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_ready.select(pl.col(G_DATE)).unique().sort(by=G_DATE)\n",
    "    .collect(streaming=True).get_column(G_DATE).to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5315",
   "metadata": {},
   "source": [
    "## 折内数据标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5174a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_ready.collect_schema().names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41652fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 明确要标准化的列\n",
    "z_cols = [c for c in lf_ready.collect_schema().names() if c not in (G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\", WEIGHT_COL, TARGET_COL)]\n",
    "\n",
    "\n",
    "# 取第一个 fold 的训练集最后一天，作为统计 z-score 的上界\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"标准化使用训练集的上限日期 = {stats_hi}\")\n",
    "\n",
    "# ========== 4) 连续特征清洗 + Z-score ==========\n",
    "inf2null_exprs  = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c) for c in z_cols]\n",
    "#isna_flag_exprs = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\") for c in z_cols]\n",
    "\n",
    "ffill_exprs = [pl.col(c).forward_fill().over(G_SYM).fill_null(0.0).alias(c) for c in z_cols]\n",
    "lf_clean = (\n",
    "    lf_ready.with_columns(inf2null_exprs).with_columns(ffill_exprs) #.with_columns(isna_flag_exprs)\n",
    ")\n",
    "\n",
    "\n",
    "# 开始计算 z-score\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .group_by(G_SYM)\n",
    "            .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in z_cols] +\n",
    "                [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in z_cols])\n",
    ")\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in z_cols] +\n",
    "                    [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in z_cols])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\").join(lf_stats_sym, on=G_SYM, how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "eps = 1e-6\n",
    "#Z_COLS, NAMARK_COLS = [], [f\"{c}__isna\" for c in RAW_FEATURES]\n",
    "\n",
    "Z_COLS = []\n",
    "for c in z_cols:\n",
    "    mu_sym, std_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_glb, std_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    mu_use, std_use = f\"{c}_mu_use\", f\"{c}_std_use\"\n",
    "    z_name = f\"{c}_z\"\n",
    "\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_sym).is_null()).then(pl.col(mu_glb)).otherwise(pl.col(mu_sym)).alias(mu_use),\n",
    "        pl.when(pl.col(std_sym).is_null() | (pl.col(std_sym) == 0)).then(pl.col(std_glb)).otherwise(pl.col(std_sym)).alias(std_use),\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(mu_use)) / (pl.col(std_use) + eps)).alias(z_name)\n",
    "    ).drop([mu_glb, std_glb, mu_sym, std_sym, mu_use, std_use])\n",
    "\n",
    "    Z_COLS.append(z_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_COLS = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL, \"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"] + Z_COLS \n",
    "lf_out = lf_z.select(OUT_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0575179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存lf_out到本地\n",
    "clean_path_local = P(\"local\", \"tft/panel/clean.parquet\")\n",
    "Path(clean_path_local).parent.mkdir(parents=True, exist_ok=True)\n",
    "lf_out.collect(streaming=True).write_parquet(clean_path_local, compression=\"zstd\")\n",
    "print(f\"[{_now()}] cleaned data saved to {clean_path_local}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028cc3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean 数据导入\n",
    "clean_path_local = P(\"local\", \"tft/panel/clean.parquet\")\n",
    "lf_clean = pl.scan_parquet(clean_path_local)\n",
    "lf_clean.limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_REALS = [c for c in lf_clean.collect_schema().names() if c not in (G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_bucket\", WEIGHT_COL, TARGET_COL)]\n",
    "\n",
    "KNOWN_CATEGORIES = [\"time_bucket\"]\n",
    "\n",
    "UNSCALE_COLS = KNOWN_REALS\n",
    "\n",
    "TRAIN_COLS = [c for c in lf_clean.collect_schema().names() if c not in (G_DATE, G_TIME)]\n",
    "\n",
    "# 定义 identity scalers\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8f841",
   "metadata": {},
   "source": [
    "## try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")   \n",
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pdf_data[TRAIN_COLS].copy()\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_unknown_reals=UNKNOWN_REALS,\n",
    "    \n",
    "    lags=None,\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=False,\n",
    "    add_target_scales=False,\n",
    "    add_encoder_length=False,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=TorchNormalizer(method=\"identity\"),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction >= val_start_idx) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c11547",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "# ========== 8) 训练（按 CV 折） ========== 先取第一折\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "#for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "####################################\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[fold_id]\n",
    "####################################\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    lf_clean\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ") \n",
    "\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51eb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建训练集 timeseries dataset\n",
    "\n",
    "t_data = pdf_data[TRAIN_COLS]\n",
    "train_df = t_data.loc[t_data[\"time_idx\"] <= train_end_idx]\n",
    "\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, min_prediction_length=PRED_LEN,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    static_categoricals=[G_SYM],\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                        \"time_bucket\": NaNLabelEncoder(add_nan=True),},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a95f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 验证集复用 train_ds 的所有 encoders/normalizer（不泄漏）\n",
    "\n",
    "val_df = t_data.loc[t_data[\"time_idx\"].between(val_start_idx, val_end_idx, inclusive=\"both\")]\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    val_df,\n",
    "    min_prediction_idx=val_start_idx+ENC_LEN+PRED_LEN-1,\n",
    "    stop_randomization=True,\n",
    "    predict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0412450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE*4,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "# 8.6 callbacks/logger/trainer\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2, check_on_train_epoch_end=False),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, dirpath=ckpt_dir_fold.as_posix(), filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\", save_on_train_epoch_end=False),\n",
    "            LearningRateMonitor(logging_interval=\"step\"),]\n",
    "RUN_NAME = (f\"f{fold_id}\"f\"_E{MAX_EPOCHS}\"f\"_lr{LR}\"f\"_bs{BATCH_SIZE}\"f\"_enc{ENC_LEN}_dec{DEC_LEN}\"f\"_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "logger = TensorBoardLogger(save_dir=LOGS_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=MAX_EPOCHS,\n",
    "                    accelerator=\"gpu\",\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    enable_model_summary=True,\n",
    "                    gradient_clip_val=1.0,\n",
    "                    #fast_dev_run=True,\n",
    "                    #limit_train_batches=300,  # coment in for training, running valiation every 30 batches\n",
    "                    log_every_n_steps=200,\n",
    "                    callbacks=callbacks,\n",
    "                    logger=logger,\n",
    "                    accumulate_grad_batches=4\n",
    "                    )\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[RMSE()],\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (torch.square(y_true - y_pred) * w).sum()\n",
    "den = (torch.square(y_true) * w).sum()  \n",
    "wr2 = 1 - num / den\n",
    "print(f\"wr2 after training: {wr2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
