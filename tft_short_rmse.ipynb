{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# â”€â”€ æ ‡å‡†åº“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# â”€â”€ ç¬¬ä¸‰æ–¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer\n",
    "\n",
    "\n",
    "# ä½ çš„å·¥ç¨‹å·¥å…·\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "from pipeline.stream_input_local import ShardedBatchStream  # ä½¿ç”¨ä¸‹æ–¹ç»™ä½ çš„æ–°ç‰ˆç±»\n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- æ€§èƒ½/å…¼å®¹å¼€å…³ï¼ˆä»…ä¸€æ¬¡ï¼‰----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "def _now() -> str:\n",
    "    import time as _t\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# æ»‘åŠ¨çª—åˆ’åˆ†\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-08 12:30:35] imports ok\n",
      "[config] ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"[{_now()}] imports ok\")\n",
    "\n",
    "# ========== 1) ç»Ÿä¸€é…ç½® ==========\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # å…è®¸ä¸º None\n",
    "\n",
    "# å·²çŸ¥æ—¶é—´ç‰¹å¾ï¼ˆç»™ encoder+decoderï¼‰\n",
    "TIME_FEATURES = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "\n",
    "# è¿ç»­ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼‰\n",
    "BASIC_FEATURES = [\"feature_36\", \n",
    "                \"feature_06\", \n",
    "                \"feature_04\", \n",
    "                \"feature_16\", \n",
    "                \"feature_69\", \n",
    "                \"feature_22\",\n",
    "                \"feature_20\", \n",
    "                \"feature_58\", \n",
    "                \"feature_24\", \n",
    "                \"feature_27\",\n",
    "                \"feature_37\"]\n",
    "\n",
    "RAW_FEATURES = BASIC_FEATURES \n",
    "\n",
    "# è®­ç»ƒ & CV è¶…å‚\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 7\n",
    "TRAIN_TO_VAL = 5\n",
    "ENC_LEN      = 10\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 512   \n",
    "LR           = 1e-3\n",
    "HIDDEN       = 16\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.1\n",
    "MAX_EPOCHS   = 10\n",
    "CHUNK_DAYS   = 20\n",
    "\n",
    "# æ•°æ®è·¯å¾„\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\")\n",
    "\n",
    "# ç›®å½•å‡†å¤‡\n",
    "ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "print(\"[config] ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0575179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-08 12:30:36] lazyframe ready\n",
      "[stats] upper bound day for z-score = 1634\n"
     ]
    }
   ],
   "source": [
    "# ========== 2) è¯»å–åŸå§‹é¢æ¿ & å»ºå…¨å±€ time_idx ==========\n",
    "data_paths = fs.glob(f\"{PANEL_DIR_AZ}/*.parquet\")\n",
    "data_paths = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in data_paths]\n",
    "lf_data = pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "lf_data = lf_data.filter(pl.col(G_DATE).is_between(1610, 1650, closed=\"both\"))\n",
    "\n",
    "lf_grid = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "grid_path_local = P(\"local\", \"tft/panel/grid_timeidx.parquet\")\n",
    "Path(grid_path_local).parent.mkdir(parents=True, exist_ok=True)\n",
    "lf_grid.collect(streaming=True).write_parquet(grid_path_local, compression=\"zstd\")\n",
    "grid_lazy = pl.scan_parquet(grid_path_local)\n",
    "\n",
    "NEED_COLS = list(dict.fromkeys([G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL] + TIME_FEATURES + RAW_FEATURES))\n",
    "lf0 = (\n",
    "    lf_data.join(grid_lazy, on=[G_DATE, G_TIME], how=\"left\")\n",
    "        .select(NEED_COLS + [\"time_idx\"])\n",
    "        .sort([G_DATE, G_TIME, G_SYM])\n",
    ")\n",
    "print(f\"[{_now()}] lazyframe ready\")\n",
    "\n",
    "# ========== 3) CV åˆ’åˆ† ==========\n",
    "all_days = (\n",
    "    lf0.select(pl.col(G_DATE)).unique().sort(by=G_DATE)\n",
    "    .collect(streaming=True).get_column(G_DATE).to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"[stats] upper bound day for z-score = {stats_hi}\")\n",
    "\n",
    "# ========== 4) è¿ç»­ç‰¹å¾æ¸…æ´— + Z-score ==========\n",
    "inf2null_exprs  = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c) for c in RAW_FEATURES]\n",
    "isna_flag_exprs = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\") for c in RAW_FEATURES]\n",
    "ffill_exprs     = [pl.col(c).forward_fill().over(G_SYM).fill_null(0.0).alias(c) for c in RAW_FEATURES]\n",
    "\n",
    "lf_clean = (\n",
    "    lf0.with_columns(inf2null_exprs)\n",
    "    .with_columns(isna_flag_exprs)\n",
    "    .with_columns(ffill_exprs)\n",
    ")\n",
    "\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .group_by(G_SYM)\n",
    "            .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in RAW_FEATURES] +\n",
    "                [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in RAW_FEATURES])\n",
    ")\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in RAW_FEATURES] +\n",
    "                    [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in RAW_FEATURES])\n",
    ")\n",
    "\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\").join(lf_stats_sym, on=G_SYM, how=\"left\")\n",
    "\n",
    "eps = 1e-6\n",
    "Z_COLS, NAMARK_COLS = [], [f\"{c}__isna\" for c in RAW_FEATURES]\n",
    "for c in RAW_FEATURES:\n",
    "    mu_sym, std_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_glb, std_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    mu_use, std_use = f\"{c}_mu_use\", f\"{c}_std_use\"\n",
    "    z_name = f\"{c}_z\"\n",
    "\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_sym).is_null()).then(pl.col(mu_glb)).otherwise(pl.col(mu_sym)).alias(mu_use),\n",
    "        pl.when(pl.col(std_sym).is_null() | (pl.col(std_sym) == 0)).then(pl.col(std_glb)).otherwise(pl.col(std_sym)).alias(std_use),\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(mu_use)) / (pl.col(std_use) + eps)).alias(z_name)\n",
    "    ).drop([mu_glb, std_glb, mu_sym, std_sym, mu_use, std_use])\n",
    "\n",
    "    Z_COLS.append(z_name)\n",
    "\n",
    "OUT_COLS = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL] + TIME_FEATURES + Z_COLS + NAMARK_COLS\n",
    "lf_out = lf_z.select(OUT_COLS).sort([G_DATE, G_TIME, G_SYM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c93231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-08 12:30:40] chunk 1/3 -> days 1610..1629 written (local)\n",
      "[2025-10-08 12:30:42] chunk 2/3 -> days 1630..1649 written (local)\n",
      "[2025-10-08 12:30:43] chunk 3/3 -> days 1650..1650 written (local)\n"
     ]
    }
   ],
   "source": [
    "# ========== 5) æŒ‰ 30 å¤©åˆ†ç»„è½åœ°ï¼ˆFeatherï¼Œå•æ–‡ä»¶/å—ï¼‰ ==========\n",
    "day_list   = list(map(int, all_days))\n",
    "day_chunks = [day_list[i:i + CHUNK_DAYS] for i in range(0, len(day_list), CHUNK_DAYS)]\n",
    "\n",
    "for ci, chunk in enumerate(day_chunks, 1):\n",
    "    df_chunk = lf_out.filter(pl.col(G_DATE).is_in(chunk)).collect()\n",
    "    local_chunk_dir = f\"{LOCAL_CLEAN_DIR}/chunk_{chunk[0]:04d}_{chunk[-1]:04d}\"\n",
    "    Path(local_chunk_dir).mkdir(parents=True, exist_ok=True)\n",
    "    df_chunk.write_ipc(f\"{local_chunk_dir}/data.feather\")\n",
    "    print(f\"[{_now()}] chunk {ci}/{len(day_chunks)} -> days {chunk[0]}..{chunk[-1]} written (local)\")\n",
    "\n",
    "# ========== 6) å½’å¹¶ chunk â†’ è·¯å¾„æ¸…å•ï¼ˆFeatherï¼‰ ==========\n",
    "all_paths = sorted(p.as_posix() for p in Path(LOCAL_CLEAN_DIR).glob(\"chunk_*\") if p.is_dir())\n",
    "\n",
    "# ========== 7) è®­ç»ƒåˆ—ï¼ˆknown/unknown åˆ†å¼€ï¼‰ ==========\n",
    "UNKNOWN_REALS = TIME_FEATURES + Z_COLS + NAMARK_COLS\n",
    "TRAIN_COLS    = [G_SYM, \"time_idx\", WEIGHT_COL, TARGET_COL, *UNKNOWN_REALS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1] train 1610..1634 (25 days), val 1642..1647 (6 days)\n",
      "[fold 1] train idx up to 24199, val idx 30976..36783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] train_loader batches = 1825\n",
      "[debug] val_loader batches = 443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Seed set to 42\n",
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.4517226815223694\n",
      "Number of parameters in network: 52.9k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:06<00:00, 15.90it/s]\n",
      "LR finder stopped early after 96 steps due to diverging loss.\n",
      "Restoring states from the checkpoint path at /home/admin_ml/Jackson/projects/js/JS/.lr_find_7d36c756-d356-4778-935f-b35ed3a1644e.ckpt\n",
      "Restored all states from the checkpoint at /home/admin_ml/Jackson/projects/js/JS/.lr_find_7d36c756-d356-4778-935f-b35ed3a1644e.ckpt\n",
      "Learning rate set to 5.888436553555889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 5.888436553555889e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========== 8) è®­ç»ƒï¼ˆæŒ‰ CV æŠ˜ï¼‰ ==========\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "    print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "        f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "    \n",
    "    # æ˜ç¡®æ—¥æœŸï¼š\n",
    "    train_start_date = int(train_days[0])\n",
    "    train_end_date   = int(train_days[-1])\n",
    "    val_start_date   = int(val_days[0])\n",
    "    val_end_date     = int(val_days[-1])      \n",
    "    \n",
    "    # æå–æ•°æ®\n",
    "    date_range = (train_start_date, val_end_date)\n",
    "    pdf_data = (\n",
    "        pl.scan_ipc(all_paths)\n",
    "        .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "        .collect(streaming=True)\n",
    "        .to_pandas()\n",
    "    )\n",
    "    \n",
    "    pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "    pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)        \n",
    "    # æ˜ç¡® indexes:\n",
    "    train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "    val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "    val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "    assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "    train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "    print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "    \n",
    "    pdf_data = pdf_data[TRAIN_COLS].copy()\n",
    "\n",
    "    # æ„å»ºè®­ç»ƒé›† timeseries dataset\n",
    "    identity_scalers = {name: None for name in (UNKNOWN_REALS)}\n",
    "    \n",
    "    base_ds = TimeSeriesDataSet(\n",
    "        pdf_data,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=TARGET_COL,\n",
    "        group_ids=[G_SYM],\n",
    "        weight=WEIGHT_COL,\n",
    "        max_encoder_length=ENC_LEN, \n",
    "        min_encoder_length=ENC_LEN,\n",
    "        max_prediction_length=PRED_LEN, \n",
    "        min_prediction_length=PRED_LEN,\n",
    "        \n",
    "        static_categoricals=[G_SYM],\n",
    "        \n",
    "        time_varying_unknown_reals=UNKNOWN_REALS,\n",
    "        \n",
    "        lags=None,\n",
    "        categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True)},\n",
    "        add_relative_time_idx=False,\n",
    "        add_target_scales=False,\n",
    "        add_encoder_length=False,\n",
    "        allow_missing_timesteps=True,\n",
    "        target_normalizer=None,\n",
    "        scalers=identity_scalers,\n",
    "    )\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†\n",
    "    train_ds = base_ds.filter(\n",
    "        lambda idx: (\n",
    "            idx.time_idx_last <= train_end_idx\n",
    "        ),\n",
    "        copy=True\n",
    "    )\n",
    "    \n",
    "    val_ds = base_ds.filter(\n",
    "        lambda idx: (\n",
    "            (idx.time_idx_first_prediction >= val_start_idx) &\n",
    "            \n",
    "            (idx.time_idx_last <= val_end_idx)\n",
    "        ),\n",
    "        copy=True\n",
    "    )\n",
    "    \n",
    "    # æ•°æ®é›†åŠ è½½\n",
    "    \n",
    "    train_loader = train_ds.to_dataloader(\n",
    "        train=True, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "    \n",
    "    n_train_batches = len(train_loader)\n",
    "    print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "    assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "    \n",
    "    val_loader = val_ds.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    n_val_batches = len(val_loader)\n",
    "    print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "    assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "    \n",
    "    # calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "    baseline_predictions = Baseline().predict(val_loader, return_y=True)\n",
    "    rmse = RMSE()(baseline_predictions.output, baseline_predictions.y)\n",
    "    print(f\"rmse: {rmse}\")\n",
    "    \n",
    "    lp.seed_everything(42)\n",
    "    trainer = lp.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "        # of the gradient for recurrent neural networks\n",
    "        gradient_clip_val=0.5,\n",
    "    )\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        train_ds,\n",
    "        # not meaningful for finding the learning rate but otherwise very important\n",
    "        learning_rate=LR,\n",
    "        hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "        # number of attention heads. Set to up to 4 for large datasets\n",
    "        attention_head_size=HEADS,\n",
    "        dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "        hidden_continuous_size=HIDDEN,  # set to <= hidden_size\n",
    "        loss=RMSE(),\n",
    "        optimizer=torch.optim.Adam,\n",
    "        # reduce learning rate if no improvement in validation loss after x epochs\n",
    "        # reduce_on_plateau_patience=1000,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "        \n",
    "    # find optimal learning rate\n",
    "    from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "    res = Tuner(trainer).lr_find(\n",
    "        tft,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader,\n",
    "        max_lr=10.0,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "    fig = res.plot(show=True, suggest=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1] train 1610..1634 (25 days), val 1642..1647 (6 days)\n",
      "[fold 1] train idx up to 24199, val idx 30976..36783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] train_loader batches = 1825\n",
      "[debug] val_loader batches = 443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 520    | train\n",
      "3  | prescalers                         | ModuleDict                      | 832    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 48     | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 38.2 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 0      | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 17     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "52.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "52.9 K    Total params\n",
      "0.212     Total estimated model params size (MB)\n",
      "417       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.4517226815223694\n",
      "Number of parameters in network: 52.9k\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:22<00:00,  2.20it/s, v_num=3131, train_loss_step=1.410, val_loss=0.912, train_loss_epoch=1.340]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ========== 8) è®­ç»ƒï¼ˆæŒ‰ CV æŠ˜ï¼‰ ==========\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "    print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "        f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "    \n",
    "    # æ˜ç¡®æ—¥æœŸï¼š\n",
    "    train_start_date = int(train_days[0])\n",
    "    train_end_date   = int(train_days[-1])\n",
    "    val_start_date   = int(val_days[0])\n",
    "    val_end_date     = int(val_days[-1])      \n",
    "    \n",
    "    # æå–æ•°æ®\n",
    "    date_range = (train_start_date, val_end_date)\n",
    "    pdf_data = (\n",
    "        pl.scan_ipc(all_paths)\n",
    "        .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "        .collect(streaming=True)\n",
    "        .to_pandas()\n",
    "    )\n",
    "    \n",
    "    pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "    pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)        \n",
    "    # æ˜ç¡® indexes:\n",
    "    train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "    val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "    val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "    assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "    train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "    print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "    \n",
    "    pdf_data = pdf_data[TRAIN_COLS].copy()\n",
    "\n",
    "    # æ„å»ºè®­ç»ƒé›† timeseries dataset\n",
    "    identity_scalers = {name: None for name in (UNKNOWN_REALS)}\n",
    "    \n",
    "    base_ds = TimeSeriesDataSet(\n",
    "        pdf_data,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=TARGET_COL,\n",
    "        group_ids=[G_SYM],\n",
    "        weight=WEIGHT_COL,\n",
    "        max_encoder_length=ENC_LEN, \n",
    "        min_encoder_length=ENC_LEN,\n",
    "        max_prediction_length=PRED_LEN, \n",
    "        min_prediction_length=PRED_LEN,\n",
    "        \n",
    "        static_categoricals=[G_SYM],\n",
    "        \n",
    "        time_varying_unknown_reals=UNKNOWN_REALS,\n",
    "        \n",
    "        lags=None,\n",
    "        categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True)},\n",
    "        add_relative_time_idx=False,\n",
    "        add_target_scales=False,\n",
    "        add_encoder_length=False,\n",
    "        allow_missing_timesteps=True,\n",
    "        target_normalizer=None,\n",
    "        scalers=identity_scalers,\n",
    "    )\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†\n",
    "    train_ds = base_ds.filter(\n",
    "        lambda idx: (\n",
    "            idx.time_idx_last <= train_end_idx\n",
    "        ),\n",
    "        copy=True\n",
    "    )\n",
    "    \n",
    "    val_ds = base_ds.filter(\n",
    "        lambda idx: (\n",
    "            (idx.time_idx_first_prediction >= val_start_idx) &\n",
    "            \n",
    "            (idx.time_idx_last <= val_end_idx)\n",
    "        ),\n",
    "        copy=True\n",
    "    )\n",
    "    \n",
    "    # æ•°æ®é›†åŠ è½½\n",
    "    \n",
    "    train_loader = train_ds.to_dataloader(\n",
    "        train=True, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "    \n",
    "    n_train_batches = len(train_loader)\n",
    "    print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "    assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "    \n",
    "    val_loader = val_ds.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    n_val_batches = len(val_loader)\n",
    "    print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "    assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "    \n",
    "    # calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "    baseline_predictions = Baseline().predict(val_loader, return_y=True)\n",
    "    rmse = RMSE()(baseline_predictions.output, baseline_predictions.y)\n",
    "    print(f\"rmse: {rmse}\")\n",
    "    \n",
    "    lp.seed_everything(42) \n",
    "\n",
    "    # 8.6 callbacks/logger/trainer\n",
    "    ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "    ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            mode=\"min\", \n",
    "            patience=3, \n",
    "            check_on_train_epoch_end=False\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            dirpath=ckpt_dir_fold.as_posix(),\n",
    "            filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\",\n",
    "            save_on_train_epoch_end=False\n",
    "        )\n",
    "        \n",
    "    ]\n",
    "    RUN_NAME = (\n",
    "        f\"f{fold_id}\"\n",
    "        f\"_E{MAX_EPOCHS}\"\n",
    "        f\"_lr{LR}\"\n",
    "        f\"_bs{BATCH_SIZE}\"\n",
    "        f\"_enc{ENC_LEN}_dec{DEC_LEN}\"\n",
    "        f\"_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    )\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=LOGS_DIR.as_posix(),\n",
    "        name=\"tft\",\n",
    "        version=RUN_NAME,\n",
    "        default_hp_metric=False\n",
    "    )\n",
    "\n",
    "    trainer = lp.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        train_ds,\n",
    "        learning_rate=0.01,\n",
    "        hidden_size=16,\n",
    "        attention_head_size=HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        hidden_continuous_size=HIDDEN,\n",
    "        loss=RMSE(),\n",
    "        log_interval=30,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        optimizer=torch.optim.Adam,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader,\n",
    "        )\n",
    "    \n",
    "    ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "    best_path = ckpt_cb.best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "    predictions = best_tft.predict(\n",
    "        val_loader,\n",
    "        return_y=True,\n",
    "        trainer_kwargs=dict(accelerator=\"cpu\")\n",
    "    )\n",
    "    rmse = RMSE()(predictions.output, predictions.y,)\n",
    "    print(f\"rmse after training: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
