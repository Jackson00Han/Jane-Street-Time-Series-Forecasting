{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "from pipeline.stream_input_local import ShardedBatchStream  # 使用下方给你的新版类\n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "def _now() -> str:\n",
    "    import time as _t\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[{_now()}] imports ok\")\n",
    "\n",
    "# ========== 1) 统一配置 ==========\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "# 已知时间特征（给 encoder+decoder）\n",
    "TIME_FEATURES = [\"time_bucket\"]\n",
    "\n",
    "# 连续特征（示例）\n",
    "BASIC_FEATURES = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "#BASIC_FEATURES = [\"feature_36\", \"feature_06\", \"feature_04\", \"feature_16\", \"feature_69\", \"feature_22\",\"feature_20\", \"feature_58\", \"feature_24\", \"feature_27\",\"feature_37\"]\n",
    "\n",
    "RAW_FEATURES = BASIC_FEATURES \n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 5\n",
    "TRAIN_TO_VAL = 5\n",
    "ENC_LEN      = 10\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 512 \n",
    "LR           = 1e-3\n",
    "HIDDEN       = 16\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.1\n",
    "MAX_EPOCHS   = 3\n",
    "CHUNK_DAYS   = 30\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\")\n",
    "\n",
    "# 目录准备\n",
    "ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "print(\"[config] ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0575179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2) 读取原始面板 & 建全局 time_idx ==========\n",
    "#data_paths = fs.glob(f\"{PANEL_DIR_AZ}/*.parquet\")\n",
    "#data_paths = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/clean_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "lf_data = lf_data.filter(pl.col(G_DATE).is_between(1610, 1615, closed=\"both\"))\n",
    "\n",
    "lf_grid = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "grid_path_local = P(\"local\", \"tft/panel/grid_timeidx.parquet\")\n",
    "Path(grid_path_local).parent.mkdir(parents=True, exist_ok=True)\n",
    "lf_grid.collect(streaming=True).write_parquet(grid_path_local, compression=\"zstd\")\n",
    "grid_lazy = pl.scan_parquet(grid_path_local)\n",
    "\n",
    "NEED_COLS = list(dict.fromkeys([G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL] + TIME_FEATURES + RAW_FEATURES))\n",
    "lf0 = (\n",
    "    lf_data.join(grid_lazy, on=[G_DATE, G_TIME], how=\"left\")\n",
    "        .select(NEED_COLS + [\"time_idx\"])\n",
    "        .sort([G_DATE, G_TIME, G_SYM])\n",
    ")\n",
    "print(f\"[{_now()}] lazyframe ready\")\n",
    "\n",
    "# ========== 3) CV 划分 ==========\n",
    "all_days = (\n",
    "    lf0.select(pl.col(G_DATE)).unique().sort(by=G_DATE)\n",
    "    .collect(streaming=True).get_column(G_DATE).to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"[stats] upper bound day for z-score = {stats_hi}\")\n",
    "\n",
    "# ========== 4) 连续特征清洗 + Z-score ==========\n",
    "inf2null_exprs  = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c) for c in RAW_FEATURES]\n",
    "isna_flag_exprs = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\") for c in RAW_FEATURES]\n",
    "ffill_exprs     = [pl.col(c).forward_fill().over(G_SYM).fill_null(0.0).alias(c) for c in RAW_FEATURES]\n",
    "\n",
    "lf_clean = (\n",
    "    lf0.with_columns(inf2null_exprs)\n",
    "    .with_columns(isna_flag_exprs)\n",
    "    .with_columns(ffill_exprs)\n",
    ")\n",
    "\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .group_by(G_SYM)\n",
    "            .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in RAW_FEATURES] +\n",
    "                [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in RAW_FEATURES])\n",
    ")\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in RAW_FEATURES] +\n",
    "                    [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in RAW_FEATURES])\n",
    ")\n",
    "\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\").join(lf_stats_sym, on=G_SYM, how=\"left\")\n",
    "\n",
    "eps = 1e-6\n",
    "Z_COLS, NAMARK_COLS = [], [f\"{c}__isna\" for c in RAW_FEATURES]\n",
    "for c in RAW_FEATURES:\n",
    "    mu_sym, std_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_glb, std_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    mu_use, std_use = f\"{c}_mu_use\", f\"{c}_std_use\"\n",
    "    z_name = f\"{c}_z\"\n",
    "\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_sym).is_null()).then(pl.col(mu_glb)).otherwise(pl.col(mu_sym)).alias(mu_use),\n",
    "        pl.when(pl.col(std_sym).is_null() | (pl.col(std_sym) == 0)).then(pl.col(std_glb)).otherwise(pl.col(std_sym)).alias(std_use),\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(mu_use)) / (pl.col(std_use) + eps)).alias(z_name)\n",
    "    ).drop([mu_glb, std_glb, mu_sym, std_sym, mu_use, std_use])\n",
    "\n",
    "    Z_COLS.append(z_name)\n",
    "\n",
    "OUT_COLS = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL] + TIME_FEATURES + Z_COLS #+ NAMARK_COLS\n",
    "lf_out = lf_z.select(OUT_COLS).sort([G_DATE, G_TIME, G_SYM])\n",
    "\n",
    "# 保存lf_out到本地\n",
    "clean_path_local = P(\"local\", \"tft/panel/cleaned.parquet\")\n",
    "Path(clean_path_local).parent.mkdir(parents=True, exist_ok=True)\n",
    "lf_out.collect(streaming=True).write_parquet(clean_path_local, compression=\"zstd\")\n",
    "print(f\"[{_now()}] cleaned data saved to {clean_path_local}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n",
    "UNKNOWN_REALS =  [TARGET_COL]\n",
    "KNOWN_REALS = TIME_FEATURES + Z_COLS\n",
    "\n",
    "#  对target col 和 unknown_reals去重\n",
    "TRAIN_COLS = list(dict.fromkeys([G_SYM, \"time_idx\", WEIGHT_COL, TARGET_COL] + UNKNOWN_REALS + KNOWN_REALS))\n",
    "t_data = pdf_data[TRAIN_COLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNSCALE_COLS = UNKNOWN_REALS + KNOWN_REALS\n",
    "UNSCALE_COLS =  [c for c in UNSCALE_COLS if c != TARGET_COL]\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_unknown_reals=UNKNOWN_REALS,\n",
    "    \n",
    "    lags=None,\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=False,\n",
    "    add_target_scales=False,\n",
    "    add_encoder_length=False,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=TorchNormalizer(method=\"identity\"),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ds.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction >= val_start_idx) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x, y  = next(iter(train_loader))\n",
    "print(\"x =\", x)\n",
    "print(\"\\ny=\", y)\n",
    "\n",
    "print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8) 训练（按 CV 折） ==========\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "    print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "        f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "    \n",
    "    # 明确日期：\n",
    "    train_start_date = int(train_days[0])\n",
    "    train_end_date   = int(train_days[-1])\n",
    "    val_start_date   = int(val_days[0])\n",
    "    val_end_date     = int(val_days[-1])      \n",
    "    \n",
    "    # 提取数据\n",
    "    date_range = (train_start_date, val_end_date)\n",
    "    pdf_data = (\n",
    "        lf_out\n",
    "        .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "        .collect(streaming=True)\n",
    "        .to_pandas()\n",
    "    )\n",
    "    \n",
    "    pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "    pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)        \n",
    "    # 明确 indexes:\n",
    "    train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "    val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "    val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "    assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "    train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "    print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "    \n",
    "    pdf_data = pdf_data[TRAIN_COLS].copy()\n",
    "\n",
    "    # 构建训练集 timeseries dataset\n",
    "    identity_scalers = {name: None for name in (UNKNOWN_REALS)}\n",
    "    \n",
    "    base_ds = TimeSeriesDataSet(\n",
    "        pdf_data,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=TARGET_COL,\n",
    "        group_ids=[G_SYM],\n",
    "        weight=WEIGHT_COL,\n",
    "        max_encoder_length=ENC_LEN, \n",
    "        min_encoder_length=ENC_LEN,\n",
    "        max_prediction_length=PRED_LEN, \n",
    "        min_prediction_length=PRED_LEN,\n",
    "        \n",
    "        static_categoricals=[G_SYM],\n",
    "        \n",
    "        time_varying_unknown_reals=UNKNOWN_REALS,\n",
    "        \n",
    "        lags=None,\n",
    "        categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True)},\n",
    "        add_relative_time_idx=False,\n",
    "        add_target_scales=False,\n",
    "        add_encoder_length=False,\n",
    "        allow_missing_timesteps=True,\n",
    "        target_normalizer=None,\n",
    "        scalers=identity_scalers,\n",
    "    )\n",
    "    \n",
    "    # 划分训练集，验证集\n",
    "    train_ds = base_ds.filter(\n",
    "        lambda idx: (\n",
    "            idx.time_idx_last <= train_end_idx\n",
    "        ),\n",
    "        copy=True\n",
    "    )\n",
    "    \n",
    "    val_ds = base_ds.filter(\n",
    "        lambda idx: (\n",
    "            (idx.time_idx_first_prediction >= val_start_idx) &\n",
    "            \n",
    "            (idx.time_idx_last <= val_end_idx)\n",
    "        ),\n",
    "        copy=True\n",
    "    )\n",
    "    \n",
    "    # 数据集加载\n",
    "    \n",
    "    train_loader = train_ds.to_dataloader(\n",
    "        train=True, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "    \n",
    "    n_train_batches = len(train_loader)\n",
    "    print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "    assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "    \n",
    "    val_loader = val_ds.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    n_val_batches = len(val_loader)\n",
    "    print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "    assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "    \n",
    "    # calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "    baseline_predictions = Baseline().predict(val_loader, return_y=True)\n",
    "    rmse = RMSE()(baseline_predictions.output, baseline_predictions.y)\n",
    "    print(f\"rmse: {rmse}\")\n",
    "    \n",
    "    lp.seed_everything(42) \n",
    "\n",
    "    # 8.6 callbacks/logger/trainer\n",
    "    ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "    ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            mode=\"min\", \n",
    "            patience=3, \n",
    "            check_on_train_epoch_end=False\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            dirpath=ckpt_dir_fold.as_posix(),\n",
    "            filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\",\n",
    "            save_on_train_epoch_end=False\n",
    "        )\n",
    "        \n",
    "    ]\n",
    "    RUN_NAME = (\n",
    "        f\"f{fold_id}\"\n",
    "        f\"_E{MAX_EPOCHS}\"\n",
    "        f\"_lr{LR}\"\n",
    "        f\"_bs{BATCH_SIZE}\"\n",
    "        f\"_enc{ENC_LEN}_dec{DEC_LEN}\"\n",
    "        f\"_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    )\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=LOGS_DIR.as_posix(),\n",
    "        name=\"tft\",\n",
    "        version=RUN_NAME,\n",
    "        default_hp_metric=False\n",
    "    )\n",
    "\n",
    "    trainer = lp.Trainer(\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.1,\n",
    "        #limit_train_batches=200,  # coment in for training, running valiation every 30 batches\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        train_ds,\n",
    "        learning_rate=LR,\n",
    "        hidden_size=HIDDEN,\n",
    "        attention_head_size=HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        hidden_continuous_size=HIDDEN // 2,\n",
    "        loss=RMSE(),\n",
    "        log_interval=200,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        optimizer=torch.optim.Adam,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader,\n",
    "        )\n",
    "    \n",
    "    ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "    best_path = ckpt_cb.best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "    predictions = best_tft.predict(\n",
    "        val_loader,\n",
    "        return_y=True,\n",
    "        trainer_kwargs=dict(accelerator=\"gpu\")\n",
    "    )\n",
    "    rmse = RMSE()(predictions.output, predictions.y,)\n",
    "    print(f\"rmse after training: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
