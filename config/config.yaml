# =========================
# Jane Street â€“ Time Series Project Configuration
# =========================
project:
  name: "janestreet"
schema_version: "v1"

# -----------------------------
# Time-series task definition
# -----------------------------
task:
  target: "responder_6"          # duplicate on purpose for clarity; can be synced with columns.target
  weight: "weight"
  horizons: 1                  # predict 1 step ahead; extend as needed e.g., [6, 12]
  loss: "wr2"                     # l2 | l1 | huber | quantile

# -----------------------------
# Core columns & target
# -----------------------------
columns:
  keys: ["symbol_id", "date_id", "time_id"]
  target: "responder_6"
  weight: "weight"
  allowlist: ["time_pos", "time_sin", "time_cos", "time_bucket"]

sorts:
  time_major: ["date_id", "time_id", "symbol_id"]
  symbol_major: ["symbol_id", "date_id", "time_id"]


seed: 42


# -----------------------------
# I/O & storage
# -----------------------------
azure: 
  root: "jackson/js_exp" # Raw data is saved here. For big intermediate artifacts-- preprocessed data, cleaned shards, feature-engineered shards, etc.
local:
  root: "/mnt/data/js/v1" # for small data: well preprocessed data, models, and reports cache

# -----------------------------
# Paths (relative roots & concrete artifacts)
# -----------------------------
paths:
  raw_partitions:      { store: "azure", rel: "raw/train.parquet" }
  clean_shards:        { store: "azure", rel: "clean_shards" } 
  fe_shards:           { store: "azure", rel: "fe_shards" }
  panel_shards:        { store: "azure", rel: "panel_shards" }
  lgbm:                { store: "local", rel: "lgbm" } 
  fs_mm:               { store: "local", rel: "fs_mm" }
  train_mm:            { store: "local", rel: "train_mm" }
  models:              { store: "local", rel: "models" }
  reports:             { store: "local", rel: "reports" }
  cache:               { store: "local", rel: "cache" }
  tmp:                 { store: "local", rel: "tmp" }
  tft:                 { store: "local", rel: "tft" }
  

# derived absolute paths (keep your original absolute defaults; can be templated)
derived_paths:
  feature_list_path: "/mnt/data/js/v1/reports/features__fs__1400-1698__cv1-g7-r4__seed42__top1000__1761771286.txt" # get it from the report after feature selection
  train_mm: "/mnt/data/js/v1/train_mm/full_train__features__features__fs__1400-1698__cv1-g7-r4__seed42__top1000__1761771286__range830-1698" # get it from the output of memmap_after_fs step

# optionally keep the selected features csv path (original)
lgbm_selected_features_path: "/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1400-1698__cv2-g7-r4__seed42__top1000__1760906660__range830-1698__range830-1698__cv2-g7-r4__1760912739.csv"

# -----------------------------
# Date ranges (by logical purpose)
# -----------------------------
dates:
  clean:  { date_lo: 800,  date_hi: 1698 }
  fe:     { date_lo: 830,  date_hi: 1698 }
  panel:  { date_lo: 830,  date_hi: 1698 }
  mfs:    { date_lo: 1400, date_hi: 1698 }
  fs:     { date_lo: 1400, date_hi: 1698 }
  mtrain: { date_lo: 830,  date_hi: 1698 }
  train:   { date_lo: 1660,  date_hi: 1698 }

# -----------------------------
# Cross-validation (non-backtest simple CV)
# -----------------------------
cv:
  n_splits: 2
  gap_days: 7
  train_to_val: 4



# -----------------------------
# Metrics & aggregation
# -----------------------------
metrics:
  point: ["mae", "rmse", "mape", "smape"]
  rank:  ["spearman_ic", "pearson_ic"]
  classification_like:
    enabled: true
    thresholds: [0.0]
  aggregate:
    method: "mean"               # mean | weighted | median
    weight_col: "weight"
  report:
    by_symbol: true
    by_time_bucket: true
    topk_long_short: [20]

# -----------------------------
# Trading-time parameters
# -----------------------------
trading:
  daily_ticks: 968
  bucket_size: 6


# -----------------------------
# Winsorization / outlier handling
# -----------------------------
winsorization:
  method: "zscore"
  groupby: ["symbol_id", "time_bucket"]
  z_k: 3
  ddof: 1
  window: 480
  min_valid: 10
  cast_float32: true
  sanitize: true


# -----------------------------
# Missing value filling
# -----------------------------
fill:
  open_tick_window: [0, 10]
  ttl_days_open: 7
  intra_ffill_max_gap_ticks: 200
  ttl_days_same_tick: 7
  order:
    - "open_ttl"
    - "intraday_ffill"
    - "same_tick_ttl"
    - "intraday_ffill"
  batch_size: 30


# -----------------------------
# Feature engineering
# -----------------------------
feature_eng:
  A:
    enabled: true
    is_sorted: false
    cast_f32: true
    tail_lags:  [1, 10, 50, 100, 500, 900, 950, 967]
    tail_diffs: [1, 5, 10, 30]
    rolling_windows: [3, 7, 14, 30]

  B:
    enabled: true
    is_sorted: false
    cast_f32: true
    lags: [1, 2, 3, 4, 5, 6, 7, 14, 21, 30]
    stats_rep_cols: null
    add_prev1_multirep: true
    batch_size: 5

  C:
    enabled: true
    is_sorted: false
    cast_f32: true
    batch_size: 10
    lags: [1, 2, 10, 50, 100, 500, 900, 950, 968, 1936, 2904, 3872, 4840, 5808, 6776, 7744]
    ret_periods:  [3, 10, 50]
    diff_periods: [3, 10, 50]
    rz_windows:   [3, 7, 14, 30]
    ewm_spans:    [5, 10, 50]
    keep_rmean_rstd: true
    cs_cols: ["feature_06", "feature_59", "feature_04", "feature_76", "feature_75", "feature_60", "feature_26"]
  fe_pad_days: 30
  fe_core_days: 30




# -----------------------------
# Feature I/O (selection, importances)
# -----------------------------
features_io:
  selected_features_in:  "/mnt/data/js/exp/v1/models/tune/selected_features.csv"   # optional
  selected_features_out: "/mnt/data/js/exp/v1/models/tune/selected_features__{naming.run_tag}.csv"
  export_importance: true

# -----------------------------
# Model registry (pluggable)
# -----------------------------
models:

  lgbm_select:
    params:
      device_type: "cpu"
      learning_rate: 0.05
      num_leaves: 63
      max_depth: 8
      feature_fraction: 0.80
      bagging_fraction: 0.80
      bagging_freq: 1
      min_data_in_leaf: 200
      num_boost_round: 2000
      early_stopping_rounds: 100
      log_period: 100
      select_top_k: 1000

  lgbm_train:
    params:
      device_type: "cpu"

      # ds_params:
      max_bin : 255
      min_data_in_bin: 5

      # structural params
      num_boost_round: 3000
      early_stopping_rounds: 300
      log_period: 100

      # training params
      learning_rate: 0.01
      num_leaves: 128
      max_depth: 10
      feature_fraction: 0.80
      bagging_fraction: 0.80
      bagging_freq: 3
      min_data_in_leaf: 5000
      min_sum_hessian_in_leaf: 5.0
      min_gain_to_split: 0.0
      lambda_l2: 10
      lambda_l1: 0.1


  lgbm_best:
    params:

  tft:
    params:


# -----------------------------
# Resources
# -----------------------------
resources:
  cpu_workers: 8
  gpu: true
