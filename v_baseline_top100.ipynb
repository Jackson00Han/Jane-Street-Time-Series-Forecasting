{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed813b1",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2cac4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-26 11:53:32] imports ok\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, MAPE, SMAPE, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "from pipeline.stream_input_local import ShardedBatchStream  \n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a03bfe",
   "metadata": {},
   "source": [
    "# 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856ce109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a432b",
   "metadata": {},
   "source": [
    "# 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4040fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] ready\n"
     ]
    }
   ],
   "source": [
    "start_date, end_date = (1000, 1600)  # 仅用于本次实验\n",
    "\n",
    "# 读入筛选的所有特征列\n",
    "\n",
    "path = Path(\"/mnt/data/js/exp/v1/models/tune/selected_covariant_features.txt\")\n",
    "filted_cols = path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "selected_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1400-1698__cv2-g7-r4__seed42__top1000__1760906660__range830-1698__range830-1698__cv2-g7-r4__1760912739.csv\")\n",
    "\n",
    "df_cov_cols = selected_features[selected_features['feature'].isin(filted_cols)].copy()\n",
    "\n",
    "# 我们这里重新归一化一下\n",
    "df_cov_cols[\"mean_gain\"] = (df_cov_cols['mean_gain'] / df_cov_cols['mean_gain'].sum()).astype(np.float32)\n",
    "df_e_features = df_cov_cols.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ========== 1) 初始化配置 ==========\n",
    "\n",
    "# 所有列\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "\n",
    "COV_FEATURES = df_e_features['feature'].iloc[:200].tolist() # 可能含有组内常数列\n",
    "\n",
    "STATIC_FEATURES = [c for c in COV_FEATURES if c in [\"feature_09\", \"feature_10\", \"feature_11\"]]\n",
    "\n",
    "CS_RANK_FEATURES = [c for c in COV_FEATURES if c.endswith(\"__csrank\")]\n",
    "\n",
    "CS_R_Z_FEATURES = [c for c in COV_FEATURES if c.endswith(\"__cs_z\") or c.endswith(\"__rz\")]\n",
    "\n",
    "\n",
    "do_z_co_cols = [c for c in COV_FEATURES if c not in STATIC_FEATURES + CS_RANK_FEATURES + CS_R_Z_FEATURES]\n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 2\n",
    "GAP_DAYS     = 5\n",
    "TRAIN_TO_VAL = 6\n",
    "ENC_LEN      = 30\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 256\n",
    "LR           = 1e-3\n",
    "HIDDEN       = 32\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.2\n",
    "MAX_EPOCHS   = 30\n",
    "GCV = 0.1\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "\n",
    "print(\"[config] ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46945dbf",
   "metadata": {},
   "source": [
    "# 导入数据，添加全局time_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917caa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *COV_FEATURES])\n",
    "    .filter(pl.col(G_DATE).is_between(start_date, end_date, closed=\"both\"))\n",
    ")\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])\n",
    "\n",
    "\n",
    "# 先归一化time_pos\n",
    "lf_data = lf_data.with_columns(\n",
    "    (pl.col(\"time_pos\") / pl.lit(968)).cast(pl.Float32).alias(\"time_pos\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a74eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_data.limit(10).collect()  # 触发计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c37451",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "        .collect(streaming=True)\n",
    ")\n",
    "\n",
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "chunk_size = 30\n",
    "for lo in range(start_date, end_date + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, end_date)\n",
    "    print(f\"processing date range: {lo} ~ {hi}\")\n",
    "    \n",
    "    lf_chunk = lf_data.filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    \n",
    "    lf_grid_chunk = (\n",
    "        grid_df.lazy().filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    )\n",
    "    \n",
    "    lf_joined = (\n",
    "        lf_chunk.join(lf_grid_chunk, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "    \n",
    "    out_path = f\"{container_prefix}/panel_clean_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    \n",
    "    lf_joined.sink_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "    \n",
    "del lf_data, lf_chunk, lf_grid_chunk, lf_joined; gc.collect()\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6633f",
   "metadata": {},
   "source": [
    "# 重新导入含有time_idx的数据 & CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec9161",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"\n",
    "data_paths = fs.glob(f\"{container_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths] \n",
    "lf_with_idx = pl.scan_parquet(data_paths, storage_options=storage_options).filter(pl.col(G_DATE).is_between(start_date, end_date, closed=\"both\")).sort([G_SYM, \"time_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63da710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_with_idx.select(pl.col(G_DATE)).unique().sort([G_DATE])\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "\n",
    "print(f\"[cv] total {len(folds_by_day)} folds\")\n",
    "\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "print(folds_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff439b97",
   "metadata": {},
   "source": [
    "# 接下来均已第一折为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e614fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id = 0\n",
    "# 取第一个 fold 的训练集最后一天，作为本折统计 z-score 的上界\n",
    "train_lo, train_hi = folds_by_day[fold_id][0][0], folds_by_day[fold_id][0][-1]\n",
    "val_lo, val_hi = folds_by_day[fold_id][1][0], folds_by_day[fold_id][1][-1]\n",
    "\n",
    "print(f\"统计标准化使用训练集的日期范围 = {train_lo} ~ {train_hi}\")\n",
    "print(f\"验证集日期范围 = {val_lo} ~ {val_hi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0689de",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d547d97",
   "metadata": {},
   "source": [
    "## 填补因为特征工程产生的缺失列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集的各特征量的中位数\n",
    "lf_tr = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_median = (\n",
    "    lf_tr\n",
    "    .group_by([G_SYM, \"time_pos\"])\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_st\") for col in COV_FEATURES\n",
    "    ])\n",
    "    .sort([G_SYM, \"time_pos\"])\n",
    ")\n",
    "\n",
    "glb_median = (\n",
    "    lf_tr\n",
    "    .group_by(\"time_pos\")\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_t\") for col in COV_FEATURES\n",
    "    ])\n",
    "    .sort(\"time_pos\")\n",
    ")\n",
    "\n",
    "# 应用于本折全部数据 trian + val\n",
    "lf_all = lf_with_idx.join(grp_median, on=[G_SYM, \"time_pos\"], how=\"left\").join(glb_median, on=[\"time_pos\"], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "# 逐列用中位数替换缺失值\n",
    "fill_exprs = []\n",
    "\n",
    "for col in COV_FEATURES:\n",
    "    fill_exprs.append(\n",
    "        pl.coalesce([\n",
    "            pl.col(col),\n",
    "            pl.col(f\"{col}_median_st\"),\n",
    "            pl.col(f\"{col}_median_t\")\n",
    "        ]).alias(col)\n",
    "    )\n",
    "\n",
    "lf_all_imputed = lf_all.with_columns(fill_exprs)\n",
    "\n",
    "# 去掉中位数列\n",
    "drop_cols = [f\"{col}_median_st\" for col in COV_FEATURES]\n",
    "drop_cols += [f\"{col}_median_t\" for col in COV_FEATURES]\n",
    "lf_all_imputed = lf_all_imputed.drop(drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f75c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:   \n",
    "    df_null_lf_all_imputed = lf_all_imputed.select(\n",
    "        pl.all().null_count()\n",
    "    ).collect().to_pandas().T.sort_values(by=0, ascending=False)\n",
    "\n",
    "    assert df_null_lf_all_imputed.iloc[0,0] == 0, \"仍有缺失值未填补完成！\"\n",
    "\n",
    "    del lf_tr, grp_median, glb_median, lf_all, df_null_lf_all_imputed; gc.collect()\n",
    "\n",
    "del lf_tr, grp_median, glb_median, lf_all; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89886d48",
   "metadata": {},
   "source": [
    "## 静态特征归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a20f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_tr_imputed = lf_all_imputed.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "static_glb_minmax = (\n",
    "    lf_tr_imputed.select(\n",
    "        *[pl.col(c).min().alias(f\"{c}_min\") for c in STATIC_FEATURES],\n",
    "        *[pl.col(c).max().alias(f\"{c}_max\") for c in STATIC_FEATURES],\n",
    "    ).collect().to_dicts()[0]   # ← 用 to_dicts()[0]\n",
    ")\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "lf_all = lf_all_imputed.with_columns([\n",
    "    (\n",
    "        ((pl.col(c) - pl.lit(static_glb_minmax[f\"{c}_min\"])) /\n",
    "         (pl.lit(static_glb_minmax[f\"{c}_max\"] - static_glb_minmax[f\"{c}_min\"]) + eps))\n",
    "        .clip(0.0, 1.0)\n",
    "    ).cast(pl.Float32).alias(c)   # ← cast/alias 作用于“整个结果”\n",
    "    for c in STATIC_FEATURES\n",
    "])\n",
    "\n",
    "# 不需要 drop（你没创建 *_glb_min/_glb_max 列）\n",
    "del lf_tr_imputed, static_glb_minmax; gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b2784",
   "metadata": {},
   "source": [
    "## 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4093f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Z-score ==========\n",
    "# 计算 训练集 stats\n",
    "\n",
    "lf_tr = lf_all.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "grp_stats = (\n",
    "    lf_tr\n",
    "    .group_by(G_SYM)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_grp_{c}\") for c in do_z_co_cols] +\n",
    "        [pl.col(c).std().alias(f\"std_grp_{c}\") for c in do_z_co_cols])\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_stats = (\n",
    "    lf_tr\n",
    "    .select([pl.col(c).mean().alias(f\"mu_glb_{c}\") for c in do_z_co_cols] +\n",
    "            [pl.col(c).std().alias(f\"std_glb_{c}\") for c in do_z_co_cols])\n",
    ").collect(streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb73a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "glb_row = glb_stats.to_dicts()[0]\n",
    "\n",
    "# 逐日标准化本折所有数据并保存\n",
    "fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(fold_root)\n",
    "z_prefix = f\"{fold_root}/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "z_done_cols = [f\"z_{c}\" for c in (do_z_co_cols)]\n",
    "min_std = 1e-5\n",
    "eps = 1e-8\n",
    "\n",
    "# 预处理全局均值/方差的兜底，避免在行级判断\n",
    "glb_mu = {c: glb_row[f\"mu_glb_{c}\"] for c in do_z_co_cols}\n",
    "glb_std = {}\n",
    "for c in do_z_co_cols:\n",
    "    s = glb_row[f\"std_glb_{c}\"]\n",
    "    if s is None or s <= 0:\n",
    "        s = min_std\n",
    "    glb_std[c] = s\n",
    "\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    lf_day = lf_all.filter(pl.col(G_DATE) == d)\n",
    "\n",
    "    lf_day_z = lf_day.join(grp_stats.lazy(), on=G_SYM, how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "    # 1) 先把所有“常量”变成具名列，避免 anonymous literal\n",
    "    const_exprs = []\n",
    "    for c in do_z_co_cols:\n",
    "        const_exprs += [\n",
    "            pl.lit(glb_mu[c]).alias(f\"__mu_glb_{c}\"),\n",
    "            pl.lit(glb_std[c] if glb_std[c] > 0 else min_std).alias(f\"__std_glb_{c}\"),\n",
    "        ]\n",
    "    lf_day_z = lf_day_z.with_columns(const_exprs)\n",
    "\n",
    "    # 2) 计算 z，并用 clip 裁剪\n",
    "    exprs = []\n",
    "    for c in do_z_co_cols:\n",
    "        mu_grp  = pl.col(f\"mu_grp_{c}\")\n",
    "        std_grp = pl.col(f\"std_grp_{c}\")\n",
    "        mu_use = pl.coalesce([mu_grp, pl.col(f\"__mu_glb_{c}\")]).cast(pl.Float32)\n",
    "\n",
    "        std_tmp = pl.when(std_grp.is_null() | (std_grp <= 0))\\\n",
    "                    .then(pl.col(f\"__std_glb_{c}\"))\\\n",
    "                    .otherwise(std_grp)\n",
    "        std_use = pl.when(std_tmp < min_std).then(min_std).otherwise(std_tmp).cast(pl.Float32)\n",
    "\n",
    "        z = ((pl.col(c).cast(pl.Float32) - mu_use) / (std_use + eps)).clip(-3.0, 3.0).alias(f\"z_{c}\")\n",
    "        exprs.append(z)\n",
    "\n",
    "    lf_day_z = lf_day_z.with_columns(exprs)\n",
    "    \n",
    "    keep = [\n",
    "        \"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL\n",
    "    ] + TIME_FEATURES + STATIC_FEATURES + CS_RANK_FEATURES + CS_R_Z_FEATURES + z_done_cols\n",
    "        \n",
    "\n",
    "    # 去重但保持顺序（避免 DuplicateError）\n",
    "    keep = list(dict.fromkeys(keep))\n",
    "    \n",
    "    lf_out = lf_day_z.select(keep).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "    out_path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    lf_out.collect(streaming=True).write_parquet(\n",
    "        out_path, storage_options=storage_options, compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"wrote z-scored data for day {d} to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ee42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del lf_tr, grp_stats, glb_stats; gc.collect()\n",
    "del lf_day_z, lf_all; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b3677",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd251d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "z_done_cols = [f\"z_{c}\" for c in do_z_co_cols]\n",
    "\n",
    "# 明确输入的各列\n",
    "known_varying_categorical_cols = [\"time_bucket\", \"gap_flag\"] # gap_flag 需要后续添加\n",
    "\n",
    "known_varying_reals_cols = [\"time_pos\", \"time_sin\", \"time_cos\"] + STATIC_FEATURES + CS_RANK_FEATURES + CS_R_Z_FEATURES + z_done_cols\n",
    "unscaler_cols = known_varying_reals_cols\n",
    "\n",
    "TRAIN_COLS = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL] + known_varying_categorical_cols + known_varying_reals_cols \n",
    "\n",
    "print(f\"weight col: {WEIGHT_COL}\")\n",
    "print(f\"target cols: {TARGET_COL}\")\n",
    "print(f\"encode length: {ENC_LEN}, pred length: {PRED_LEN}\")\n",
    "print(f\"input varying_reals_cols: {known_varying_reals_cols}\")\n",
    "print(f\"input varying_categorical_cols: {known_varying_categorical_cols}\")\n",
    "\n",
    "\n",
    "# 导入 本折z-score 后的数据进行后续处理\n",
    "fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"\n",
    "z_prefix = f\"{fold_root}/z_shards\"\n",
    "\n",
    "data_paths = fs.glob(f\"{z_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths]\n",
    "lf = pl.scan_parquet(data_paths, storage_options=storage_options).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "# 添加 gap_flag 列\n",
    "# 只保留 symbol×date 唯一组合并排序\n",
    "lf_grp_date = (\n",
    "    lf.select([pl.col(G_SYM), pl.col(G_DATE)])\n",
    "      .unique()\n",
    "      .sort([G_SYM, G_DATE])\n",
    ")\n",
    "\n",
    "lf_gap = lf_grp_date.with_columns(\n",
    "    pl.col(G_DATE).diff().over(G_SYM).fill_null(1).alias(\"date_diff\")\n",
    ").with_columns(\n",
    "    (pl.col(\"date_diff\") > 1).cast(pl.Int8).alias(\"gap_flag\")\n",
    ").select(\n",
    "    G_SYM, G_DATE, \"gap_flag\"\n",
    ").sort([G_SYM, G_DATE])\n",
    "\n",
    "\n",
    "lf = lf.join(lf_gap, on=[G_SYM, G_DATE], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "del lf_grp_date, lf_gap; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04668e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = lf.select(TRAIN_COLS)\n",
    "lf = lf.filter(pl.col(G_SYM)==0).sort([G_SYM, \"time_idx\"])  # 先只取一个symbol训练\n",
    "df = lf.collect(streaming=True).to_pandas()\n",
    "\n",
    "df[G_SYM] = df[G_SYM].astype(str).astype(\"category\")\n",
    "\n",
    "# 确保 pandas dtypes\n",
    "df[\"time_bucket\"] = df[\"time_bucket\"].astype(str).astype(\"category\")\n",
    "df[\"gap_flag\"] = df[\"gap_flag\"].astype(str).astype(\"category\")\n",
    "\n",
    "for c in known_varying_reals_cols:\n",
    "    df[c] = df[c].astype(np.float32)\n",
    "\n",
    "    \n",
    "df[TARGET_COL] = df[TARGET_COL].astype(np.float32)\n",
    "df[WEIGHT_COL] = df[WEIGHT_COL].astype(np.float32)\n",
    "\n",
    "df.sort_values(by=[G_SYM, \"time_idx\"], inplace=True)\n",
    "\n",
    "\n",
    "train_start_idx = df[df[G_DATE] == train_lo][\"time_idx\"].min()\n",
    "train_end_idx   = df[df[G_DATE] == train_hi][\"time_idx\"].max()\n",
    "val_start_idx   = df[df[G_DATE] == val_lo][\"time_idx\"].min()\n",
    "val_end_idx     = df[df[G_DATE] == val_hi][\"time_idx\"].max()\n",
    "\n",
    "df_train = df.loc[df.time_idx.between(train_start_idx, train_end_idx)].copy()\n",
    "\n",
    "warmup_len = ENC_LEN\n",
    "df_val_ctx   = df.loc[df.time_idx.between(val_start_idx-warmup_len, val_end_idx)].copy()\n",
    "\n",
    "df_train[\"time_idx\"] = df_train[\"time_idx\"].astype(\"int64\")\n",
    "df_val_ctx[\"time_idx\"]   = df_val_ctx[\"time_idx\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15297c",
   "metadata": {},
   "source": [
    "## 核查数据是否适合训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"总行数:\", len(df))\n",
    "print(\"训练/验证行数:\", len(df_train), len(df_val_ctx))\n",
    "print(\"分组数:\", df[G_SYM].nunique())\n",
    "print(\"训练集分组数:\", df_train[G_SYM].nunique())\n",
    "print(\"验证集分组数:\", df_val_ctx[G_SYM].nunique())\n",
    "print(\"日期范围:\", df[G_DATE].min(), \"→\", df[G_DATE].max())\n",
    "\n",
    "# 关键键是否唯一（防重复）\n",
    "dup_keys = df.duplicated(subset=[G_SYM, \"time_idx\"], keep=False).sum()\n",
    "print(\"重复 (sym, time_idx) 行数:\", dup_keys)\n",
    "assert dup_keys == 0, \"发现 (sym, time_idx) 重复，请先去重！\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标/权重合法性\n",
    "print(\"目标缺失/非有限:\", (~np.isfinite(df[TARGET_COL])).sum())\n",
    "print(\"权重缺失/非有限:\", (~np.isfinite(df[WEIGHT_COL])).sum())\n",
    "print(\"权重负值行数:\", (df[WEIGHT_COL] < 0).sum())\n",
    "\n",
    "# 特征缺失\n",
    "na_counts = df[known_varying_reals_cols].isna().sum().sort_values(ascending=False)\n",
    "print(\"Top-10 特征缺失：\")\n",
    "print(na_counts.head(10))\n",
    "\n",
    "# 常数列（无方差，喂模型没意义）\n",
    "const_cols = []\n",
    "for c in known_varying_reals_cols:\n",
    "    s = df[c]\n",
    "    if s.notna().any() and np.nanstd(s.values.astype(np.float32)) == 0.0:\n",
    "        const_cols.append(c)\n",
    "print(\"常数列数量:\", len(const_cols), \"示例:\", const_cols[:10])\n",
    "\n",
    "# 极端值（Q0.1%~99.9%之外的占比）\n",
    "def extreme_ratio(s):\n",
    "    ql, qh = np.nanpercentile(s.values, [0.1, 99.9])\n",
    "    return float(((s < ql) | (s > qh)).mean())\n",
    "extreme_report = {c: extreme_ratio(df[c]) for c in known_varying_reals_cols[:50]}  # 先抽50列看\n",
    "print(\"极端值占比（前50列示例）:\", sorted(extreme_report.items(), key=lambda x: -x[1])[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个 symbol 的 gap 比例 & 最大连续缺口\n",
    "def gap_stats(g):\n",
    "    gaps = g[\"gap_flag\"].astype(bool).to_numpy()\n",
    "    gap_ratio = gaps.mean()\n",
    "    # 估算最大连续 gap 段长度\n",
    "    # 若 time_idx 连续递增，连续段可用 diff>1 的 run-length 来近似\n",
    "    d = g[\"time_idx\"].diff().fillna(1).to_numpy()\n",
    "    max_gap = int(d[d>1].max()) if (d>1).any() else 1\n",
    "    return pd.Series({\"gap_ratio\": gap_ratio, \"max_gap\": max_gap})\n",
    "\n",
    "gs = df.groupby(G_SYM, observed=True).apply(gap_stats)\n",
    "print(gs.describe())\n",
    "\n",
    "# 验证集首个 time_idx 是否有足够 warmup\n",
    "first_val = df_val_ctx.groupby(G_SYM, observed=True)[\"time_idx\"].min()\n",
    "need_warmup = (first_val < (val_start_idx - ENC_LEN)).sum()\n",
    "print(\"有 warmup 的分组数:\", (first_val <= val_start_idx).sum(), \"不足 warmup 的分组数:\", need_warmup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_end_idx < val_start_idx, \"训练/验证时间有重叠或倒序，请检查 train_hi/val_lo！\"\n",
    "\n",
    "# 各分组在训练/验证的覆盖情况\n",
    "cov = pd.DataFrame({\n",
    "    \"train_rows\": df_train.groupby(G_SYM, observed=True).size(),\n",
    "    \"val_rows\":   df_val_ctx.groupby(G_SYM, observed=True).size(),\n",
    "}).fillna(0).astype(int)\n",
    "print(\"训练/验证覆盖（前10）：\")\n",
    "print(cov.head(10))\n",
    "bad_groups = cov.query(\"train_rows < @ENC_LEN or val_rows < 1\").index.tolist()\n",
    "print(\"潜在不可训练分组数:\", len(bad_groups), \"示例:\", bad_groups[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"目标分布:\", df[TARGET_COL].describe(percentiles=[.01,.1,.5,.9,.99]))\n",
    "print(\"权重分布:\", df[WEIGHT_COL].describe(percentiles=[.01,.1,.5,.9,.99]))\n",
    "\n",
    "# 权重为0的比例（有的赛题权重0表示忽略，需明确）\n",
    "w0_ratio = (df[WEIGHT_COL] == 0).mean()\n",
    "print(\"权重=0 占比:\", round(float(w0_ratio), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取最近一段的抽样，避免大表过慢\n",
    "sample = df.sample(min(500_000, len(df)), random_state=42)\n",
    "\n",
    "# 简易皮尔逊相关（注意：时序相关不等于可用性，这只是红/绿灯）\n",
    "corrs = sample[known_varying_reals_cols + [TARGET_COL]].corr(numeric_only=True)[TARGET_COL].sort_values(ascending=False)\n",
    "print(\"与目标最相关 Top-15：\")\n",
    "print(corrs.head(15))\n",
    "print(\"与目标最负相关 Top-15：\")\n",
    "print(corrs.tail(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b806bd",
   "metadata": {},
   "source": [
    "## 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TimeSeriesDataSet(\n",
    "    df_train,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    min_encoder_length=ENC_LEN // 2,\n",
    "    max_encoder_length=ENC_LEN,\n",
    "    min_prediction_length=PRED_LEN,\n",
    "    max_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    \n",
    "    time_varying_unknown_categoricals = [],\n",
    "    time_varying_unknown_reals = [],\n",
    "\n",
    "    time_varying_known_categoricals=known_varying_categorical_cols,\n",
    "    time_varying_known_reals=known_varying_reals_cols,\n",
    "    \n",
    "    target_normalizer=GroupNormalizer(groups=[G_SYM], method=\"standard\"),\n",
    "    \n",
    "    allow_missing_timesteps=True,\n",
    "    \n",
    "    scalers= {name: None for name in (unscaler_cols)}\n",
    ")\n",
    "\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    df_val_ctx,\n",
    "    min_prediction_idx=val_start_idx,\n",
    "    predict=False,\n",
    "    stop_randomization=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42)\n",
    "# 配置：每个 epoch 想训练多少个“样本/窗口”\n",
    "# 任选其一：按比例或固定数量\n",
    "SAMPLE_FRACTION = 0.5       # 例：每个 epoch 只随机抽取 20% 的窗口\n",
    "# SAMPLES_PER_EPOCH = 10000  # 或者：固定每个 epoch 抽 1e4 个窗口\n",
    "\n",
    "# 计算 num_samples（即本 epoch 的样本数）\n",
    "_train_total = len(train_ds); _val_total = len(val_ds)\n",
    "num_samples = max(1, int(_train_total * SAMPLE_FRACTION))\n",
    "# 如果用固定数量就改为：\n",
    "# num_samples = min(_train_total, SAMPLES_PER_EPOCH)\n",
    "\n",
    "# 打印epoch 样本数\n",
    "print(f\"train full samples: {_train_total}, val full samples: {_val_total}\")\n",
    "\n",
    "# 关键：带放回的随机采样器（每个 epoch 都会重新抽样）\n",
    "train_sampler = RandomSampler(train_ds, replacement=True, num_samples=num_samples)\n",
    "\n",
    "\n",
    "train_loader = train_ds.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=8, pin_memory=True, persistent_workers=True, prefetch_factor=2, shuffle=True)\n",
    "\n",
    "\n",
    "val_loader   = val_ds.to_dataloader(train=False, batch_size=BATCH_SIZE*8, num_workers=8, pin_memory=True, persistent_workers=True, prefetch_factor=2, shuffle=False)\n",
    "\n",
    "del df, df_train, df_val_ctx; gc.collect()\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"train_loader batches = {n_train_batches}\")\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"val_loader batches = {n_val_batches}\")\n",
    "\n",
    "print(f\"[{_now()}] data loaders ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cf6b4",
   "metadata": {},
   "source": [
    "## 优化学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10edb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "pl.set_random_seed(42)\n",
    "\n",
    "# 1) 运行前清理缓存（可选）\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# 2) 给 LR finder 专用的小 batch loader\n",
    "#    不要动正式的 train_loader/val_loader\n",
    "# 3) 混合精度（优先 bf16，其次 fp16）\n",
    "trainer_lr = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "# 4) 可选：只为 LR finder 构建一个“轻量化”的 TFT（不影响正式模型）\n",
    "tft_lr = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=32,             # 降低一点隐藏维度\n",
    "    attention_head_size=1,       # 降低头数\n",
    "    hidden_continuous_size=16,\n",
    "    dropout=0.2,\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"ranger\",\n",
    ")\n",
    "\n",
    "# 5) 运行 LR finder，限制搜索步数\n",
    "res = Tuner(trainer_lr).lr_find(\n",
    "    tft_lr,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    min_lr=1e-6,\n",
    "    max_lr=1e-1,         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2237c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d5322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a850949c",
   "metadata": {},
   "source": [
    "## 正式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c42bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoneinfo import ZoneInfo   # py>=3.9 自带\n",
    "\n",
    "LOCAL_TZ = ZoneInfo(\"Europe/Copenhagen\")     # 换成你想用的时区，如 \"Asia/Shanghai\"\n",
    "ts = datetime.now(LOCAL_TZ).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN//2,\n",
    "    loss=QuantileLoss(quantiles=[0.5]),                            # 点预测 OK，会自动用 weight                     \n",
    "    optimizer=\"adamw\",\n",
    "    reduce_on_plateau_patience=1,\n",
    "    reduce_on_plateau_min_lr=1e-5\n",
    ")\n",
    "# 打印模型参数\n",
    "print(f\"fold_id = {fold_id}\")\n",
    "print(f\"train_lo = {train_lo}, train_hi = {train_hi}\")\n",
    "print(f\"val_lo = {val_lo}, val_hi = {val_hi}\")\n",
    "print(f\"target_col = {TARGET_COL}\")\n",
    "print(f\"weight_col = {WEIGHT_COL}\")\n",
    "print(f\"encode length = {ENC_LEN}, pred length = {PRED_LEN}\")\n",
    "print(f\"lr = {LR}, batch size = {BATCH_SIZE}\")\n",
    "print(f\"hiden size = {HIDDEN}, heads = {HEADS}, dropout = {DROPOUT}\")\n",
    "print(f\"max epochs = {MAX_EPOCHS}\")\n",
    "print(f\"gradient clip val = {GCV}\")\n",
    "\n",
    "\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_NAME = f\"full_symbol_50perc_adamw_lr{LR:g}_bs{BATCH_SIZE}_enc{ENC_LEN}_hidden{HIDDEN}_heads{HEADS}_time_{ts}\"\n",
    "TEMP_LOG_DIR = Path(\"./tft_logs\"); TEMP_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logger = TensorBoardLogger(save_dir=TEMP_LOG_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3), \n",
    "            LearningRateMonitor()\n",
    "            ] \n",
    "\n",
    "trainer = lp.Trainer(\n",
    "    #fast_dev_run=1,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=GCV,          \n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=callbacks,   # ← 记得把 lr_logger 放进来\n",
    "    logger=logger,\n",
    "    #accumulate_grad_batches=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def patch_tft_mask_bias(model):\n",
    "    hit = 0\n",
    "    for n, m in model.named_modules():\n",
    "        if hasattr(m, \"mask_bias\"):\n",
    "            old = getattr(m, \"mask_bias\")\n",
    "            if isinstance(old, (int, float)) and old < -1e8:\n",
    "                setattr(m, \"mask_bias\", -65504.0)   # 关键：FP16 最小可表示\n",
    "                hit += 1\n",
    "    print(f\"[TFT patch] mask_bias replaced in {hit} module(s).\")\n",
    "    # 诊断输出（确认真的改到了）\n",
    "    for n, m in model.named_modules():\n",
    "        if hasattr(m, \"mask_bias\"):\n",
    "            print(\" -\", n, type(m.mask_bias), m.mask_bias)\n",
    "\n",
    "patch_tft_mask_bias(tft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(tft, train_dataloaders=train_loader, val_dataloaders=val_loader)#####2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cedd4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
