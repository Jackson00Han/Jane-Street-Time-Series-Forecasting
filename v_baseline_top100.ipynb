{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed813b1",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cac4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "from pipeline.stream_input_local import ShardedBatchStream  \n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a03bfe",
   "metadata": {},
   "source": [
    "# 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ce109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a432b",
   "metadata": {},
   "source": [
    "# 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4040fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date = (1400, 1600)  # 仅用于本次实验\n",
    "\n",
    "# 读入筛选的所有特征列\n",
    "\n",
    "path = Path(\"/mnt/data/js/exp/v1/models/tune/selected_covariant_features.txt\")\n",
    "filted_cols = path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "selected_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1400-1698__cv2-g7-r4__seed42__top1000__1760906660__range830-1698__range830-1698__cv2-g7-r4__1760912739.csv\")\n",
    "\n",
    "df_cov_cols = selected_features[selected_features['feature'].isin(filted_cols)].copy()\n",
    "\n",
    "# 我们这里重新归一化一下\n",
    "df_cov_cols[\"mean_gain\"] = (df_cov_cols['mean_gain'] / df_cov_cols['mean_gain'].sum()).astype(np.float32)\n",
    "df_e_features = df_cov_cols.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========== 1) 初始化配置 ==========\n",
    "\n",
    "# 所有列\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "COV_FEATURES = df_e_features['feature'].iloc[:100].tolist() # 可能含有组内常数列\n",
    "\n",
    "STATIC_FEATURES = [c for c in COV_FEATURES if c in [\"feature_09\", \"feature_10\", \"feature_11\"]]\n",
    "\n",
    "CS_RANK_FEATURES = [c for c in COV_FEATURES if c.endswith(\"__csrank\")]\n",
    "\n",
    "CS_R_Z_FEATURES = [c for c in COV_FEATURES if c.endswith(\"__cs_z\") or c.endswith(\"__rz\")]\n",
    "\n",
    "\n",
    "do_z_co_cols = [c for c in COV_FEATURES if c not in STATIC_FEATURES + CS_RANK_FEATURES + CS_R_Z_FEATURES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 0\n",
    "TRAIN_TO_VAL = 8\n",
    "ENC_LEN      = 50\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 1024\n",
    "LR           = 1e-2\n",
    "HIDDEN       = 64\n",
    "HEADS        = 2\n",
    "DROPOUT      = 0.2\n",
    "MAX_EPOCHS   = 5\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "\n",
    "print(\"[config] ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46945dbf",
   "metadata": {},
   "source": [
    "# 导入数据，添加全局time_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917caa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *COV_FEATURES])\n",
    "    .filter(pl.col(G_DATE).is_between(start_date, end_date, closed=\"both\"))\n",
    ")\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先归一化time_pos\n",
    "lf_data = lf_data.with_columns(\n",
    "    (pl.col(\"time_pos\") / pl.lit(968)).cast(pl.Float32).alias(\"time_pos\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c37451",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "        .collect(streaming=True)\n",
    ")\n",
    "\n",
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "chunk_size = 30\n",
    "for lo in range(start_date, end_date + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, end_date)\n",
    "    print(f\"processing date range: {lo} ~ {hi}\")\n",
    "    \n",
    "    lf_chunk = lf_data.filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    \n",
    "    lf_grid_chunk = (\n",
    "        grid_df.lazy().filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    )\n",
    "    \n",
    "    lf_joined = (\n",
    "        lf_chunk.join(lf_grid_chunk, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "    \n",
    "    out_path = f\"{container_prefix}/panel_clean_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    \n",
    "    lf_joined.sink_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6633f",
   "metadata": {},
   "source": [
    "# 重新导入含有time_idx的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"\n",
    "data_paths = fs.glob(f\"{container_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths] \n",
    "lf_with_idx = pl.scan_parquet(data_paths, storage_options=storage_options).filter(pl.col(G_DATE).is_between(start_date, end_date, closed=\"both\")).sort([G_SYM, \"time_idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc3dc1",
   "metadata": {},
   "source": [
    "# CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63da710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_with_idx.select(pl.col(G_DATE)).unique().sort([G_DATE])\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "\n",
    "print(f\"[cv] total {len(folds_by_day)} folds\")\n",
    "\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "print(folds_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff439b97",
   "metadata": {},
   "source": [
    "# 接下来均已第一折为例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0689de",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fold_id = 0\n",
    "# 取第一个 fold 的训练集最后一天，作为本折统计 z-score 的上界\n",
    "train_lo, train_hi = folds_by_day[fold_id][0][0], folds_by_day[fold_id][0][-1]\n",
    "val_lo, val_hi = folds_by_day[fold_id][1][0], folds_by_day[fold_id][1][-1]\n",
    "\n",
    "print(f\"统计标准化使用训练集的日期范围 = {train_lo} ~ {train_hi}\")\n",
    "print(f\"验证集日期范围 = {val_lo} ~ {val_hi}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d547d97",
   "metadata": {},
   "source": [
    "## 填补因为特征工程产生的缺失列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_with_idx.select(pl.len()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_with_idx.select(\n",
    "    pl.all().null_count()\n",
    ").collect().to_pandas().T.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集的各特征量的中位数\n",
    "lf_tr = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_median = (\n",
    "    lf_tr\n",
    "    .group_by([G_SYM, \"time_pos\"])\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_st\") for col in COV_FEATURES\n",
    "    ])\n",
    "    .sort([G_SYM, \"time_pos\"])\n",
    ")\n",
    "\n",
    "glb_median = (\n",
    "    lf_tr\n",
    "    .group_by(\"time_pos\")\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_t\") for col in COV_FEATURES\n",
    "    ])\n",
    "    .sort(\"time_pos\")\n",
    ")\n",
    "\n",
    "\n",
    "# 应用于本折全部数据 trian + val\n",
    "lf_all = lf_with_idx.join(grp_median, on=[G_SYM, \"time_pos\"], how=\"left\").join(glb_median, on=[\"time_pos\"], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "# 逐列用中位数替换缺失值\n",
    "fill_exprs = []\n",
    "\n",
    "for col in COV_FEATURES:\n",
    "    fill_exprs.append(\n",
    "        pl.coalesce([\n",
    "            pl.col(col),\n",
    "            pl.col(f\"{col}_median_st\"),\n",
    "            pl.col(f\"{col}_median_t\")\n",
    "        ]).alias(col)\n",
    "    )\n",
    "\n",
    "lf_all_imputed = lf_all.with_columns(fill_exprs)\n",
    "\n",
    "# 去掉中位数列\n",
    "drop_cols = [f\"{col}_median_st\" for col in COV_FEATURES]\n",
    "drop_cols += [f\"{col}_median_t\" for col in COV_FEATURES]\n",
    "lf_all_imputed = lf_all_imputed.drop(drop_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514397df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_lf_all_imputed = lf_all_imputed.select(\n",
    "    pl.all().null_count()\n",
    ").collect().to_pandas().T.sort_values(by=0, ascending=False)\n",
    "\n",
    "assert df_null_lf_all_imputed.iloc[0,0] == 0, \"仍有缺失值未填补完成！\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b2784",
   "metadata": {},
   "source": [
    "## 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4093f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Z-score ==========\n",
    "# 计算 训练集 stats\n",
    "lf_tr_imputed = lf_all_imputed.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_stats = (\n",
    "    lf_tr_imputed\n",
    "    .group_by(G_SYM)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_grp_{c}\") for c in do_z_co_cols] +\n",
    "        [pl.col(c).std().alias(f\"std_grp_{c}\") for c in do_z_co_cols])\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_stats = (\n",
    "    lf_tr_imputed\n",
    "    .select([pl.col(c).mean().alias(f\"mu_glb_{c}\") for c in do_z_co_cols] +\n",
    "            [pl.col(c).std().alias(f\"std_glb_{c}\") for c in do_z_co_cols])\n",
    ").collect(streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb73a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "glb_row = glb_stats.to_dicts()[0]\n",
    "\n",
    "# 逐日标准化本折所有数据并保存\n",
    "fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(fold_root)\n",
    "z_prefix = f\"{fold_root}/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "z_done_cols = [f\"z_{c}\" for c in (do_z_co_cols)]\n",
    "min_std = 1e-5\n",
    "eps = 1e-8\n",
    "\n",
    "# 预处理全局均值/方差的兜底，避免在行级判断\n",
    "glb_mu = {c: glb_row[f\"mu_glb_{c}\"] for c in do_z_co_cols}\n",
    "glb_std = {}\n",
    "for c in do_z_co_cols:\n",
    "    s = glb_row[f\"std_glb_{c}\"]\n",
    "    if s is None or s <= 0:\n",
    "        s = min_std\n",
    "    glb_std[c] = s\n",
    "\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    lf_day = lf_all_imputed.filter(pl.col(G_DATE) == d)\n",
    "\n",
    "    lf_day_z = lf_day.join(grp_stats.lazy(), on=G_SYM, how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "    # 1) 先把所有“常量”变成具名列，避免 anonymous literal\n",
    "    const_exprs = []\n",
    "    for c in do_z_co_cols:\n",
    "        const_exprs += [\n",
    "            pl.lit(glb_mu[c]).alias(f\"__mu_glb_{c}\"),\n",
    "            pl.lit(glb_std[c] if glb_std[c] > 0 else min_std).alias(f\"__std_glb_{c}\"),\n",
    "        ]\n",
    "    lf_day_z = lf_day_z.with_columns(const_exprs)\n",
    "\n",
    "    # 2) 计算 z，并用 clip 裁剪\n",
    "    exprs = []\n",
    "    for c in do_z_co_cols:\n",
    "        mu_grp  = pl.col(f\"mu_grp_{c}\")\n",
    "        std_grp = pl.col(f\"std_grp_{c}\")\n",
    "        mu_use = pl.coalesce([mu_grp, pl.col(f\"__mu_glb_{c}\")]).cast(pl.Float32)\n",
    "\n",
    "        std_tmp = pl.when(std_grp.is_null() | (std_grp <= 0))\\\n",
    "                    .then(pl.col(f\"__std_glb_{c}\"))\\\n",
    "                    .otherwise(std_grp)\n",
    "        std_use = pl.when(std_tmp < min_std).then(min_std).otherwise(std_tmp).cast(pl.Float32)\n",
    "\n",
    "        z = ((pl.col(c).cast(pl.Float32) - mu_use) / (std_use + eps)).clip(-10.0, 10.0).alias(f\"z_{c}\")\n",
    "        exprs.append(z)\n",
    "\n",
    "    lf_day_z = lf_day_z.with_columns(exprs)\n",
    "    \n",
    "    keep = [\n",
    "        \"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL\n",
    "    ] + TIME_FEATURES + STATIC_FEATURES + CS_RANK_FEATURES + CS_R_Z_FEATURES + z_done_cols\n",
    "        \n",
    "\n",
    "    # 去重但保持顺序（避免 DuplicateError）\n",
    "    keep = list(dict.fromkeys(keep))\n",
    "    \n",
    "    lf_out = lf_day_z.select(keep).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "    out_path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    lf_out.collect(streaming=True).write_parquet(\n",
    "        out_path, storage_options=storage_options, compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"wrote z-scored data for day {d} to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b3677",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04668e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "KNOW_CATEGORICALS = [\"time_bucket\"]\n",
    "\n",
    "STATIC_REALS = STATIC_FEATURES\n",
    "\n",
    "KNOWN_REALS = TIME_FEATURES + z_done_cols + CS_RANK_FEATURES + CS_R_Z_FEATURES\n",
    "\n",
    "TRAIN_COLS = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL] + KNOWN_REALS + STATIC_REALS\n",
    "\n",
    "\n",
    "# 导入 本折z-score 后的数据进行后续处理\n",
    "data_paths = fs.glob(f\"{z_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths]\n",
    "lf = pl.scan_parquet(data_paths, storage_options=storage_options).select(TRAIN_COLS).sort([G_SYM, \"time_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lf.collect(streaming=True).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
