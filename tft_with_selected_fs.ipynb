{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2317e57f",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4586e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-27 12:09:02] imports ok\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "from pipeline.stream_input_local import ShardedBatchStream  \n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c956339",
   "metadata": {},
   "source": [
    "## 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cd586",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c7e58",
   "metadata": {},
   "source": [
    "读入选出的特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64883d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入筛选的所有特征列\n",
    "\n",
    "path = Path(\"/mnt/data/js/exp/v1/models/tune/selected_covariant_features.txt\")\n",
    "filted_cols = path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "selected_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1400-1698__cv2-g7-r4__seed42__top1000__1760906660__range830-1698__range830-1698__cv2-g7-r4__1760912739.csv\")\n",
    "\n",
    "df_cov_cols = selected_features[selected_features['feature'].isin(filted_cols)].copy()\n",
    "\n",
    "# 我们这里重新归一化一下\n",
    "df_cov_cols[\"mean_gain\"] = (df_cov_cols['mean_gain'] / df_cov_cols['mean_gain'].sum()).astype(np.float32)\n",
    "df_e_features = df_cov_cols.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] ready\n"
     ]
    }
   ],
   "source": [
    "# ========== 1) 初始化配置 ==========\n",
    "\n",
    "# 所有列\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "COV_FEATURES = df_e_features['feature'].tolist() # 含有常数列\n",
    "\n",
    "\n",
    "STATIC_FEATURES = [c for c in COV_FEATURES if c in [\"feature_09\", \"feature_10\", \"feature_11\"]]\n",
    "\n",
    "CS_RANK_FEATURES = [c for c in COV_FEATURES if c.endswith(\"__csrank\")]\n",
    "\n",
    "CS_R_Z_FEATURES = [c for c in COV_FEATURES if c.endswith(\"__cs_z\") or c.endswith(\"__rz\")]\n",
    "\n",
    "do_z_co_cols = [c for c in COV_FEATURES if c not in STATIC_FEATURES + CS_RANK_FEATURES + CS_R_Z_FEATURES]\n",
    "\n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 2\n",
    "GAP_DAYS     = 7\n",
    "TRAIN_TO_VAL = 5\n",
    "ENC_LEN      = 128\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 64 # 有效batch_size = 128\n",
    "ACCUM        = 2\n",
    "LR           = 5e-4\n",
    "HIDDEN       = 64\n",
    "HEADS        = 2\n",
    "DROPOUT      = 0.2\n",
    "MAX_EPOCHS   = 40\n",
    "GCV = 0.1\n",
    "precision=\"16-mixed\"\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "\n",
    "start_date, end_date = (1000, 1600)  # \n",
    "s_start_data, s_end_date = 1400, 1600\n",
    "\n",
    "print(\"[config] ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300bbba",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *COV_FEATURES])\n",
    "    .filter(pl.col(G_DATE).is_between(start_date, end_date, closed=\"both\"))\n",
    ")\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1aaf",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d516",
   "metadata": {},
   "source": [
    "### 添加全局时间序列号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "        .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "chunk_size = 30\n",
    "for lo in range(start_date, end_date + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, end_date)\n",
    "    print(f\"processing date range: {lo} ~ {hi}\")\n",
    "    \n",
    "    lf_chunk = lf_data.filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    \n",
    "    lf_grid_chunk = (\n",
    "        grid_df.lazy().filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    )\n",
    "    \n",
    "    lf_joined = (\n",
    "        lf_chunk.join(lf_grid_chunk, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "    \n",
    "    out_path = f\"{container_prefix}/panel_clean_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    \n",
    "    lf_joined.sink_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb09de",
   "metadata": {},
   "source": [
    "## 导入新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11120fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新读入数据\n",
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "data_paths = fs.glob(f\"{container_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths] \n",
    "lf_with_idx = pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "lf_with_idx = lf_with_idx.filter(pl.col(G_DATE).is_between(s_start_data, s_end_date)).sort([G_SYM, G_DATE, G_TIME])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e04edc",
   "metadata": {},
   "source": [
    "## CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a016c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cv] total 2 folds\n",
      "[(array([1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410,\n",
      "       1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421,\n",
      "       1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432,\n",
      "       1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443,\n",
      "       1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454,\n",
      "       1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465,\n",
      "       1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476,\n",
      "       1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487,\n",
      "       1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498,\n",
      "       1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509,\n",
      "       1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520,\n",
      "       1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531,\n",
      "       1532, 1533, 1534], dtype=int32), array([1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552,\n",
      "       1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563,\n",
      "       1564, 1565, 1566, 1567, 1568, 1569], dtype=int32)), (array([1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438,\n",
      "       1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449,\n",
      "       1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460,\n",
      "       1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471,\n",
      "       1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482,\n",
      "       1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493,\n",
      "       1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504,\n",
      "       1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515,\n",
      "       1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526,\n",
      "       1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537,\n",
      "       1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548,\n",
      "       1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559,\n",
      "       1560, 1561, 1562], dtype=int32), array([1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580,\n",
      "       1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591,\n",
      "       1592, 1593, 1594, 1595, 1596, 1597], dtype=int32))]\n"
     ]
    }
   ],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_with_idx.select(pl.col(G_DATE)).unique().sort([G_DATE])\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "\n",
    "print(f\"[cv] total {len(folds_by_day)} folds\")\n",
    "\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "print(folds_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d60364",
   "metadata": {},
   "source": [
    "下面我们以一折为例，验证成功后，再使用多折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a390ff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计标准化使用训练集的日期范围 = 1400 ~ 1534\n"
     ]
    }
   ],
   "source": [
    "fold_id = 0\n",
    "# 取第一个 fold 的训练集最后一天，作为本折统计 z-score 的上界\n",
    "train_lo, train_hi = folds_by_day[fold_id][0][0], folds_by_day[fold_id][0][-1]\n",
    "val_lo, val_hi = folds_by_day[fold_id][1][0], folds_by_day[fold_id][1][-1]\n",
    "\n",
    "print(f\"统计标准化使用训练集的日期范围 = {train_lo} ~ {train_hi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5315",
   "metadata": {},
   "source": [
    "## 数据标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先归一化 time_pos\n",
    "\n",
    "lf_full = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, val_hi, closed=\"both\"))\n",
    "\n",
    "lf_full = lf_full.with_columns(\n",
    "    (pl.col(\"time_pos\") / 968).alias(\"time_pos\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ebdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_tr = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "# ========= 归一化静态特征 ==========\n",
    "static_glb_minmax = (\n",
    "    lf_tr.select(\n",
    "        *[pl.col(c).min().alias(f\"{c}_min\") for c in STATIC_FEATURES],\n",
    "        *[pl.col(c).max().alias(f\"{c}_max\") for c in STATIC_FEATURES],\n",
    "    ).collect().to_dicts()[0]   # ← 用 to_dicts()[0]\n",
    ")\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "lf_full = lf_full.with_columns([\n",
    "    (\n",
    "        ((pl.col(c) - pl.lit(static_glb_minmax[f\"{c}_min\"])) /\n",
    "         (pl.lit(static_glb_minmax[f\"{c}_max\"] - static_glb_minmax[f\"{c}_min\"]) + eps))\n",
    "        .clip(0.0, 1.0)\n",
    "    ).cast(pl.Float32).alias(c)   # ← cast/alias 作用于“整个结果”\n",
    "    for c in STATIC_FEATURES\n",
    "])\n",
    "\n",
    "del static_glb_minmax; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33103e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Z-score ==========\n",
    "# 开始计算 stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_stats = (\n",
    "    lf_tr\n",
    "    .group_by(G_SYM)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_grp_{c}\") for c in do_z_co_cols] +\n",
    "        [pl.col(c).std(ddof=1).alias(f\"std_grp_{c}\") for c in do_z_co_cols])\n",
    ").collect(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41652fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "glb_stats = (\n",
    "    lf_tr\n",
    "    .select([pl.col(c).mean().alias(f\"mu_glb_{c}\") for c in do_z_co_cols] +\n",
    "            [pl.col(c).std(ddof=1).alias(f\"std_glb_{c}\") for c in do_z_co_cols])\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_row = glb_stats.to_dicts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 逐日处理本折所有数据\n",
    "fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(fold_root)\n",
    "z_prefix = f\"{fold_root}/start{s_start_data}_train{s_end_date}_z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "min_std = 1e-5\n",
    "eps = 1e-6\n",
    "# 预处理全局均值/方差的兜底，避免在行级判断\n",
    "glb_mu = {c: glb_row[f\"mu_glb_{c}\"] for c in do_z_co_cols}\n",
    "glb_std = {}\n",
    "for c in do_z_co_cols:\n",
    "    s = glb_row[f\"std_glb_{c}\"]\n",
    "    if s is None or s <= 0:\n",
    "        s = min_std\n",
    "    glb_std[c] = s\n",
    "z_done_cols = [f\"z_{c}\" for c in do_z_co_cols]\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    lf_day = lf_full.filter(pl.col(G_DATE) == d)\n",
    "\n",
    "    lf_day_z = lf_day.join(grp_stats.lazy(), on=G_SYM, how=\"left\")\n",
    "\n",
    "    # 1) 先把所有“常量”变成具名列，避免 anonymous literal\n",
    "    const_exprs = []\n",
    "    for c in do_z_co_cols:\n",
    "        const_exprs += [\n",
    "            pl.lit(glb_mu[c]).alias(f\"__mu_glb_{c}\"),\n",
    "            pl.lit(glb_std[c] if glb_std[c] > 0 else min_std).alias(f\"__std_glb_{c}\"),\n",
    "        ]\n",
    "    lf_day_z = lf_day_z.with_columns(const_exprs)\n",
    "\n",
    "    # 2) 计算 z，并用 clip 裁剪\n",
    "    exprs = []\n",
    "    for c in do_z_co_cols:\n",
    "        mu_grp  = pl.col(f\"mu_grp_{c}\")\n",
    "        std_grp = pl.col(f\"std_grp_{c}\")\n",
    "        mu_use = pl.coalesce([mu_grp, pl.col(f\"__mu_glb_{c}\")]).cast(pl.Float32)\n",
    "\n",
    "        std_tmp = pl.when(std_grp.is_null() | (std_grp <= 0))\\\n",
    "                    .then(pl.col(f\"__std_glb_{c}\"))\\\n",
    "                    .otherwise(std_grp)\n",
    "        std_use = pl.when(std_tmp < min_std).then(min_std).otherwise(std_tmp).cast(pl.Float32)\n",
    "\n",
    "        z = ((pl.col(c).cast(pl.Float32) - mu_use) / (std_use + eps)).clip(-8.0, 8.0).alias(f\"z_{c}\")\n",
    "        exprs.append(z)\n",
    "\n",
    "    lf_day_z = lf_day_z.with_columns(exprs)\n",
    "    \n",
    "    # 标准化后，用0填补缺失值\n",
    "    lf_day_z = lf_day_z.with_columns([\n",
    "        pl.col(c).fill_null(pl.lit(0.0)).cast(pl.Float32) for c in [*z_done_cols, *CS_RANK_FEATURES, *CS_R_Z_FEATURES]\n",
    "    ])\n",
    "    \n",
    "    lf_day_z = lf_day_z.sort([G_SYM, \"time_idx\"]).with_columns([\n",
    "        pl.col(c).forward_fill().over(G_SYM) for c in STATIC_FEATURES\n",
    "    ]).with_columns([\n",
    "        pl.col(c).fill_null(pl.lit(0.0)).cast(pl.Float32) for c in STATIC_FEATURES\n",
    "    ])\n",
    "    \n",
    "    keep = [\n",
    "        \"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *STATIC_FEATURES, *CS_RANK_FEATURES, *CS_R_Z_FEATURES, *z_done_cols\n",
    "    ]\n",
    "    \n",
    "    lf_out = lf_day_z.select(keep).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "    out_path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    lf_out.collect(streaming=True).write_parquet(\n",
    "        out_path, storage_options=storage_options, compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"wrote z-scored data for day {d} to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d388623",
   "metadata": {},
   "source": [
    "### 检查缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查缺失值\n",
    "z_paths = fs.glob(f\"{z_prefix}/*.parquet\")\n",
    "z_paths = [f\"az://{p}\" for p in z_paths]\n",
    "lx = pl.scan_parquet(z_paths, storage_options=storage_options).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "\n",
    "df_null = lx.select(\n",
    "    [pl.col(c).null_count().alias(c) for c in CS_RANK_FEATURES]\n",
    ").collect()\n",
    "\n",
    "df_null = df_null.to_pandas()\n",
    "df_null.rename(columns={0: \"null_count\"}, inplace=True)\n",
    "\n",
    "df_null = df_null.T\n",
    "df_null.rename(columns={0: \"null_count\"}, inplace=True)\n",
    "df_null.sort_values(by=\"null_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52252703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查标准差\n",
    "stds = lx.select(\n",
    "    [pl.col(c).std().alias(c) for c in z_done_cols]\n",
    ").collect(streaming=True)\n",
    "\n",
    "pd_stds = stds.to_pandas().T\n",
    "\n",
    "pd_stds.sort_values(by=0, ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf18c8",
   "metadata": {},
   "source": [
    "## PCA 降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ee887",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_done_cols = [f\"z_{c}\" for c in do_z_co_cols]\n",
    "\n",
    "keep = [\n",
    "    \"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *STATIC_FEATURES, *CS_RANK_FEATURES, *CS_R_Z_FEATURES, *z_done_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定各个特征列处理方案：\n",
    "# already_z_cols = csrank_cols + other_already_z_cols\n",
    "\n",
    "# 挑选出不需要PCA的列：看z_done_cols 和other_already_z_cols中哪些 由COV_FEATURES[:200] 衍生的\n",
    "if False:\n",
    "        topk_cols = COV_FEATURES[:200]\n",
    "\n",
    "        csrank_cols = [c for c in already_z_cols if \"__csrank\" in c]\n",
    "        other_already_z_cols = [c for c in already_z_cols if c not in csrank_cols]\n",
    "        no_pca_cols_from_other_already = [c for c in other_already_z_cols if c in topk_cols]\n",
    "        no_pca_cols_from_z_done = [f\"z_{c}\" for c in do_z_cols if c in topk_cols]\n",
    "        no_pca_cols = [\"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL,\n",
    "                \"time_sin\", \"time_cos\", \"time_bucket\", \"z_time_pos\"] + no_z_grp_static_cols + csrank_cols + no_pca_cols_from_other_already + no_pca_cols_from_z_done\n",
    "        no_pca_cols = list(dict.fromkeys(no_pca_cols))  # 去重且保持顺序\n",
    "\n",
    "pca_cols =  TIME_FEATURES + STATIC_FEATURES + CS_RANK_FEATURES + CS_R_Z_FEATURES + z_done_cols\n",
    "no_pca_cols = [c for c in [\"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集上计算 PCA\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import joblib\n",
    "# 逐日处理本折所有数据\n",
    "fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"\n",
    "z_prefix = f\"{fold_root}/start{s_start_data}_train{s_end_date}_z_shards\"\n",
    "\n",
    "max_components = min(200, len(pca_cols))\n",
    "\n",
    "ipca = IncrementalPCA(n_components=max_components)\n",
    "\n",
    "for d in range(train_lo, train_hi + 1): \n",
    "    path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    print(f\"fitting IPCA on date: {d} from the path: {path}\")\n",
    "    \n",
    "    df_day = pl.read_parquet(path, storage_options=storage_options).select(pca_cols)\n",
    "    X = df_day.to_numpy()\n",
    "    ipca.partial_fit(X)  # 增量拟合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8237058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合完成后，看看累计方差占比，决定真正保留的维度k (95%)\n",
    "cum = ipca.explained_variance_ratio_.cumsum()\n",
    "tau =0.85\n",
    "k = int(np.searchsorted(cum, tau)) + 1\n",
    "print(f\"PCA components k = {k} for tau = {tau}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_final = 192\n",
    "print(f\"保留主成分维度： {k_final}\")\n",
    "\n",
    "# 保存模型\n",
    "# —— 裁剪参数到前 k 维（不要重新 new 模型）——\n",
    "ipca.components_              = ipca.components_[:k_final]\n",
    "ipca.explained_variance_      = ipca.explained_variance_[:k_final]\n",
    "ipca.explained_variance_ratio_= ipca.explained_variance_ratio_[:k_final]\n",
    "ipca.singular_values_         = ipca.singular_values_[:k_final]\n",
    "ipca.n_components_            = k_final\n",
    "\n",
    "\n",
    "pca_path = f\"{TFT_LOCAL_ROOT}/fold_{fold_id}_ipca_{train_lo}_{train_hi}_k{k_final}_{_now()}.joblib\"\n",
    "joblib.dump(\n",
    "    {\"ipca\": ipca, \"pca_cols\": pca_cols, \"cum_ratio\": cum, \"chosen_k\": k_final}, \n",
    "    pca_path\n",
    ")\n",
    "print(f\"IPCA 模型已保存至: {pca_path}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16efc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本折全部数据进行PCA变换\n",
    "\n",
    "pca_prefix = f\"{fold_root}/start{s_start_data}_train{s_end_date}_pca_shards\"; ensure_dir_az(pca_prefix)\n",
    "keep_cols = no_pca_cols\n",
    "\n",
    "pca_k = ipca.n_components_\n",
    "pca_out_cols = [f\"PC{i+1}\" for i in range(pca_k)]\n",
    "\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    in_path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    out_path = f\"{pca_prefix}/pca_shard_{d:04d}.parquet\"\n",
    "    \n",
    "    df = pl.read_parquet(in_path, storage_options=storage_options)\n",
    "    \n",
    "    X = (\n",
    "        df.select(pca_cols).to_numpy()\n",
    "    )\n",
    "    if X.size == 0:\n",
    "        # 当天没有行，直接跳过\n",
    "        continue\n",
    "    if np.isnan(X).any():\n",
    "        raise ValueError(\"PCA transform 输入里仍有 NaN，请检查上游补值/标准化。\")\n",
    "\n",
    "    Z = ipca.transform(X).astype(np.float32)\n",
    "    \n",
    "    # 组装输出\n",
    "    df_pca = pl.DataFrame(Z, schema=pca_out_cols)\n",
    "    \n",
    "    lf_out = pl.concat([\n",
    "        df.select(no_pca_cols),\n",
    "        df_pca\n",
    "    ],\n",
    "    how=\"horizontal\"\n",
    "    ).sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    lf_out.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"written PCA features to: {out_path}\")\n",
    "print(f\"[{_now()}] all done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b907e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按10天合并\n",
    "ten_days_prefix = f\"{fold_root}/feat_pca_ten_days_shards\"; ensure_dir_az(ten_days_prefix)\n",
    "\n",
    "chunk_size = 10\n",
    "\n",
    "for lo in range(train_lo, val_hi + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, folds_by_day[fold_id][1][-1])\n",
    "    paths = [f\"{pca_prefix}/pca_shard_{d:04d}.parquet\" for d in range(lo, hi + 1)]\n",
    "    \n",
    "    print(f\"merging date range: {lo} ~ {hi}, num files = {len(paths)}\")\n",
    "    \n",
    "    lf = pl.scan_parquet(paths, storage_options=storage_options)\n",
    "    \n",
    "    df = lf.collect(streaming=True).sort([G_SYM, \"time_idx\"]).rechunk()\n",
    "    \n",
    "    out_path = f\"{ten_days_prefix}/feat_pca_ten_days_chunk_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    df.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028cc3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f8cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取处理好的数据\n",
    "fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"\n",
    "ten_days_prefix = f\"{fold_root}/feat_pca_ten_days_shards\"\n",
    "\n",
    "lf_pca = pl.scan_parquet(f\"{ten_days_prefix}/*.parquet\", storage_options=storage_options)\n",
    "lf_pca = lf_pca.sort([G_SYM, \"time_idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63517c20",
   "metadata": {},
   "source": [
    "### 选择性PCA时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "        \n",
    "    lf_clean_train = lf_clean.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "    sym_med = lf_clean_train.group_by(G_SYM).agg(\n",
    "        [pl.col(c).median().alias(f\"median_{c}\") for c in no_z_grp_static_cols]\n",
    "    ).collect()\n",
    "\n",
    "    glb_med = lf_clean_train.select(\n",
    "        [pl.col(c).median().alias(f\"glb_median_{c}\") for c in no_z_grp_static_cols]\n",
    "    ).collect()\n",
    "\n",
    "\n",
    "    lf_pure = (\n",
    "        lf_clean.join(sym_med.lazy(), on=G_SYM, how=\"left\")\n",
    "        .join(glb_med.lazy(), how=\"cross\")\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.coalesce([\n",
    "                    pl.col(f\"median_{c}\"),\n",
    "                    pl.col(f\"glb_median_{c}\")\n",
    "                ]).alias(c)\n",
    "                for c in no_z_grp_static_cols\n",
    "            ]\n",
    "            \n",
    "        )\n",
    "        .drop([f\"median_{c}\" for c in no_z_grp_static_cols] + [f\"glb_median_{c}\" for c in no_z_grp_static_cols])\n",
    "        .sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "\n",
    "\n",
    "    # 对Feature_09, 10, 11 标准化\n",
    "    EPS = 1e-6\n",
    "    cols = no_z_grp_static_cols\n",
    "    # 训练期统计 -> 字典\n",
    "    stats = (\n",
    "        lf_pure.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "            .select([pl.mean(c).alias(f\"mean_{c}\") for c in cols] +\n",
    "                    [pl.std(c).alias(f\"std_{c}\")  for c in cols])\n",
    "            .collect(streaming=True)\n",
    "    )\n",
    "    mu  = {c: float(stats[0, f\"mean_{c}\"]) for c in cols}\n",
    "    sd  = {c: max(float(stats[0, f\"std_{c}\"]) or 0.0, EPS) for c in cols}\n",
    "\n",
    "\n",
    "    lf_final = lf_pure.with_columns([\n",
    "        ((pl.col(c) - pl.lit(mu[c])) / pl.lit(sd[c])).alias(c)   # 原地覆盖\n",
    "        for c in cols\n",
    "    ])\n",
    "    \n",
    "    KNOWN_CATEGORIES = [\"time_bucket\"]\n",
    "\n",
    "    STATIC_REALS = [c for c in no_z_grp_static_cols]\n",
    "\n",
    "\n",
    "    KNOWN_REALS = [c for c in lf_final.collect_schema().names() if c not in ([G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_bucket\", WEIGHT_COL, TARGET_COL] + STATIC_REALS)]\n",
    "\n",
    "\n",
    "    TRAIN_COLS = KNOWN_REALS + STATIC_REALS + KNOWN_CATEGORIES + [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8f841",
   "metadata": {},
   "source": [
    "## try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                          \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in KNOWN_CATEGORIES else None,\n",
    "                          },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction == val_start_idx + ENC_LEN) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c11547",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b07cdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train 1400..1534 (135 days), val 1542..1569 (28 days)\n"
     ]
    }
   ],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "####################################\n",
    "train_days, val_days = folds_by_day[fold_id]\n",
    "####################################\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "# 将数据转为 Pandas 以供 TFT 使用\n",
    "df = lf_pca.collect().to_pandas()\n",
    "\n",
    "\n",
    "df[G_SYM] = df[G_SYM].astype(str).astype(\"category\")\n",
    "df.sort_values([G_SYM, \"time_idx\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62859c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # STATIC 与 KNOWN 不重叠\n",
    "    assert not (set(STATIC_REALS) & set(KNOWN_REALS))\n",
    "    # 这些列都在数据里\n",
    "    need = set(KNOWN_REALS) | set(STATIC_REALS) | set(KNOWN_CATEGORIES) | {G_SYM, \"time_idx\", TARGET_COL}\n",
    "    missing = [c for c in need if c not in pdf_data.columns]\n",
    "    assert not missing, f\"missing columns: {missing}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51eb04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train idx up to 517879, val idx 524656..551759\n"
     ]
    }
   ],
   "source": [
    "# 明确 indexes:\n",
    "train_end_idx = df.loc[df[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = df.loc[df[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = df.loc[df[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    pdf_data = pdf_data[TRAIN_COLS]\n",
    "    # 定义 identity scalers\n",
    "    UNSCALE_COLS = KNOWN_REALS #+ STATIC_REALS\n",
    "    identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "\n",
    "meta_cols = [G_SYM, \"time_idx\", G_DATE, G_TIME, WEIGHT_COL, TARGET_COL]\n",
    "pca_cols = [c for c in df.columns if c not in meta_cols]\n",
    "identity_scalers = {name: None for name in pca_cols}\n",
    "\n",
    "# 构建训练集 timeseries dataset\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    df.loc[df[\"time_idx\"] <= train_end_idx],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    \n",
    "    max_encoder_length=ENC_LEN, min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    time_varying_known_reals=pca_cols,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True)},\n",
    "    \n",
    "    allow_missing_timesteps=True,\n",
    "    \n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM]),\n",
    "    scalers=identity_scalers,\n",
    "    \n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# 验证集复用 train_ds 的所有 encoders/normalizer（不泄漏）\n",
    "\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    df.loc[df[\"time_idx\"].between(val_start_idx-ENC_LEN, val_end_idx, inclusive=\"both\")],\n",
    "    min_prediction_idx=val_start_idx,\n",
    "    stop_randomization=True,\n",
    "    predict=False\n",
    ")\n",
    "\n",
    "#del pdf_data, lf_clean, lf_with_idx; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    lp.seed_everything(42)\n",
    "    # 配置：每个 epoch 想训练多少个“样本/窗口”\n",
    "    # 任选其一：按比例或固定数量\n",
    "    SAMPLE_FRACTION = 0.1      # 例：每个 epoch 只随机抽取 20% 的窗口\n",
    "    # SAMPLES_PER_EPOCH = 10000  # 或者：固定每个 epoch 抽 1e4 个窗口\n",
    "\n",
    "    # 计算 num_samples（即本 epoch 的样本数）\n",
    "    _train_total = len(train_ds); _val_total = len(val_ds)\n",
    "    num_samples = max(1, int(_train_total * SAMPLE_FRACTION))\n",
    "    # 如果用固定数量就改为：\n",
    "    # num_samples = min(_train_total, SAMPLES_PER_EPOCH)\n",
    "\n",
    "    # 打印epoch 样本数\n",
    "    print(f\"train full samples: {_train_total}, val full samples: {_val_total}\")\n",
    "\n",
    "    # 关键：带放回的随机采样器（每个 epoch 都会重新抽样）\n",
    "    train_sampler = RandomSampler(train_ds, replacement=True, num_samples=num_samples)\n",
    "\n",
    "\n",
    "    train_loader = train_ds.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=12, pin_memory=True, persistent_workers=True, prefetch_factor=2, shuffle=False, sampler=train_sampler, drop_last=True)\n",
    "\n",
    "    val_loader   = val_ds.to_dataloader(train=False, batch_size=BATCH_SIZE*8, num_workers=12, pin_memory=True, persistent_workers=True, prefetch_factor=2, shuffle=False)\n",
    "\n",
    "    n_train_batches = len(train_loader)\n",
    "    print(f\"train_loader batches = {n_train_batches}\")\n",
    "    n_val_batches = len(val_loader)\n",
    "    print(f\"val_loader batches = {n_val_batches}\")\n",
    "\n",
    "    print(f\"[{_now()}] data loaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec4e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader batches = 76226\n",
      "val_loader batches = 2011\n",
      "[2025-10-27 12:09:57] data loaders ready\n"
     ]
    }
   ],
   "source": [
    "train_loader = train_ds.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=12, pin_memory=True, persistent_workers=True, prefetch_factor=2, shuffle=True, drop_last=True)\n",
    "\n",
    "val_loader   = val_ds.to_dataloader(train=False, batch_size=BATCH_SIZE*8, num_workers=12, pin_memory=True, persistent_workers=True, prefetch_factor=2, shuffle=False, drop_last=True) # 先用 drop_last\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"train_loader batches = {n_train_batches}\")\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"val_loader batches = {n_val_batches}\")\n",
    "\n",
    "print(f\"[{_now()}] data loaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a72838",
   "metadata": {},
   "source": [
    "## 优化学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "pl.set_random_seed(42)\n",
    "\n",
    "# 1) 运行前清理缓存（可选）\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# 2) 给 LR finder 专用的小 batch loader\n",
    "#    不要动正式的 train_loader/val_loader\n",
    "# 3) 混合精度（优先 bf16，其次 fp16）\n",
    "trainer_lr = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "# 4) 可选：只为 LR finder 构建一个“轻量化”的 TFT（不影响正式模型）\n",
    "tft_lr = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,             # 降低一点隐藏维度\n",
    "    attention_head_size=HEADS,       # 降低头数\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    dropout=DROPOUT,\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"adamw\",\n",
    ")\n",
    "\n",
    "# 5) 运行 LR finder，限制搜索步数\n",
    "res = Tuner(trainer_lr).lr_find(\n",
    "    tft_lr,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    min_lr=1e-6,\n",
    "    max_lr=1e-2,         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25415ee6",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb25ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_id = 0\n",
      "train_lo = 1400, train_hi = 1534\n",
      "val_lo = 1542, val_hi = 1569\n",
      "target_col = responder_6\n",
      "weight_col = weight\n",
      "encode length = 128, pred length = 1\n",
      "fake lr = 0.0005, batch size = 64\n",
      "hiden size = 64, heads = 2, dropout = 0.2\n",
      "max epochs = 40\n",
      "gradient clip val = 0.1\n"
     ]
    }
   ],
   "source": [
    "from zoneinfo import ZoneInfo   # py>=3.9 自带\n",
    "import types\n",
    "\n",
    "LOCAL_TZ = ZoneInfo(\"Europe/Copenhagen\")     # 换成你想用的时区，如 \"Asia/Shanghai\"\n",
    "ts = datetime.now(LOCAL_TZ).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR, # 只是占位，实际由 OneCycle 控\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN//2,\n",
    "    loss=QuantileLoss(quantiles=[0.5]),                            # 点预测 OK，会自动用 weight                     \n",
    "    optimizer=\"adamw\",\n",
    ")\n",
    "# 打印模型参数\n",
    "print(f\"fold_id = {fold_id}\")\n",
    "print(f\"train_lo = {train_lo}, train_hi = {train_hi}\")\n",
    "print(f\"val_lo = {val_lo}, val_hi = {val_hi}\")\n",
    "print(f\"target_col = {TARGET_COL}\")\n",
    "print(f\"weight_col = {WEIGHT_COL}\")\n",
    "print(f\"encode length = {ENC_LEN}, pred length = {PRED_LEN}\")\n",
    "print(f\"fake lr = {LR}, batch size = {BATCH_SIZE}\")\n",
    "print(f\"hiden size = {HIDDEN}, heads = {HEADS}, dropout = {DROPOUT}\")\n",
    "print(f\"max epochs = {MAX_EPOCHS}\")\n",
    "print(f\"gradient clip val = {GCV}\")\n",
    "\n",
    "steps_per_epoch = 1000  \n",
    "def _cfg(self):\n",
    "    opt = torch.optim.AdamW(self.parameters(), lr=LR, weight_decay=1e-2)\n",
    "    sch = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        opt, max_lr=LR,\n",
    "        epochs=MAX_EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "        pct_start=0.3, anneal_strategy='cos',\n",
    "        div_factor=10, final_div_factor=100,\n",
    "        cycle_momentum=False,  # AdamW 建议关\n",
    "    )\n",
    "    return {\n",
    "        \"optimizer\": opt,\n",
    "        \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"step\"}  # 每 step 调度\n",
    "    }\n",
    "\n",
    "tft.configure_optimizers = types.MethodType(_cfg, tft)\n",
    "\n",
    "\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_NAME = f\"pca_time_{ts}adamw_onecycle_maxlr{LR:g}_bs{BATCH_SIZE}_enc{ENC_LEN}_hidden{HIDDEN}_heads{HEADS}\"\n",
    "TEMP_LOG_DIR = Path(\"./tft_logs\"); TEMP_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logger = TensorBoardLogger(save_dir=TEMP_LOG_DIR.as_posix(),version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5), \n",
    "            LearningRateMonitor()\n",
    "            ] \n",
    "\n",
    "trainer = lp.Trainer(\n",
    "    #fast_dev_run=1,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_model_summary=True,\n",
    "    limit_train_batches=1000,  # 用全部 batch 训练\n",
    "    limit_val_batches=30,    # 用全部 batch 验证\n",
    "    gradient_clip_val=GCV,          \n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=callbacks,   # ← 记得把 lr_logger 放进来\n",
    "    logger=logger,\n",
    "    accumulate_grad_batches=ACCUM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14cb1173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TFT patch] mask_bias replaced in 2 module(s).\n",
      " - multihead_attn <class 'float'> -65504.0\n",
      " - multihead_attn.attention <class 'float'> -65504.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def patch_tft_mask_bias(model):\n",
    "    hit = 0\n",
    "    for n, m in model.named_modules():\n",
    "        if hasattr(m, \"mask_bias\"):\n",
    "            old = getattr(m, \"mask_bias\")\n",
    "            if isinstance(old, (int, float)) and old < -1e8:\n",
    "                setattr(m, \"mask_bias\", -65504.0)   # 关键：FP16 最小可表示\n",
    "                hit += 1\n",
    "    print(f\"[TFT patch] mask_bias replaced in {hit} module(s).\")\n",
    "    # 诊断输出（确认真的改到了）\n",
    "    for n, m in model.named_modules():\n",
    "        if hasattr(m, \"mask_bias\"):\n",
    "            print(\" -\", n, type(m.mask_bias), m.mask_bias)\n",
    "\n",
    "patch_tft_mask_bias(tft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15778616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 520    | train\n",
      "3  | prescalers                         | ModuleDict                      | 12.5 K | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 20.9 K | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.7 M  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.7 M  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 12.4 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 65     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.685    Total estimated model params size (MB)\n",
      "5804      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1000/1000 [31:49<00:00,  0.52it/s, v_num=ads2, train_loss_step=1.240, val_loss=1.550, train_loss_epoch=1.360]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(tft, train_dataloaders=train_loader, val_dataloaders=val_loader)#####2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46887075",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://localhost:16006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e801b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5ab45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 callbacks/logger/trainer\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, dirpath=ckpt_dir_fold.as_posix(), filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\", save_on_train_epoch_end=False),\n",
    "            ] # LearningRateMonitor(logging_interval=\"step\"),\n",
    "RUN_NAME = f\"without_static_f{fold_id}_E{MAX_EPOCHS}_lr{LR:g}_bs{BATCH_SIZE}_enc{ENC_LEN}_dec{DEC_LEN}_{datetime.now():%Y%m%d-%H%M%S}\"\n",
    "\n",
    "TEMP_LOG_DIR = Path(\"./tft_logs\")\n",
    "TEMP_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=TEMP_LOG_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=3,\n",
    "                    accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    enable_model_summary=True,\n",
    "                    gradient_clip_val=1.0,\n",
    "                    gradient_clip_algorithm=\"norm\",\n",
    "                    #fast_dev_run=1,\n",
    "                    limit_train_batches=1.0,\n",
    "                    limit_val_batches=1.0,\n",
    "                    val_check_interval=0.5,\n",
    "                    num_sanity_val_steps=0,\n",
    "                    log_every_n_steps=200,\n",
    "                    callbacks=callbacks,\n",
    "                    logger=logger,\n",
    "                    #accumulate_grad_batches=1,\n",
    "                    )\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[],\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    reduce_on_plateau_patience=3, \n",
    ")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a0c6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d117f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323975ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (torch.square(y_true - y_pred) * w).sum()\n",
    "den = (torch.square(y_true) * w).sum()  \n",
    "wr2 = 1 - num / den\n",
    "print(f\"wr2 after training: {wr2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
