{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2317e57f",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4586e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-15 23:24:01] imports ok\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "from pipeline.stream_input_local import ShardedBatchStream  \n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c956339",
   "metadata": {},
   "source": [
    "## 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cd586",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96a700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入筛选的特征列\n",
    "df_ranking_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1300-1500__cv3-g7-r4__seed42__top1000__1760299442__range1000-1600__range1000-1600__cv2-g7-r4__1760347190.csv\")\n",
    "de_corr_features = pd.read_csv(\"/mnt/data/js/exp/v1/tft/selected_features/selected_features__decorr__tau0.95__1400-1600.csv\")\n",
    "df_ranking_decorr_features = df_ranking_features.loc[df_ranking_features['feature'].isin(de_corr_features['feature'])].copy()\n",
    "df_e_features = df_ranking_decorr_features.reset_index(drop=True)\n",
    "\n",
    "# 我们这里重新归一化一下\n",
    "df_e_features['mean_gain'] = (df_e_features['mean_gain'] / df_e_features['mean_gain'].sum()).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2272790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e_features.iloc[:150]['mean_gain'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] ready\n"
     ]
    }
   ],
   "source": [
    "# ========== 1) 统一配置 ==========\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "COV_FEATURES = df_e_features['feature'].tolist()\n",
    "# 明确要标准化的列\n",
    "z_cols = [\"time_pos\"] + COV_FEATURES\n",
    "\n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 0\n",
    "TRAIN_TO_VAL = 8\n",
    "ENC_LEN      = 10\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 512\n",
    "LR           = 1e-4\n",
    "HIDDEN       = 64\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.1\n",
    "MAX_EPOCHS   = 30\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "\n",
    "start_date, end_date = (1500, 1600)\n",
    "\n",
    "\n",
    "print(\"[config] ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300bbba",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *COV_FEATURES])\n",
    "    .filter(pl.col(G_DATE).is_between(start_date, end_date, closed=\"both\"))\n",
    ")\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查缺失值\n",
    "df_null_case = lf_data.select(pl.all().is_null().sum()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = df_null_case.to_pandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d41e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改列名\n",
    "df_null.index.name = 'feature'\n",
    "df_null = df_null.rename(columns={df_null.columns[0]: \"null_count\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = df_null.sort_values(by=\"null_count\", ascending=False)\n",
    "df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_data.select(pl.len()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1aaf",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d516",
   "metadata": {},
   "source": [
    "### 添加全局时间序列号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "        .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "chunk_size = 30\n",
    "for lo in range(start_date, end_date + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, end_date)\n",
    "    print(f\"processing date range: {lo} ~ {hi}\")\n",
    "    \n",
    "    lf_chunk = lf_data.filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    \n",
    "    lf_grid_chunk = (\n",
    "        grid_df.lazy().filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    )\n",
    "    \n",
    "    lf_joined = (\n",
    "        lf_chunk.join(lf_grid_chunk, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "    \n",
    "    out_path = f\"{container_prefix}/panel_clean_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    \n",
    "    lf_joined.sink_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb09de",
   "metadata": {},
   "source": [
    "## 导入新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11120fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新读入数据\n",
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "data_paths = fs.glob(f\"{container_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths] \n",
    "lf_with_idx = pl.scan_parquet(data_paths, storage_options=storage_options).sort([G_SYM, G_DATE, G_TIME])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e04edc",
   "metadata": {},
   "source": [
    "## CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a016c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cv] total 1 folds\n",
      "[(array([1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510,\n",
      "       1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521,\n",
      "       1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532,\n",
      "       1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543,\n",
      "       1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554,\n",
      "       1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565,\n",
      "       1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576,\n",
      "       1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587],\n",
      "      dtype=int32), array([1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598,\n",
      "       1599], dtype=int32))]\n"
     ]
    }
   ],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_with_idx.select(pl.col(G_DATE)).unique().sort([G_DATE])\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "\n",
    "print(f\"[cv] total {len(folds_by_day)} folds\")\n",
    "\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "print(folds_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5315",
   "metadata": {},
   "source": [
    "## 数据标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d932b1",
   "metadata": {},
   "source": [
    "下面我们以一折为例，验证成功后，再使用多折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13dab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计标准化使用训练集的日期范围 = 1500 ~ 1587\n"
     ]
    }
   ],
   "source": [
    "fold_id = 0\n",
    "# 取第一个 fold 的训练集最后一天，作为本折统计 z-score 的上界\n",
    "train_lo, train_hi = folds_by_day[fold_id][0][0], folds_by_day[fold_id][0][-1]\n",
    "val_lo, val_hi = folds_by_day[fold_id][1][0], folds_by_day[fold_id][1][-1]\n",
    "\n",
    "print(f\"统计标准化使用训练集的日期范围 = {train_lo} ~ {train_hi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f563a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集的各特征量的中位数\n",
    "lf_tr = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_median = (\n",
    "    lf_tr\n",
    "    .group_by([G_SYM, \"time_pos\"])\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_st\") for col in COV_FEATURES\n",
    "    ])\n",
    ").sort([G_SYM, \"time_pos\"])\n",
    "\n",
    "glb_median = (\n",
    "    lf_tr\n",
    "    .group_by(\"time_pos\")\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_s\") for col in COV_FEATURES\n",
    "    ])\n",
    ").sort(\"time_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用于本折全部数据 trian + val\n",
    "lf_all = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, val_hi, closed=\"both\"))\n",
    "lf_all = lf_all.join(grp_median, on=[G_SYM, \"time_pos\"], how=\"left\").join(glb_median, on=[\"time_pos\"], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "# 逐列用中位数替换缺失值\n",
    "fill_exprs = []\n",
    "\n",
    "for col in COV_FEATURES:\n",
    "    fill_exprs.append(\n",
    "        pl.coalesce([\n",
    "            pl.col(col),\n",
    "            pl.col(f\"{col}_median_st\"),\n",
    "            pl.col(f\"{col}_median_s\")\n",
    "        ]).alias(col)\n",
    "    )\n",
    "\n",
    "lf_all_imputed = lf_all.with_columns(fill_exprs)\n",
    "\n",
    "# 去掉中位数列\n",
    "drop_cols = [f\"{col}_median_st\" for col in COV_FEATURES]\n",
    "drop_cols += [f\"{col}_median_s\" for col in COV_FEATURES]\n",
    "lf_all_imputed = lf_all_imputed.drop(drop_cols)\n",
    "\n",
    "# 显式检查：若仍有缺失，直接报错\n",
    "remain = lf_all_imputed.select([pl.col(c).is_null().sum().alias(c) for c in COV_FEATURES]).collect()\n",
    "bad = {c: remain[c][0] for c in COV_FEATURES if remain[c][0] > 0}\n",
    "if bad:\n",
    "    raise ValueError(f\"补值失败：以下列在 (symbol_id,time_pos) 和 time_pos 层级都无中位数 -> {bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41652fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Z-score ==========\n",
    "# 开始计算 stats\n",
    "lf_tr_imputed = lf_all_imputed.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_mean = (\n",
    "    lf_tr_imputed\n",
    "    .group_by(G_SYM)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in z_cols] +\n",
    "        [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in z_cols])\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_mean = (\n",
    "    lf_tr_imputed\n",
    "    .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in z_cols] +\n",
    "            [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in z_cols])\n",
    ").collect(streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2038473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐日处理本折所有数据\n",
    "\n",
    "z_prefix = \"az://jackson/js_exp/exp/v1/tft/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "eps = 1e-6\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    print(f\"processing date: {d}\")\n",
    "    lf_day = lf_all_imputed.filter(pl.col(G_DATE) == d)\n",
    "    \n",
    "    lf_day_z = (\n",
    "        lf_day.join(glb_mean.lazy(), how=\"cross\").join(grp_mean.lazy(), on=G_SYM, how=\"left\") # 循环内拼接，避免OOM\n",
    "        \n",
    "    ).sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    for c in z_cols:\n",
    "        mu_sym, std_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "        mu_glb, std_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "        mu_use, std_use = f\"mu_{c}_use\", f\"std_{c}_use\"\n",
    "        z_c = f\"z_{c}\"\n",
    "        \n",
    "        lf_day_z = lf_day_z.with_columns([\n",
    "            pl.when(pl.col(mu_sym).is_null()).then(pl.col(mu_glb)).otherwise(pl.col(mu_sym)).alias(mu_use),\n",
    "            pl.when(pl.col(std_sym).is_null() | (pl.col(std_sym) == 0)).then(pl.col(std_glb)).otherwise(pl.col(std_sym)).alias(std_use),   \n",
    "        ]).with_columns(\n",
    "            ((pl.col(c) - pl.col(mu_use)) / (pl.col(std_use) + eps)).alias(z_c)\n",
    "        ).drop([mu_sym, std_sym, mu_glb, std_glb, mu_use, std_use])\n",
    "    \n",
    "    out_cols = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL, \"time_bucket\", \"time_sin\", \"time_cos\", *[f\"z_{c}\" for c in z_cols]] # 这个地方z_cols 含 z_time_pos\n",
    "    lf_day_z = lf_day_z.select([c for c in out_cols if c in lf_day_z.collect_schema().names()])\n",
    "    lf_day_z = lf_day_z.sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    # 写出\n",
    "    out_path = f\"{z_prefix}/z_shard_{d:04d}.parquet\"\n",
    "    \n",
    "    lf_day_z.collect(streaming=True).write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf18c8",
   "metadata": {},
   "source": [
    "## PCA 降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集上计算 PCA\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import joblib\n",
    "\n",
    "\n",
    "pca_cols = [f\"z_{c}\" for c in COV_FEATURES[150:]]\n",
    "keep_cols = [f\"z_{c}\" for c in COV_FEATURES[:150]]\n",
    "\n",
    "max_components = min(100, len(pca_cols))\n",
    "\n",
    "ipca = IncrementalPCA(n_components=max_components)\n",
    "\n",
    "for d in range(train_lo, train_hi + 1):\n",
    "    path = f\"{z_prefix}/z_shard_{d:04d}.parquet\"\n",
    "    print(f\"fitting IPCA on date: {d} from the path: {path}\")\n",
    "    \n",
    "    df_day = pl.read_parquet(path, storage_options=storage_options).select(pca_cols).to_pandas()\n",
    "    X = df_day.to_numpy()\n",
    "    ipca.partial_fit(X)  # 增量拟合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合完成后，看看累计方差占比，决定真正保留的维度k (95%)\n",
    "cum = ipca.explained_variance_ratio_.cumsum()\n",
    "tau =0.5\n",
    "k = int(np.searchsorted(cum, tau)) + 1\n",
    "\n",
    "print(f\"{k} components explain {cum[k-1]:.4f} of variance\")\n",
    "\n",
    "# 保存模型\n",
    "# —— 裁剪参数到前 k 维（不要重新 new 模型）——\n",
    "ipca.components_              = ipca.components_[:k]\n",
    "ipca.explained_variance_      = ipca.explained_variance_[:k]\n",
    "ipca.explained_variance_ratio_= ipca.explained_variance_ratio_[:k]\n",
    "ipca.singular_values_         = ipca.singular_values_[:k]\n",
    "ipca.n_components_            = k\n",
    "\n",
    "\n",
    "pca_path = f\"{TFT_LOCAL_ROOT}/fold_{fold_id}_ipca_{train_lo}_{train_hi}_k{k}_{_now()}.joblib\"\n",
    "joblib.dump(\n",
    "    {\"ipca\": ipca, \"pca_cols\": pca_cols, \"cum_ratio\": cum, \"chosen_k\": k}, \n",
    "    pca_path\n",
    ")\n",
    "print(f\"IPCA 模型已保存至: {pca_path}\")\n",
    "print(f\"保留主成分{tau}维度 k = {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16efc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本折全部数据进行PCA变换\n",
    "\n",
    "pca_prefix = \"az://jackson/js_exp/exp/v1/tft/feat_pca_shards\"; ensure_dir_az(pca_prefix)\n",
    "meta_cols = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL, \"time_bucket\", \"time_sin\", \"time_cos\", \"z_time_pos\"]\n",
    "\n",
    "pca_k = ipca.n_components_\n",
    "pca_out_cols = [f\"PC{i+1}\" for i in range(pca_k)]\n",
    "\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    in_path = f\"{z_prefix}/z_shard_{d:04d}.parquet\"\n",
    "    out_path = f\"{pca_prefix}/pca_shard_{d:04d}.parquet\"\n",
    "    \n",
    "    lf = pl.read_parquet(in_path, storage_options=storage_options)\n",
    "    \n",
    "    X = (\n",
    "        lf.select([\n",
    "            pl.col(c).cast(pl.Float32) for c in pca_cols\n",
    "        ]).to_numpy()\n",
    "    )\n",
    "    if X.size == 0:\n",
    "        # 当天没有行，直接跳过\n",
    "        continue\n",
    "    if np.isnan(X).any():\n",
    "        raise ValueError(\"PCA transform 输入里仍有 NaN，请检查上游补值/标准化。\")\n",
    "\n",
    "    Z = ipca.transform(X).astype(np.float32)\n",
    "    \n",
    "    # 组装输出\n",
    "    df_pca = pl.DataFrame(Z, schema=pca_out_cols)\n",
    "    lf_out = pl.concat([\n",
    "        lf.select([c for c in meta_cols if c in lf.columns]),\n",
    "        lf.select([c for c in keep_cols if c in lf.columns]),\n",
    "        df_pca\n",
    "    ],\n",
    "    how=\"horizontal\"\n",
    "    ).sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    lf_out.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"written PCA features to: {out_path}\")\n",
    "print(f\"[{_now()}] all done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b907e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按10天合并\n",
    "tft_fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(tft_fold_root)\n",
    "ten_days_prefix = f\"{tft_fold_root}/feat_pca_ten_days_shards\"; ensure_dir_az(ten_days_prefix)\n",
    "\n",
    "chunk_size = 10\n",
    "\n",
    "for lo in range(train_lo, val_hi + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, folds_by_day[fold_id][1][-1])\n",
    "    paths = [f\"{pca_prefix}/pca_shard_{d:04d}.parquet\" for d in range(lo, hi + 1)]\n",
    "    \n",
    "    print(f\"merging date range: {lo} ~ {hi}, num files = {len(paths)}\")\n",
    "    \n",
    "    lf = pl.scan_parquet(paths, storage_options=storage_options)\n",
    "    \n",
    "    df = lf.collect(streaming=True).sort([G_SYM, \"time_idx\"]).rechunk()\n",
    "    \n",
    "    out_path = f\"{ten_days_prefix}/feat_pca_ten_days_chunk_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    df.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028cc3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f8cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取处理好的数据\n",
    "tft_fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(tft_fold_root)\n",
    "ten_days_prefix = f\"{tft_fold_root}/feat_pca_ten_days_shards\"; ensure_dir_az(ten_days_prefix)\n",
    "\n",
    "\n",
    "lf_clean = pl.scan_parquet(f\"{ten_days_prefix}/*.parquet\", storage_options=storage_options)\n",
    "lf_clean = lf_clean.sort([G_SYM, \"time_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_REALS = [c for c in lf_clean.collect_schema().names() if c not in (G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_bucket\", WEIGHT_COL, TARGET_COL)]\n",
    "\n",
    "KNOWN_CATEGORIES = [\"time_bucket\"]\n",
    "\n",
    "UNSCALE_COLS = KNOWN_REALS\n",
    "\n",
    "TRAIN_COLS = [c for c in lf_clean.collect_schema().names() if c not in (G_DATE, G_TIME)]\n",
    "\n",
    "# 定义 identity scalers\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8f841",
   "metadata": {},
   "source": [
    "## try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                          \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in KNOWN_CATEGORIES else None,\n",
    "                          },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction == val_start_idx + ENC_LEN) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c11547",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b07cdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train 1500..1587 (88 days), val 1588..1599 (12 days)\n"
     ]
    }
   ],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "# ========== 训练（按 CV 折） ========== 先取第一折\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "#for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "\n",
    "####################################\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[fold_id]\n",
    "####################################\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    lf_clean\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ") \n",
    "\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(str).astype(\"category\")\n",
    "pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(str).astype(\"category\")\n",
    "pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51eb04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train idx up to 85183, val idx 85184..96799\n"
     ]
    }
   ],
   "source": [
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ede7fb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "# 构建训练集 timeseries dataset\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    pdf_data.loc[pdf_data[\"time_idx\"] <= train_end_idx],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, min_prediction_length=PRED_LEN,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    static_categoricals=[G_SYM],\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                        \"time_bucket\": NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")\n",
    "\n",
    "# 验证集复用 train_ds 的所有 encoders/normalizer（不泄漏）\n",
    "\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    pdf_data.loc[pdf_data[\"time_idx\"].between(val_start_idx, val_end_idx, inclusive=\"both\")],\n",
    "    min_prediction_idx=val_start_idx+ENC_LEN,\n",
    "    stop_randomization=True,\n",
    "    predict=False\n",
    ")\n",
    "\n",
    "del pdf_data, lf_clean, lf_with_idx; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0412450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] train_loader batches = 6394\n",
      "[debug] val_loader batches = 433\n"
     ]
    }
   ],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE*2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 callbacks/logger/trainer\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, dirpath=ckpt_dir_fold.as_posix(), filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\", save_on_train_epoch_end=False),\n",
    "            ] # LearningRateMonitor(logging_interval=\"step\"),\n",
    "RUN_NAME = f\"quick_check_f{fold_id}_E{MAX_EPOCHS}_lr{LR:g}_bs{BATCH_SIZE}_enc{ENC_LEN}_dec{DEC_LEN}_{datetime.now():%Y%m%d-%H%M%S}\"\n",
    "\n",
    "TEMP_LOG_DIR = Path(\"./tft_logs\")\n",
    "TEMP_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=TEMP_LOG_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=20,\n",
    "                    accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    enable_model_summary=True,\n",
    "                    gradient_clip_val=1.0,\n",
    "                    gradient_clip_algorithm=\"norm\",\n",
    "                    #fast_dev_run=1,\n",
    "                    limit_train_batches=0.3,\n",
    "                    limit_val_batches=0.2,\n",
    "                    val_check_interval= 1.0,\n",
    "                    num_sanity_val_steps=0,\n",
    "                    log_every_n_steps=1000,\n",
    "                    callbacks=callbacks,\n",
    "                    logger=logger,\n",
    "                    #accumulate_grad_batches=1,\n",
    "                    )\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[],\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    reduce_on_plateau_patience=3, \n",
    ")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a0c6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323975ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (torch.square(y_true - y_pred) * w).sum()\n",
    "den = (torch.square(y_true) * w).sum()  \n",
    "wr2 = 1 - num / den\n",
    "print(f\"wr2 after training: {wr2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
