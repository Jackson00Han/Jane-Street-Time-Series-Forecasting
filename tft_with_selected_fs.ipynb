{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2317e57f",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4586e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '/tmp/tmpejdj3_h6'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcudnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcudnn\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mL\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlp\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__about__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: E402, F403\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__version__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/fabric/__init__.py:35\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     32\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE_LIBUV\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/fabric/fabric.py:40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PLUGIN_INPUT, _PRECISION_INPUT, _Connector, _is_using_cli\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Precision  \u001b[38;5;66;03m# avoid circular imports: # isort: split\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     DataParallelStrategy,\n\u001b[1;32m     44\u001b[0m     DeepSpeedStrategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     XLAStrategy,\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/fabric/loggers/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv_logs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CSVLogger  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Logger  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorBoardLogger  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/fabric/loggers/tensorboard.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrank_zero\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rank_zero_only, rank_zero_warn\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PATH\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _unwrap_objects\n\u001b[1;32m     33\u001b[0m _TENSORBOARD_AVAILABLE \u001b[38;5;241m=\u001b[39m RequirementCache(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m _TENSORBOARDX_AVAILABLE \u001b[38;5;241m=\u001b[39m RequirementCache(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboardX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/fabric/wrappers.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn \u001b[38;5;28;01mas\u001b[39;00m nn\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptimizedModule\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _IncompatibleKeys\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackTrigger\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_method\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     59\u001b[0m     config,\n\u001b[1;32m     60\u001b[0m     exc,\n\u001b[1;32m     61\u001b[0m     graph_break_hints,\n\u001b[1;32m     62\u001b[0m     logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging,\n\u001b[1;32m     63\u001b[0m     trace_rules,\n\u001b[1;32m     64\u001b[0m     variables,\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     67\u001b[0m     get_indexof,\n\u001b[1;32m     68\u001b[0m     JUMP_OPNAMES,\n\u001b[1;32m     69\u001b[0m     livevars_analysis,\n\u001b[1;32m     70\u001b[0m     propagate_line_nums,\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     73\u001b[0m     cleaned_instructions,\n\u001b[1;32m     74\u001b[0m     create_call_function,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     unique_id,\n\u001b[1;32m     82\u001b[0m )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresume_execution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TORCH_DYNAMO_RESUME_IN_PREFIX\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getfile, hashable, NP_SUPPORTED_MODULES, unwrap_if_wrapper\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     57\u001b[0m     BuiltinVariable,\n\u001b[1;32m     58\u001b[0m     FunctionalCallVariable,\n\u001b[1;32m     59\u001b[0m     FunctorchHigherOrderVariable,\n\u001b[1;32m     60\u001b[0m     LocalGeneratorFunctionVariable,\n\u001b[1;32m     61\u001b[0m     LocalGeneratorObjectVariable,\n\u001b[1;32m     62\u001b[0m     NestedUserFunctionVariable,\n\u001b[1;32m     63\u001b[0m     PolyfilledFunctionVariable,\n\u001b[1;32m     64\u001b[0m     SkipFunctionVariable,\n\u001b[1;32m     65\u001b[0m     TorchInGraphFunctionVariable,\n\u001b[1;32m     66\u001b[0m     UserFunctionVariable,\n\u001b[1;32m     67\u001b[0m     UserMethodVariable,\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     71\u001b[0m np: Optional[types\u001b[38;5;241m.\u001b[39mModuleType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/variables/__init__.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package implements variable tracking and symbolic execution capabilities for Dynamo,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mwhich are essential for converting Python code into FX graphs. It provides a comprehensive\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mallows Dynamo to accurately trace and optimize Python code while preserving its semantics.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VariableTracker\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BuiltinVariable\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConstantVariable, EnumVariable\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/variables/base.py:642\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(typestr, objs))\n\u001b[0;32m--> 642\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m builder\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:93\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GuardBuilder, install_guard, make_dupe_guard\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpgo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     87\u001b[0m     auto_dynamic,\n\u001b[1;32m     88\u001b[0m     auto_unset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     process_automatic_dynamic,\n\u001b[1;32m     92\u001b[0m )\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mside_effects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SideEffects\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     95\u001b[0m     AttrProxySource,\n\u001b[1;32m     96\u001b[0m     AttrSource,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     UnspecializedNNModuleSource,\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m     _extract_tensor_dict,\n\u001b[1;32m    124\u001b[0m     build_checkpoint_variable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     wrap_fake_exception,\n\u001b[1;32m    154\u001b[0m )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/side_effects.py:44\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_break_hints, utils, variables\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     bytecode_from_template,\n\u001b[1;32m     40\u001b[0m     create_call_function,\n\u001b[1;32m     41\u001b[0m     create_call_method,\n\u001b[1;32m     42\u001b[0m     create_instruction,\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodegen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyCodegen\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SideEffectsError, unimplemented_v2\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalSource, LocalCellSource, LocalSource, Source\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/codegen.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_safe_constant, rot_n_helper\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ValueMutationExisting, VariableTracker\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     44\u001b[0m     ContextlibContextManagerLocalGeneratorObjectVariable,\n\u001b[1;32m     45\u001b[0m     LocalGeneratorObjectVariable,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNModuleVariable\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     NumpyNdarrayVariable,\n\u001b[1;32m     50\u001b[0m     SymNodeVariable,\n\u001b[1;32m     51\u001b[0m     TensorVariable,\n\u001b[1;32m     52\u001b[0m     UnspecializedPythonVariable,\n\u001b[1;32m     53\u001b[0m )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:81\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConstantVariable\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fully_shard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fsdp_param_group\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     _fsdp_param_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/fsdp/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_flat_param\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlatParameter \u001b[38;5;28;01mas\u001b[39;00m FlatParameter\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fully_shard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     CPUOffloadPolicy,\n\u001b[1;32m      4\u001b[0m     FSDPModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     UnshardHandle,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfully_sharded_data_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     BackwardPrefetch,\n\u001b[1;32m     13\u001b[0m     CPUOffload,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     StateDictType,\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/fsdp/_flat_param.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ParameterMeta  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_pg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeProcessGroup\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fsdp_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     _ext_post_unflatten_transform,\n\u001b[1;32m     35\u001b[0m     _ext_pre_flatten_transform,\n\u001b[1;32m     36\u001b[0m     FSDPExtensions,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     40\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlatParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlatParamHandle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHandleShardingStrategy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m ]\n\u001b[1;32m     49\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/fsdp/_fsdp_extensions.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharded_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardedTensor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharded_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Shard\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     _all_gather_dtensor,\n\u001b[1;32m     10\u001b[0m     _create_chunk_dtensor,\n\u001b[1;32m     11\u001b[0m     _create_chunk_sharded_tensor,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/_shard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _shard_tensor, load_with_process_group, shard_module, shard_parameter\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/_shard/api.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributed_c10d\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharded_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardedTensor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sharder\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharding_plan\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardingPlan\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_registry_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _decorator_func\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     _CUSTOM_SHARDED_OPS,\n\u001b[1;32m     10\u001b[0m     _SHARDED_OPS,\n\u001b[1;32m     11\u001b[0m     Shard,\n\u001b[1;32m     12\u001b[0m     ShardedTensor,\n\u001b[1;32m     13\u001b[0m     ShardedTensorBase,\n\u001b[1;32m     14\u001b[0m     ShardedTensorMetadata,\n\u001b[1;32m     15\u001b[0m     TensorProperties,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardMetadata  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pytree \u001b[38;5;28;01mas\u001b[39;00m pytree\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardedTensorMetadata, TensorProperties\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reshard_local_shard, reshuffle_local_shard\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Shard\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     _flatten_tensor_size,\n\u001b[1;32m     36\u001b[0m     _parse_and_validate_remote_device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     build_metadata_from_local_shards,\n\u001b[1;32m     40\u001b[0m )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/reshard.py:13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardMetadata\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharding_spec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     get_chunked_dim_size,\n\u001b[1;32m     11\u001b[0m     get_split_size,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m all_to_all, all_to_all_single\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Shard\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_idx_from_placements\u001b[39m(placements, current_rank) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/nn/__init__.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mrpc\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremote_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RemoteModule\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/nn/api/remote_module.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device, dtype, nn, Tensor\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _remote_device\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m instantiator\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _internal_rpc_pickler\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/distributed/nn/jit/instantiator.py:21\u001b[0m\n\u001b[1;32m     17\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     20\u001b[0m _FILE_PREFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_remote_module_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m _TEMP_DIR \u001b[38;5;241m=\u001b[39m \u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTemporaryDirectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m INSTANTIATED_TEMPLATE_DIR_PATH \u001b[38;5;241m=\u001b[39m _TEMP_DIR\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     23\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(_TEMP_DIR\u001b[38;5;241m.\u001b[39mcleanup)\n",
      "File \u001b[0;32m/usr/lib/python3.10/tempfile.py:974\u001b[0m, in \u001b[0;36mTemporaryDirectory.__init__\u001b[0;34m(self, suffix, prefix, dir, ignore_cleanup_errors)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    973\u001b[0m              ignore_cleanup_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[43mmkdtemp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_cleanup_errors \u001b[38;5;241m=\u001b[39m ignore_cleanup_errors\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalizer \u001b[38;5;241m=\u001b[39m _weakref\u001b[38;5;241m.\u001b[39mfinalize(\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    978\u001b[0m         warn_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplicitly cleaning up \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    979\u001b[0m         ignore_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_cleanup_errors)\n",
      "File \u001b[0;32m/usr/lib/python3.10/tempfile.py:523\u001b[0m, in \u001b[0;36mmkdtemp\u001b[0;34m(suffix, prefix, dir)\u001b[0m\n\u001b[1;32m    521\u001b[0m _sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempfile.mkdtemp\u001b[39m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m     \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0o700\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/tmp/tmpejdj3_h6'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "from pipeline.stream_input_local import ShardedBatchStream  \n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c956339",
   "metadata": {},
   "source": [
    "## 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cd586",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c7e58",
   "metadata": {},
   "source": [
    "读入选出的特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64883d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入筛选的所有特征列\n",
    "\n",
    "df_ranking_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1300-1500__cv3-g7-r4__seed42__top1000__1760299442__range1000-1600__range1000-1600__cv2-g7-r4__1760347190.csv\")\n",
    "de_corr_features = pd.read_csv(\"/mnt/data/js/exp/v1/tft/selected_features/selected_features__decorr__tau0.95__1400-1600.csv\")\n",
    "df_ranking_decorr_features = df_ranking_features.loc[df_ranking_features['feature'].isin(de_corr_features['feature'])].copy()\n",
    "df_e_features = df_ranking_decorr_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们这里重新归一化一下\n",
    "df_e_features['mean_gain'] = (df_e_features['mean_gain'] / df_e_features['mean_gain'].sum()).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2272790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留前150个特征不进行PCA降维\n",
    "df_e_features.iloc[:150]['mean_gain'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1) 初始化配置 ==========\n",
    "\n",
    "# 所有列\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "COV_FEATURES = df_e_features['feature'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 0\n",
    "TRAIN_TO_VAL = 8\n",
    "ENC_LEN      = 10\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 512\n",
    "LR           = 1e-4\n",
    "HIDDEN       = 32\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.1\n",
    "MAX_EPOCHS   = 30\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "\n",
    "start_date, end_date = (1500, 1503)  # 仅用于本次实验\n",
    "\n",
    "\n",
    "print(\"[config] ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300bbba",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *COV_FEATURES])\n",
    "    .filter(pl.col(G_DATE).is_between(start_date, end_date, closed=\"both\"))\n",
    ")\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查缺失值\n",
    "df_null_case = lf_data.select(pl.all().is_null().sum()).collect()\n",
    "df_null = df_null_case.to_pandas().T\n",
    "df_null.rename(columns={0: 'null_count'}, inplace=True)\n",
    "df_null.index.name = 'feature'\n",
    "df_null.reset_index(inplace=True)\n",
    "df_null = df_null.sort_values(by='null_count', ascending=False)\n",
    "df_null.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除缺失值超过40%的列\n",
    "\n",
    "# 返回总行数\n",
    "total_nrow = lf_data.select(pl.len()).collect()[0, 0]\n",
    "drop_cols =df_null[df_null[\"null_count\"] / total_nrow > 0.4][\"feature\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新特征列\n",
    "COV_FEATURES = [c for c in COV_FEATURES if c not in drop_cols]\n",
    "\n",
    "# 明确要标准化的列\n",
    "\n",
    "NO_Z = [c for c in COV_FEATURES if re.search(r'(?:__rz\\d+$|_cs_z$|_csrank$)', c)]\n",
    "DO_Z = [c for c in COV_FEATURES if c not in NO_Z] + [\"time_pos\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91624ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14793149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1aaf",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d516",
   "metadata": {},
   "source": [
    "### 添加全局时间序列号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "        .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "chunk_size = 30\n",
    "for lo in range(start_date, end_date + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, end_date)\n",
    "    print(f\"processing date range: {lo} ~ {hi}\")\n",
    "    \n",
    "    lf_chunk = lf_data.filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    \n",
    "    lf_grid_chunk = (\n",
    "        grid_df.lazy().filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    )\n",
    "    \n",
    "    lf_joined = (\n",
    "        lf_chunk.join(lf_grid_chunk, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "    \n",
    "    out_path = f\"{container_prefix}/panel_clean_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    \n",
    "    lf_joined.sink_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb09de",
   "metadata": {},
   "source": [
    "## 导入新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11120fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新读入数据\n",
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "data_paths = fs.glob(f\"{container_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths] \n",
    "lf_with_idx = pl.scan_parquet(data_paths, storage_options=storage_options).sort([G_SYM, G_DATE, G_TIME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea261ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lf_with_idx.collect_schema().names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e04edc",
   "metadata": {},
   "source": [
    "## CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_with_idx.select(pl.col(G_DATE)).unique().sort([G_DATE])\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "\n",
    "print(f\"[cv] total {len(folds_by_day)} folds\")\n",
    "\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "print(folds_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5315",
   "metadata": {},
   "source": [
    "## 数据标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d932b1",
   "metadata": {},
   "source": [
    "下面我们以一折为例，验证成功后，再使用多折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13dab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id = 0\n",
    "# 取第一个 fold 的训练集最后一天，作为本折统计 z-score 的上界\n",
    "train_lo, train_hi = folds_by_day[fold_id][0][0], folds_by_day[fold_id][0][-1]\n",
    "val_lo, val_hi = folds_by_day[fold_id][1][0], folds_by_day[fold_id][1][-1]\n",
    "\n",
    "print(f\"统计标准化使用训练集的日期范围 = {train_lo} ~ {train_hi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f563a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集的各特征量的中位数\n",
    "lf_tr = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_median = (\n",
    "    lf_tr\n",
    "    .group_by([G_SYM, \"time_pos\"])\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_st\") for col in COV_FEATURES\n",
    "    ])\n",
    ").sort([G_SYM, \"time_pos\"])\n",
    "\n",
    "glb_median = (\n",
    "    lf_tr\n",
    "    .group_by(\"time_pos\")\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_t\") for col in COV_FEATURES\n",
    "    ])\n",
    ").sort(\"time_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用于本折全部数据 trian + val\n",
    "lf_all = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, val_hi, closed=\"both\"))\n",
    "lf_all = lf_all.join(grp_median, on=[G_SYM, \"time_pos\"], how=\"left\").join(glb_median, on=[\"time_pos\"], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "# 逐列用中位数替换缺失值\n",
    "fill_exprs = []\n",
    "\n",
    "for col in COV_FEATURES:\n",
    "    fill_exprs.append(\n",
    "        pl.coalesce([\n",
    "            pl.col(col),\n",
    "            pl.col(f\"{col}_median_st\"),\n",
    "            pl.col(f\"{col}_median_t\")\n",
    "        ]).alias(col)\n",
    "    )\n",
    "\n",
    "lf_all_imputed = lf_all.with_columns(fill_exprs)\n",
    "\n",
    "# 去掉中位数列\n",
    "drop_cols = [f\"{col}_median_st\" for col in COV_FEATURES]\n",
    "drop_cols += [f\"{col}_median_t\" for col in COV_FEATURES]\n",
    "lf_all_imputed = lf_all_imputed.drop(drop_cols)\n",
    "\n",
    "# 显式检查：若仍有缺失，直接报错\n",
    "remain = lf_all_imputed.select([pl.col(c).is_null().sum().alias(c) for c in COV_FEATURES]).collect()\n",
    "bad = {c: remain[c][0] for c in COV_FEATURES if remain[c][0] > 0}\n",
    "if bad:\n",
    "    raise ValueError(f\"补值失败：以下列在 (symbol_id,time_pos) 和 time_pos 层级都无中位数 -> {bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41652fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Z-score ==========\n",
    "# 开始计算 stats\n",
    "lf_tr_imputed = lf_all_imputed.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_stats = (\n",
    "    lf_tr_imputed\n",
    "    .group_by(G_SYM)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_grp_{c}\") for c in z_cols] +\n",
    "        [pl.col(c).std(ddof=0).alias(f\"std_grp_{c}\") for c in z_cols])\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_stats = (\n",
    "    lf_tr_imputed\n",
    "    .select([pl.col(c).mean().alias(f\"mu_glb_{c}\") for c in z_cols] +\n",
    "            [pl.col(c).std(ddof=0).alias(f\"std_glb_{c}\") for c in z_cols])\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_row = glb_stats.to_dicts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 逐日处理本折所有数据\n",
    "z_prefix = \"az://jackson/js_exp/exp/v1/tft/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "min_std = 1e-5\n",
    "eps = 1e-6\n",
    "# 预处理全局均值/方差的兜底，避免在行级判断\n",
    "glb_mu = {c: glb_row[f\"mu_glb_{c}\"] for c in z_cols}\n",
    "glb_std = {}\n",
    "for c in z_cols:\n",
    "    s = glb_row[f\"std_glb_{c}\"]\n",
    "    if s is None or (isinstance(s, float) and math.isnan(s)) or s <= 0:\n",
    "        s = min_std\n",
    "    glb_std[c] = s\n",
    "\n",
    "for d in range(train_lo, train_lo+1):\n",
    "    lf_day = lf_all_imputed.filter(pl.col(G_DATE) == d)\n",
    "\n",
    "    # 左连分组统计\n",
    "    lf_day_z = lf_day.join(grp_stats.lazy(), on=G_SYM, how=\"left\")\n",
    "    \n",
    "    exprs = []\n",
    "    for c in z_cols:\n",
    "        mu_grp = pl.col(f\"mu_grp_{c}\")\n",
    "        std_grp = pl.col(f\"std_grp_{c}\")\n",
    "        \n",
    "        mu_use = (\n",
    "            pl.when(mu_grp.is_null())\n",
    "            .then(pl.lit(glb_mu[c]))\n",
    "            .otherwise(mu_grp)\n",
    "            .cast(pl.Float32)\n",
    "        )  \n",
    "        std_use = (\n",
    "            pl.when(std_grp.is_null() | (std_grp <= 0))\n",
    "            .then(pl.lit(glb_std[c]))\n",
    "            .otherwise(std_grp)\n",
    "            .cast(pl.Float32)\n",
    "        )\n",
    "        std_use = pl.max_horizontal([std_use, pl.lit(min_std, dtype=pl.Float32)])\n",
    "        \n",
    "        z_raw = ((pl.col(c).cast(pl.Float32) - mu_use) / (std_use + eps))\n",
    "        \n",
    "        z_clipped = pl.min_horizontal([pl.max_horizontal([z_raw, pl.lit(-8.0)]), pl.lit(8.0)]).alias(f\"z_{c}\")\n",
    "\n",
    "        exprs.append(z_clipped)\n",
    "        \n",
    "    lf_day_z = lf_day_z.with_columns(exprs)\n",
    "    \n",
    "    \n",
    "    # 选择并排序\n",
    "    keep = [\"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL,\n",
    "            \"time_bucket\", \"time_sin\", \"time_cos\", *[f\"z_{c}\" for c in z_cols]]\n",
    "    \n",
    " \n",
    "    lf_day_z = lf_day_z.select(keep).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "    \n",
    "    out_path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    lf_day_z.collect(streaming=True).write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "    print(f\"wrote z-scored data for day {d} to {out_path}\")\n",
    "print(f\"[{_now()}] all done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc45f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_paths = fs.glob(f\"{z_prefix}/*.parquet\")\n",
    "z_paths = [f\"az://{p}\" for p in z_paths]\n",
    "lx = pl.scan_parquet(z_paths, storage_options=storage_options).sort([G_SYM, \"time_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "lx.select(pl.col([\"z_feature_26__rz3\"]).max()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lx.select(pl.col([\"z_feature_26__rz3\"]).min()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lx.select(pl.col([\"z_feature_26__rz3\"]).mean()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5dac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lx.select(pl.col([\"z_feature_25__rz30\"]).max()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9585097",
   "metadata": {},
   "outputs": [],
   "source": [
    "lx.select(((pl.col(\"z_feature_26__rz3\").abs() >= 8).mean())\n",
    "          .alias(\"clip_rate_ge8\")).collect()\n",
    "# 经验：< 0.1% 更安心；高的话考虑用 robust/tanh 软裁剪或调小 min_std/做分位数裁剪\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82544e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sample = pl.scan_parquet(\"az://jackson/js_exp/exp/v1/tft/z_shards/z_1597.parquet\", storage_options=storage_options).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "l_sample.limit().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779840e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52252703",
   "metadata": {},
   "outputs": [],
   "source": [
    "overal_stas = lx.select(\n",
    "    [pl.col(f\"z_{c}\").mean().alias(f\"mu_z_{c}\") for c in z_cols] +\n",
    "    [pl.col(f\"z_{c}\").std(ddof=0).alias(f\"std_z_{c}\") for c in z_cols]\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "overal_stas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99852333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = overal_stas.to_pandas().T\n",
    "df_stats = df_stats.rename(columns={df_stats.columns[0]: \"value\"})\n",
    "df_stats['feature'] = df_stats.index\n",
    "df_stats = df_stats[['feature', 'value']]\n",
    "df_stats.reset_index(drop=True, inplace=True)\n",
    "df_stats = df_stats.sort_values(by=\"value\", ascending=False)\n",
    "df_stats.iloc[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.rename(columns={df_stats.columns[0]: \"value\"}, inplace=True)\n",
    "df_stats['feature'] = df_stats.index\n",
    "df_stats = df_stats[['feature', 'value']]\n",
    "df_stats.reset_index(drop=True, inplace=True)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2038473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐日处理本折所有数据\n",
    "\n",
    "z_prefix = \"az://jackson/js_exp/exp/v1/tft/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "eps = 1e-6\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    print(f\"processing date: {d}\")\n",
    "    lf_day = lf_all_imputed.filter(pl.col(G_DATE) == d)\n",
    "    \n",
    "    lf_day_z = (\n",
    "        lf_day.join(glb_mean.lazy(), how=\"cross\").join(grp_mean.lazy(), on=G_SYM, how=\"left\") # 循环内拼接，避免OOM\n",
    "        \n",
    "    ).sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    for c in z_cols:\n",
    "        mu_sym, std_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "        mu_glb, std_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "        mu_use, std_use = f\"mu_{c}_use\", f\"std_{c}_use\"\n",
    "        z_c = f\"z_{c}\"\n",
    "        \n",
    "        lf_day_z = lf_day_z.with_columns([\n",
    "            pl.when(pl.col(mu_sym).is_null()).then(pl.col(mu_glb)).otherwise(pl.col(mu_sym)).alias(mu_use),\n",
    "            pl.when(pl.col(std_sym).is_null() | (pl.col(std_sym) == 0)).then(pl.col(std_glb)).otherwise(pl.col(std_sym)).alias(std_use),   \n",
    "        ]).with_columns(\n",
    "            ((pl.col(c) - pl.col(mu_use)) / (pl.col(std_use) + eps)).alias(z_c)\n",
    "        ).drop([mu_sym, std_sym, mu_glb, std_glb, mu_use, std_use])\n",
    "    \n",
    "    out_cols = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL, \"time_bucket\", \"time_sin\", \"time_cos\", *[f\"z_{c}\" for c in z_cols]] # 这个地方z_cols 含 z_time_pos\n",
    "    lf_day_z = lf_day_z.select([c for c in out_cols if c in lf_day_z.collect_schema().names()])\n",
    "    lf_day_z = lf_day_z.sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    # 写出\n",
    "    out_path = f\"{z_prefix}/z_shard_{d:04d}.parquet\"\n",
    "    \n",
    "    lf_day_z.collect(streaming=True).write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf18c8",
   "metadata": {},
   "source": [
    "## PCA 降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集上计算 PCA\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import joblib\n",
    "\n",
    "z_prefix = \"az://jackson/js_exp/exp/v1/tft/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "pca_cols = [f\"z_{c}\" for c in COV_FEATURES[150:]]\n",
    "keep_cols = [f\"z_{c}\" for c in COV_FEATURES[:150]]\n",
    "\n",
    "max_components = min(100, len(pca_cols))\n",
    "\n",
    "ipca = IncrementalPCA(n_components=max_components)\n",
    "\n",
    "for d in range(train_lo, train_hi + 1):\n",
    "    path = f\"{z_prefix}/z_shard_{d:04d}.parquet\"\n",
    "    print(f\"fitting IPCA on date: {d} from the path: {path}\")\n",
    "    \n",
    "    df_day = pl.read_parquet(path, storage_options=storage_options).select(pca_cols).to_pandas()\n",
    "    X = df_day.to_numpy()\n",
    "    ipca.partial_fit(X)  # 增量拟合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查验 z feature 的分布\n",
    "z_paths = fs.glob(f\"{z_prefix}/*.parquet\")\n",
    "z_paths = [f\"az://{p}\" for p in z_paths]\n",
    "lf_z_all = pl.scan_parquet(z_paths, storage_options=storage_options)\n",
    "\n",
    "dfz = lf_z_all.collect(streaming=True).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21fc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz.iloc[:,:10].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc = dfz.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16977303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc.iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选df_desc 中std>1的列\n",
    "std_row = df_desc.loc[\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_row[std_row > 1].sort_values(ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合完成后，看看累计方差占比，决定真正保留的维度k (95%)\n",
    "cum = ipca.explained_variance_ratio_.cumsum()\n",
    "tau =0.5\n",
    "k = int(np.searchsorted(cum, tau)) + 1\n",
    "\n",
    "print(f\"{k} components explain {cum[k-1]:.4f} of variance\")\n",
    "\n",
    "# 保存模型\n",
    "# —— 裁剪参数到前 k 维（不要重新 new 模型）——\n",
    "ipca.components_              = ipca.components_[:k]\n",
    "ipca.explained_variance_      = ipca.explained_variance_[:k]\n",
    "ipca.explained_variance_ratio_= ipca.explained_variance_ratio_[:k]\n",
    "ipca.singular_values_         = ipca.singular_values_[:k]\n",
    "ipca.n_components_            = k\n",
    "\n",
    "\n",
    "pca_path = f\"{TFT_LOCAL_ROOT}/fold_{fold_id}_ipca_{train_lo}_{train_hi}_k{k}_{_now()}.joblib\"\n",
    "joblib.dump(\n",
    "    {\"ipca\": ipca, \"pca_cols\": pca_cols, \"cum_ratio\": cum, \"chosen_k\": k}, \n",
    "    pca_path\n",
    ")\n",
    "print(f\"IPCA 模型已保存至: {pca_path}\")\n",
    "print(f\"保留主成分{tau}维度 k = {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16efc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本折全部数据进行PCA变换\n",
    "\n",
    "pca_prefix = \"az://jackson/js_exp/exp/v1/tft/feat_pca_shards\"; ensure_dir_az(pca_prefix)\n",
    "meta_cols = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL, \"time_bucket\", \"time_sin\", \"time_cos\", \"z_time_pos\"]\n",
    "\n",
    "pca_k = ipca.n_components_\n",
    "pca_out_cols = [f\"PC{i+1}\" for i in range(pca_k)]\n",
    "\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    in_path = f\"{z_prefix}/z_shard_{d:04d}.parquet\"\n",
    "    out_path = f\"{pca_prefix}/pca_shard_{d:04d}.parquet\"\n",
    "    \n",
    "    lf = pl.read_parquet(in_path, storage_options=storage_options)\n",
    "    \n",
    "    X = (\n",
    "        lf.select([\n",
    "            pl.col(c).cast(pl.Float32) for c in pca_cols\n",
    "        ]).to_numpy()\n",
    "    )\n",
    "    if X.size == 0:\n",
    "        # 当天没有行，直接跳过\n",
    "        continue\n",
    "    if np.isnan(X).any():\n",
    "        raise ValueError(\"PCA transform 输入里仍有 NaN，请检查上游补值/标准化。\")\n",
    "\n",
    "    Z = ipca.transform(X).astype(np.float32)\n",
    "    \n",
    "    # 组装输出\n",
    "    df_pca = pl.DataFrame(Z, schema=pca_out_cols)\n",
    "    lf_out = pl.concat([\n",
    "        lf.select([c for c in meta_cols if c in lf.columns]),\n",
    "        lf.select([c for c in keep_cols if c in lf.columns]),\n",
    "        df_pca\n",
    "    ],\n",
    "    how=\"horizontal\"\n",
    "    ).sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    lf_out.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"written PCA features to: {out_path}\")\n",
    "print(f\"[{_now()}] all done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c98423",
   "metadata": {},
   "source": [
    "标准化pca列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "pca_paths = fs.glob(f\"{pca_prefix}/*.parquet\")\n",
    "pca_paths = [f\"az://{p}\" for p in pca_paths]\n",
    "lf_after_pca = pl.scan_parquet(pca_paths, storage_options=storage_options)\n",
    "\n",
    "# === 1) 计算训练期统计（保证列名一致） ===\n",
    "grp_pca_stats = (\n",
    "    lf_after_pca.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "    .group_by(G_SYM)\n",
    "    .agg(\n",
    "        [pl.col(c).mean().alias(f\"mu_grp_{c}\") for c in pca_out_cols] +\n",
    "        [pl.col(c).std(ddof=0).alias(f\"std_grp_{c}\") for c in pca_out_cols]\n",
    "    )\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_pca_stats = (\n",
    "    lf_after_pca.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "    .select(\n",
    "        [pl.col(c).mean().alias(f\"mu_glb_{c}\") for c in pca_out_cols] +\n",
    "        [pl.col(c).std(ddof=0).alias(f\"std_glb_{c}\") for c in pca_out_cols]\n",
    "    )\n",
    ").collect(streaming=True)\n",
    "\n",
    "# 把全局统计变成字典，后面用 pl.lit() 直接注入，避免 cross join\n",
    "glb_row = glb_pca_stats.to_dicts()[0]  # 单行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2) 逐日处理 ===\n",
    "\n",
    "eps = 1e-6\n",
    "out_cols = meta_cols + keep_cols + [f\"z_{c}\" for c in pca_out_cols]  # 注意：这只包含 z_PC*，不影响你已有 z_time_pos\n",
    "z_pca_done_prefix = \"az://jackson/js_exp/exp/v1/tft/z_pca_done_shards\"; ensure_dir_az(z_pca_done_prefix)\n",
    "\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    lf_day = lf_after_pca.filter(pl.col(G_DATE) == d)\n",
    "\n",
    "    # 左连分组统计\n",
    "    lf_day_z = lf_day.join(grp_pca_stats.lazy(), on=G_SYM, how=\"left\")\n",
    "\n",
    "    # 为每个 PC 列做 z 标准化（组内优先，缺失/0 用全局，且给下限）\n",
    "    for c in pca_out_cols:\n",
    "        mu_expr = (\n",
    "            pl.when(pl.col(f\"mu_grp_{c}\").is_null())\n",
    "              .then(pl.lit(glb_row[f\"mu_glb_{c}\"]))\n",
    "              .otherwise(pl.col(f\"mu_grp_{c}\"))\n",
    "        )\n",
    "        std_expr = (\n",
    "            pl.when(pl.col(f\"std_grp_{c}\").is_null() | (pl.col(f\"std_grp_{c}\") <= 0))\n",
    "              .then(pl.lit(glb_row[f\"std_glb_{c}\"]))\n",
    "              .otherwise(pl.col(f\"std_grp_{c}\"))\n",
    "        )\n",
    "        std_expr = pl.max_horizontal([std_expr, pl.lit(1e-3)])\n",
    "\n",
    "        lf_day_z = lf_day_z.with_columns(\n",
    "            ((pl.col(c) - mu_expr) / (std_expr + eps)).alias(f\"z_{c}\")\n",
    "        )\n",
    "\n",
    "    # 可选重尾裁剪\n",
    "    lf_day_z = lf_day_z.with_columns(\n",
    "        [pl.col(f\"z_{c}\").clip(-8.0, 8.0) for c in pca_out_cols]\n",
    "    )\n",
    "\n",
    "    # 只保留输出列；用 strict=False 忽略缺列；最后再排序\n",
    "    lf_day_z = lf_day_z.select(pl.col(out_cols)).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "    out_path = f\"{z_pca_done_prefix}/z_pca_shard_{d:04d}.parquet\"\n",
    "    lf_day_z.collect(streaming=True).write_parquet(\n",
    "        out_path, storage_options=storage_options, compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"written z-pca to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b907e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按10天合并\n",
    "tft_fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(tft_fold_root)\n",
    "ten_days_prefix = f\"{tft_fold_root}/feat_pca_ten_days_shards\"; ensure_dir_az(ten_days_prefix)\n",
    "\n",
    "chunk_size = 10\n",
    "\n",
    "for lo in range(train_lo, val_hi + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, folds_by_day[fold_id][1][-1])\n",
    "    paths = [f\"{pca_prefix}/pca_shard_{d:04d}.parquet\" for d in range(lo, hi + 1)]\n",
    "    \n",
    "    print(f\"merging date range: {lo} ~ {hi}, num files = {len(paths)}\")\n",
    "    \n",
    "    lf = pl.scan_parquet(paths, storage_options=storage_options)\n",
    "    \n",
    "    df = lf.collect(streaming=True).sort([G_SYM, \"time_idx\"]).rechunk()\n",
    "    \n",
    "    out_path = f\"{ten_days_prefix}/feat_pca_ten_days_chunk_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    df.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028cc3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取处理好的数据\n",
    "tft_fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(tft_fold_root)\n",
    "ten_days_prefix = f\"{tft_fold_root}/feat_pca_ten_days_shards\"; ensure_dir_az(ten_days_prefix)\n",
    "\n",
    "\n",
    "lf_clean = pl.scan_parquet(f\"{ten_days_prefix}/*.parquet\", storage_options=storage_options)\n",
    "lf_clean = lf_clean.sort([G_SYM, \"time_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23112933",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_clean.limit().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_REALS = [c for c in lf_clean.collect_schema().names() if c not in (G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_bucket\", WEIGHT_COL, TARGET_COL)]\n",
    "\n",
    "KNOWN_CATEGORIES = [\"time_bucket\"]\n",
    "\n",
    "UNSCALE_COLS = KNOWN_REALS\n",
    "\n",
    "TRAIN_COLS = [c for c in lf_clean.collect_schema().names() if c not in (G_DATE, G_TIME)]\n",
    "\n",
    "# 定义 identity scalers\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8f841",
   "metadata": {},
   "source": [
    "## try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                          \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in KNOWN_CATEGORIES else None,\n",
    "                          },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction == val_start_idx + ENC_LEN) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c11547",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "# ========== 训练（按 CV 折） ========== 先取第一折\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "#for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "\n",
    "####################################\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[fold_id]\n",
    "####################################\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    lf_clean\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ") \n",
    "\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(str).astype(\"category\")\n",
    "pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(str).astype(\"category\")\n",
    "pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850956a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data.iloc[:,:10].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data.iloc[:,10:20].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1675251",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data.iloc[:,20:30].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data.iloc[:,30:40].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf61d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data.iloc[:,-30:-20].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42002318",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data.iloc[:,-20:-10].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51eb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "# 构建训练集 timeseries dataset\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    pdf_data.loc[pdf_data[\"time_idx\"] <= train_end_idx],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, min_prediction_length=PRED_LEN,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    static_categoricals=[G_SYM],\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                        \"time_bucket\": NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")\n",
    "\n",
    "# 验证集复用 train_ds 的所有 encoders/normalizer（不泄漏）\n",
    "\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    pdf_data.loc[pdf_data[\"time_idx\"].between(val_start_idx, val_end_idx, inclusive=\"both\")],\n",
    "    min_prediction_idx=val_start_idx+ENC_LEN,\n",
    "    stop_randomization=True,\n",
    "    predict=False\n",
    ")\n",
    "\n",
    "#del pdf_data, lf_clean, lf_with_idx; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0412450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE*2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 callbacks/logger/trainer\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, dirpath=ckpt_dir_fold.as_posix(), filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\", save_on_train_epoch_end=False),\n",
    "            ] # LearningRateMonitor(logging_interval=\"step\"),\n",
    "RUN_NAME = f\"quick_check_f{fold_id}_E{MAX_EPOCHS}_lr{LR:g}_bs{BATCH_SIZE}_enc{ENC_LEN}_dec{DEC_LEN}_{datetime.now():%Y%m%d-%H%M%S}\"\n",
    "\n",
    "TEMP_LOG_DIR = Path(\"./tft_logs\")\n",
    "TEMP_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=TEMP_LOG_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=20,\n",
    "                    accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    enable_model_summary=True,\n",
    "                    gradient_clip_val=1.0,\n",
    "                    gradient_clip_algorithm=\"norm\",\n",
    "                    #fast_dev_run=1,\n",
    "                    limit_train_batches=0.3,\n",
    "                    limit_val_batches=0.2,\n",
    "                    val_check_interval= 1.0,\n",
    "                    num_sanity_val_steps=0,\n",
    "                    log_every_n_steps=1000,\n",
    "                    callbacks=callbacks,\n",
    "                    logger=logger,\n",
    "                    #accumulate_grad_batches=1,\n",
    "                    )\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[],\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    reduce_on_plateau_patience=3, \n",
    ")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a0c6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323975ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (torch.square(y_true - y_pred) * w).sum()\n",
    "den = (torch.square(y_true) * w).sum()  \n",
    "wr2 = 1 - num / den\n",
    "print(f\"wr2 after training: {wr2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
