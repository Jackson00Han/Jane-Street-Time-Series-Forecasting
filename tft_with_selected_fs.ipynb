{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2317e57f",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4586e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 07:49:55] imports ok\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "from pipeline.stream_input_local import ShardedBatchStream  \n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c956339",
   "metadata": {},
   "source": [
    "## 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cd586",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96a700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入筛选的特征列\n",
    "df_ranking_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1300-1500__cv3-g7-r4__seed42__top1000__1760299442__range1000-1600__range1000-1600__cv2-g7-r4__1760347190.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "866ec40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5137157718121288)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ranking_features[\"mean_gain\"].iloc[:200].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5763072d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.443650384606832)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ranking_features['mean_gain'].iloc[:150].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed63ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ls_topk_features = df_ranking_features['feature'].tolist()\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "TO_Z_INPUTS = [c for c in ls_topk_features if c not in TIME_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcffd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] ready\n"
     ]
    }
   ],
   "source": [
    "# ========== 1) 统一配置 ==========\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 0\n",
    "TRAIN_TO_VAL = 8\n",
    "ENC_LEN      = 2\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 512\n",
    "LR           = 1e-5\n",
    "HIDDEN       = 16\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.1\n",
    "MAX_EPOCHS   = 30\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "print(\"[config] ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300bbba",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_date = 1500\n",
    "data_end_date = 1600\n",
    "\n",
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *TO_Z_INPUTS])\n",
    "    .filter(pl.col(G_DATE).is_between(data_start_date, data_end_date, closed=\"both\"))\n",
    ")\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1aaf",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d516",
   "metadata": {},
   "source": [
    "### 添加全局时间序列号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "        .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e369c7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing date range: 1500 ~ 1529\n",
      "writing to: az://jackson/js_exp/exp/v1/tft/panel_clean_shards/panel_clean_1500_1529.parquet\n",
      "processing date range: 1530 ~ 1559\n",
      "writing to: az://jackson/js_exp/exp/v1/tft/panel_clean_shards/panel_clean_1530_1559.parquet\n",
      "processing date range: 1560 ~ 1589\n",
      "writing to: az://jackson/js_exp/exp/v1/tft/panel_clean_shards/panel_clean_1560_1589.parquet\n",
      "processing date range: 1590 ~ 1600\n",
      "writing to: az://jackson/js_exp/exp/v1/tft/panel_clean_shards/panel_clean_1590_1600.parquet\n",
      "[2025-10-14 07:54:03] all done\n"
     ]
    }
   ],
   "source": [
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "chunk_size = 30\n",
    "for lo in range(data_start_date, data_end_date, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, data_end_date)\n",
    "    print(f\"processing date range: {lo} ~ {hi}\")\n",
    "    \n",
    "    lf_chunk = lf_data.filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    \n",
    "    lf_grid_chunk = (\n",
    "        grid_df.lazy().filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    )\n",
    "    \n",
    "    lf_joined = (\n",
    "        lf_chunk.join(lf_grid_chunk, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "    \n",
    "    out_path = f\"{container_prefix}/panel_clean_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    \n",
    "    lf_joined.sink_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb09de",
   "metadata": {},
   "source": [
    "### 导入新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11120fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新读入数据\n",
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "data_paths = fs.glob(f\"{container_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths] \n",
    "lf_with_idx = pl.scan_parquet(data_paths, storage_options=storage_options).sort([G_SYM, G_DATE, G_TIME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e187be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 1_010)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>weight</th><th>responder_6</th><th>time_bucket</th><th>time_pos</th><th>time_sin</th><th>time_cos</th><th>feature_06</th><th>feature_36</th><th>feature_04</th><th>feature_75__rstd14</th><th>feature_60__cs_z</th><th>feature_59</th><th>responder_0_close_roll30_std</th><th>feature_59__rstd30</th><th>feature_07</th><th>feature_61__lag900</th><th>feature_60</th><th>feature_61__lag1936</th><th>responder_6_prevday_std</th><th>responder_8_prev_tail_lag10</th><th>feature_61__ret50</th><th>feature_61__lag6776</th><th>feature_25__diff50</th><th>feature_76__rstd7</th><th>feature_48</th><th>responder_5_prevday_std</th><th>feature_60__rstd30</th><th>responder_5_prevday_mean</th><th>feature_51__rmean14</th><th>responder_1_close_roll14_std</th><th>feature_37__rstd30</th><th>responder_2_close_roll30_std</th><th>feature_31__diff50</th><th>feature_58</th><th>&hellip;</th><th>feature_12__lag2904</th><th>feature_67__lag4840</th><th>feature_70__lag6776</th><th>feature_21__lag968</th><th>feature_33__lag3872</th><th>feature_64__rstd30</th><th>feature_47__lag500</th><th>feature_67__lag7744</th><th>feature_75</th><th>responder_4_prev_tail_lag1</th><th>feature_72__lag50</th><th>feature_04__lag5808</th><th>feature_69__lag3872</th><th>feature_67__lag5808</th><th>feature_70__diff50</th><th>feature_01__lag3872</th><th>feature_27__rstd3</th><th>responder_7_same_t_last10_slope</th><th>feature_33__lag4840</th><th>feature_69__lag968</th><th>feature_57__diff50</th><th>feature_69__lag2904</th><th>feature_29__rz14</th><th>feature_27__diff3</th><th>feature_65__lag50</th><th>feature_12__ret50</th><th>feature_00__lag5808</th><th>feature_07__lag4840</th><th>responder_4_same_t_last10_slope</th><th>feature_42__lag100</th><th>feature_65__lag968</th><th>responder_5_prevday_close</th><th>responder_1_same_t_prev3</th><th>responder_2_prevday_close</th><th>feature_30__rstd14</th><th>responder_8_prev_tail_lag1</th><th>time_idx</th></tr><tr><td>i32</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>u8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>1500</td><td>0</td><td>4.426604</td><td>-1.314976</td><td>0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>-0.159566</td><td>-2.922452</td><td>2.150813</td><td>0.649587</td><td>-0.850858</td><td>1.746195</td><td>0.908512</td><td>0.517499</td><td>-0.829518</td><td>-1.370043</td><td>0.641324</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.172714</td><td>0.898012</td><td>-0.224687</td><td>0.665439</td><td>1.725063</td><td>0.434996</td><td>0.832047</td><td>-0.01</td><td>-0.181106</td><td>0.580871</td><td>0.186617</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.665987</td><td>-0.39041</td><td>-1.242651</td><td>-0.050829</td><td>-1.686733</td><td>0.084058</td><td>-0.01854</td><td>-0.803619</td><td>2.612922</td><td>0.165235</td><td>-0.195847</td><td>-0.586194</td><td>0.25757</td><td>-0.84916</td><td>-0.659239</td><td>2.059735</td><td>0.000669</td><td>0.033831</td><td>-0.661166</td><td>-0.461735</td><td>2.671528</td><td>-0.460689</td><td>-0.000516</td><td>0.050152</td><td>0.564481</td><td>-25.604591</td><td>-0.473339</td><td>-2.192881</td><td>-0.053192</td><td>-1.107264</td><td>-1.738099</td><td>0.047745</td><td>0.186933</td><td>-1.005825</td><td>0.0</td><td>0.301072</td><td>0</td></tr><tr><td>0</td><td>1500</td><td>1</td><td>4.426604</td><td>-0.671698</td><td>0</td><td>1.0</td><td>0.006491</td><td>0.999979</td><td>1.069108</td><td>-2.794829</td><td>2.031462</td><td>0.836284</td><td>1.438851</td><td>1.23963</td><td>0.908512</td><td>0.65714</td><td>-0.089751</td><td>-1.370043</td><td>0.988123</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.188648</td><td>0.9046</td><td>-0.224687</td><td>0.759381</td><td>1.759171</td><td>0.434996</td><td>0.841773</td><td>-0.01</td><td>-0.07661</td><td>0.580871</td><td>0.227577</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-1.234799</td><td>-0.530497</td><td>-0.762113</td><td>-0.050829</td><td>-1.686733</td><td>0.084335</td><td>0.029866</td><td>-0.490434</td><td>2.648023</td><td>0.165235</td><td>-0.197776</td><td>-0.426806</td><td>0.752326</td><td>-0.7315</td><td>-0.906033</td><td>2.097947</td><td>0.028961</td><td>0.020192</td><td>-0.661166</td><td>-0.548755</td><td>1.858275</td><td>-0.358871</td><td>-3.2803</td><td>0.053028</td><td>0.827406</td><td>-13.701934</td><td>-1.003718</td><td>-1.360826</td><td>-0.04475</td><td>-0.782763</td><td>-1.273083</td><td>0.047745</td><td>-0.26652</td><td>-1.005825</td><td>0.011843</td><td>0.301072</td><td>1</td></tr><tr><td>0</td><td>1500</td><td>2</td><td>4.426604</td><td>-0.290113</td><td>0</td><td>2.0</td><td>0.012981</td><td>0.999916</td><td>1.138704</td><td>-2.443655</td><td>2.43745</td><td>0.953172</td><td>1.20742</td><td>2.128175</td><td>0.908512</td><td>0.721343</td><td>0.062938</td><td>-1.370043</td><td>1.320846</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.203366</td><td>0.911017</td><td>-0.224687</td><td>0.779975</td><td>1.77037</td><td>0.434996</td><td>0.868895</td><td>-0.01</td><td>0.06524</td><td>0.580871</td><td>0.258462</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-1.222237</td><td>-0.640656</td><td>-1.536348</td><td>-0.050829</td><td>-1.686733</td><td>0.084741</td><td>0.071782</td><td>-0.863921</td><td>2.688747</td><td>0.165235</td><td>-0.120331</td><td>-0.701958</td><td>0.650036</td><td>-0.79736</td><td>-0.655329</td><td>2.145121</td><td>0.029824</td><td>0.034855</td><td>-0.661166</td><td>-0.472839</td><td>2.921851</td><td>-0.397401</td><td>-2.28656</td><td>0.05582</td><td>0.347862</td><td>-16.002048</td><td>-0.64903</td><td>-1.178232</td><td>0.055282</td><td>-0.402001</td><td>-1.885703</td><td>0.047745</td><td>-0.275355</td><td>-1.005825</td><td>0.016092</td><td>0.301072</td><td>2</td></tr><tr><td>0</td><td>1500</td><td>3</td><td>4.426604</td><td>-0.338634</td><td>0</td><td>3.0</td><td>0.019471</td><td>0.99981</td><td>1.024881</td><td>-1.752336</td><td>2.53054</td><td>1.020966</td><td>1.261943</td><td>2.167717</td><td>0.908512</td><td>0.848355</td><td>0.152608</td><td>-1.370043</td><td>1.082904</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.217116</td><td>0.917273</td><td>-0.224687</td><td>0.786966</td><td>1.202953</td><td>0.434996</td><td>0.933084</td><td>-0.01</td><td>0.177734</td><td>0.580871</td><td>0.275996</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.552355</td><td>-0.420501</td><td>-1.087695</td><td>-0.050829</td><td>-1.686733</td><td>0.08198</td><td>0.075735</td><td>-0.724895</td><td>1.87707</td><td>0.165235</td><td>0.084185</td><td>-0.854242</td><td>0.853553</td><td>-0.628053</td><td>-0.861989</td><td>2.174752</td><td>0.002858</td><td>-0.052003</td><td>-0.661166</td><td>-0.430539</td><td>2.603309</td><td>-0.275922</td><td>-1.802911</td><td>0.008386</td><td>0.078301</td><td>-5.068503</td><td>-0.565976</td><td>-0.827309</td><td>-0.091356</td><td>-0.443677</td><td>-1.864025</td><td>0.047745</td><td>-0.336798</td><td>-1.005825</td><td>0.01887</td><td>0.301072</td><td>3</td></tr><tr><td>0</td><td>1500</td><td>4</td><td>4.426604</td><td>-0.530085</td><td>0</td><td>4.0</td><td>0.025961</td><td>0.999663</td><td>1.954541</td><td>-2.193815</td><td>2.395972</td><td>1.006895</td><td>0.862566</td><td>1.732435</td><td>0.908512</td><td>0.956367</td><td>0.468781</td><td>-1.370043</td><td>1.062898</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.23007</td><td>0.92338</td><td>-0.224687</td><td>0.70098</td><td>1.503926</td><td>0.434996</td><td>0.976965</td><td>-0.01</td><td>0.301686</td><td>0.580871</td><td>0.290789</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.8053</td><td>-0.526361</td><td>-0.917775</td><td>-0.050829</td><td>-1.686733</td><td>0.081711</td><td>0.085322</td><td>-0.837613</td><td>1.397569</td><td>0.165235</td><td>0.024061</td><td>-1.390298</td><td>0.704476</td><td>-0.919837</td><td>-0.93792</td><td>2.203427</td><td>0.002768</td><td>-0.05121</td><td>-0.661166</td><td>-0.445658</td><td>2.298639</td><td>-0.501758</td><td>-1.494783</td><td>0.008158</td><td>0.179187</td><td>-8.762663</td><td>-0.335041</td><td>-0.300222</td><td>-0.049125</td><td>-0.331161</td><td>-1.575063</td><td>0.047745</td><td>-0.254777</td><td>-1.005825</td><td>0.020776</td><td>0.301072</td><td>4</td></tr><tr><td>0</td><td>1500</td><td>5</td><td>4.426604</td><td>-0.523631</td><td>0</td><td>5.0</td><td>0.032449</td><td>0.999473</td><td>0.468671</td><td>-1.852185</td><td>2.583213</td><td>0.963734</td><td>0.692108</td><td>0.595211</td><td>0.908512</td><td>1.006524</td><td>0.264905</td><td>-1.370043</td><td>0.967309</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.24235</td><td>0.92935</td><td>-0.224687</td><td>0.777294</td><td>0.468923</td><td>0.434996</td><td>0.998191</td><td>-0.01</td><td>0.280917</td><td>0.580871</td><td>0.299506</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.689993</td><td>-0.419114</td><td>-1.344129</td><td>-0.050829</td><td>-1.686733</td><td>0.087196</td><td>0.188119</td><td>-0.593926</td><td>1.286738</td><td>0.165235</td><td>0.086693</td><td>-1.501489</td><td>0.681864</td><td>-0.95835</td><td>-0.933405</td><td>2.21269</td><td>0.002686</td><td>-0.022412</td><td>-0.661166</td><td>-0.477593</td><td>2.944041</td><td>-0.30584</td><td>-1.271117</td><td>0.007949</td><td>0.100722</td><td>-8.030293</td><td>-0.386803</td><td>-0.174156</td><td>-0.055867</td><td>-0.138473</td><td>-1.542874</td><td>0.047745</td><td>-0.482733</td><td>-1.005825</td><td>0.022037</td><td>0.301072</td><td>5</td></tr><tr><td>0</td><td>1500</td><td>6</td><td>4.426604</td><td>-0.56909</td><td>0</td><td>6.0</td><td>0.038936</td><td>0.999242</td><td>-0.078664</td><td>-1.991771</td><td>3.055325</td><td>0.898562</td><td>0.7424</td><td>0.231122</td><td>0.908512</td><td>1.007153</td><td>-0.005691</td><td>-1.370043</td><td>0.992348</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.254052</td><td>0.93519</td><td>-0.224687</td><td>0.763281</td><td>0.687261</td><td>0.434996</td><td>1.040608</td><td>-0.01</td><td>0.232937</td><td>0.580871</td><td>0.315011</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.809958</td><td>-0.295684</td><td>-0.737493</td><td>-0.050829</td><td>-1.686733</td><td>0.087121</td><td>0.232508</td><td>-0.928018</td><td>0.969025</td><td>0.165235</td><td>0.041407</td><td>-1.753176</td><td>0.946623</td><td>-0.902968</td><td>-0.586024</td><td>2.252421</td><td>0.002612</td><td>0.011225</td><td>-0.661166</td><td>-0.446725</td><td>1.62205</td><td>-0.207938</td><td>-1.09522</td><td>0.007752</td><td>0.390485</td><td>-8.977188</td><td>-0.426779</td><td>0.138501</td><td>0.041062</td><td>-0.145896</td><td>-1.682145</td><td>0.047745</td><td>0.0823</td><td>-1.005825</td><td>0.022759</td><td>0.301072</td><td>6</td></tr><tr><td>0</td><td>1500</td><td>7</td><td>4.426604</td><td>-0.729039</td><td>0</td><td>7.0</td><td>0.045421</td><td>0.998968</td><td>0.10524</td><td>-1.826996</td><td>2.184363</td><td>0.820525</td><td>0.790378</td><td>0.00997</td><td>0.908512</td><td>1.007391</td><td>0.099631</td><td>-1.370043</td><td>0.55643</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.265253</td><td>0.940908</td><td>-0.224687</td><td>0.821404</td><td>0.64151</td><td>0.434996</td><td>1.07784</td><td>-0.01</td><td>0.252241</td><td>0.580871</td><td>0.303542</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.923364</td><td>-0.409874</td><td>-0.965199</td><td>-0.050829</td><td>-1.686733</td><td>0.089139</td><td>0.114082</td><td>-0.596197</td><td>0.425887</td><td>0.165235</td><td>0.051658</td><td>-1.928109</td><td>1.060069</td><td>-0.798959</td><td>-0.629064</td><td>2.285988</td><td>0.002549</td><td>0.057432</td><td>-0.661166</td><td>-0.529096</td><td>2.309217</td><td>-0.141837</td><td>-0.948937</td><td>0.00757</td><td>1.109996</td><td>-6.574678</td><td>-0.868704</td><td>0.113869</td><td>0.13511</td><td>-0.948322</td><td>-1.574444</td><td>0.047745</td><td>-0.035614</td><td>-1.005825</td><td>0.022994</td><td>0.301072</td><td>7</td></tr><tr><td>0</td><td>1500</td><td>8</td><td>4.426604</td><td>-1.032072</td><td>0</td><td>8.0</td><td>0.051904</td><td>0.998652</td><td>-0.864718</td><td>-1.532152</td><td>2.492193</td><td>0.796218</td><td>0.390078</td><td>-0.010016</td><td>0.908512</td><td>1.005947</td><td>-0.286703</td><td>-1.370043</td><td>0.455362</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.27601</td><td>0.946512</td><td>-0.224687</td><td>0.900719</td><td>0.026692</td><td>0.434996</td><td>1.093246</td><td>-0.01</td><td>0.215117</td><td>0.580871</td><td>0.307478</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.821925</td><td>-0.329315</td><td>-1.005486</td><td>-0.050829</td><td>-1.686733</td><td>0.088686</td><td>0.080459</td><td>-0.823782</td><td>0.278089</td><td>0.165235</td><td>0.046512</td><td>-1.890853</td><td>1.151143</td><td>-0.588894</td><td>-0.576537</td><td>2.328777</td><td>0.00249</td><td>0.05335</td><td>-0.661166</td><td>-0.483563</td><td>1.832548</td><td>-0.109526</td><td>-0.821592</td><td>0.007398</td><td>0.43348</td><td>-12.403069</td><td>-0.34293</td><td>0.102463</td><td>0.089992</td><td>-0.154216</td><td>-1.617033</td><td>0.047745</td><td>-0.251343</td><td>-1.005825</td><td>0.022764</td><td>0.301072</td><td>8</td></tr><tr><td>0</td><td>1500</td><td>9</td><td>4.426604</td><td>-0.609382</td><td>0</td><td>9.0</td><td>0.058385</td><td>0.998294</td><td>0.573761</td><td>-1.496487</td><td>2.304881</td><td>0.840634</td><td>0.452221</td><td>0.011837</td><td>0.908512</td><td>1.003755</td><td>0.207682</td><td>-1.370043</td><td>0.542987</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.286369</td><td>0.952008</td><td>-0.224687</td><td>0.891581</td><td>-0.013964</td><td>0.434996</td><td>1.103102</td><td>-0.01</td><td>0.119246</td><td>0.580871</td><td>0.307522</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.758572</td><td>-0.375304</td><td>-0.763938</td><td>-0.050829</td><td>-1.686733</td><td>0.090107</td><td>-0.016491</td><td>-0.794234</td><td>0.120118</td><td>0.165235</td><td>0.011739</td><td>-2.448828</td><td>0.658324</td><td>-1.012865</td><td>-0.7372</td><td>2.357362</td><td>0.002435</td><td>0.0072</td><td>-0.661166</td><td>-0.37065</td><td>2.349816</td><td>-0.044383</td><td>-0.706429</td><td>0.007237</td><td>-0.352614</td><td>-5.183767</td><td>-0.508246</td><td>-0.043192</td><td>0.094173</td><td>-0.396566</td><td>-1.183666</td><td>0.047745</td><td>-0.18378</td><td>-1.005825</td><td>0.022042</td><td>0.301072</td><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 1_010)\n",
       "┌───────────┬─────────┬─────────┬──────────┬───┬─────────────┬─────────────┬────────────┬──────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ weight   ┆ … ┆ responder_2 ┆ feature_30_ ┆ responder_ ┆ time_idx │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ ---      ┆   ┆ _prevday_cl ┆ _rstd14     ┆ 8_prev_tai ┆ ---      │\n",
       "│ i32       ┆ i32     ┆ i32     ┆ f32      ┆   ┆ ose         ┆ ---         ┆ l_lag1     ┆ i64      │\n",
       "│           ┆         ┆         ┆          ┆   ┆ ---         ┆ f32         ┆ ---        ┆          │\n",
       "│           ┆         ┆         ┆          ┆   ┆ f32         ┆             ┆ f32        ┆          │\n",
       "╞═══════════╪═════════╪═════════╪══════════╪═══╪═════════════╪═════════════╪════════════╪══════════╡\n",
       "│ 0         ┆ 1500    ┆ 0       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.0         ┆ 0.301072   ┆ 0        │\n",
       "│ 0         ┆ 1500    ┆ 1       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.011843    ┆ 0.301072   ┆ 1        │\n",
       "│ 0         ┆ 1500    ┆ 2       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.016092    ┆ 0.301072   ┆ 2        │\n",
       "│ 0         ┆ 1500    ┆ 3       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.01887     ┆ 0.301072   ┆ 3        │\n",
       "│ 0         ┆ 1500    ┆ 4       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.020776    ┆ 0.301072   ┆ 4        │\n",
       "│ 0         ┆ 1500    ┆ 5       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.022037    ┆ 0.301072   ┆ 5        │\n",
       "│ 0         ┆ 1500    ┆ 6       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.022759    ┆ 0.301072   ┆ 6        │\n",
       "│ 0         ┆ 1500    ┆ 7       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.022994    ┆ 0.301072   ┆ 7        │\n",
       "│ 0         ┆ 1500    ┆ 8       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.022764    ┆ 0.301072   ┆ 8        │\n",
       "│ 0         ┆ 1500    ┆ 9       ┆ 4.426604 ┆ … ┆ -1.005825   ┆ 0.022042    ┆ 0.301072   ┆ 9        │\n",
       "└───────────┴─────────┴─────────┴──────────┴───┴─────────────┴─────────────┴────────────┴──────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_with_idx.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "640d5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 明确要标准化的列\n",
    "z_cols = [c for c in TO_Z_INPUTS if c in lf_with_idx.collect_schema().names()]\n",
    "\n",
    "# ========== 连续特征清洗 ==========\n",
    "inf2null_exprs  = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c) for c in z_cols]\n",
    "ffill_exprs = [pl.col(c).forward_fill().over(G_SYM).fill_null(0.0).alias(c) for c in z_cols]\n",
    "\n",
    "lf_clean = (\n",
    "    lf_with_idx.with_columns(inf2null_exprs).with_columns(ffill_exprs) #.with_columns(isna_flag_exprs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad4c2ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1_010)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>weight</th><th>responder_6</th><th>time_bucket</th><th>time_pos</th><th>time_sin</th><th>time_cos</th><th>feature_06</th><th>feature_36</th><th>feature_04</th><th>feature_75__rstd14</th><th>feature_60__cs_z</th><th>feature_59</th><th>responder_0_close_roll30_std</th><th>feature_59__rstd30</th><th>feature_07</th><th>feature_61__lag900</th><th>feature_60</th><th>feature_61__lag1936</th><th>responder_6_prevday_std</th><th>responder_8_prev_tail_lag10</th><th>feature_61__ret50</th><th>feature_61__lag6776</th><th>feature_25__diff50</th><th>feature_76__rstd7</th><th>feature_48</th><th>responder_5_prevday_std</th><th>feature_60__rstd30</th><th>responder_5_prevday_mean</th><th>feature_51__rmean14</th><th>responder_1_close_roll14_std</th><th>feature_37__rstd30</th><th>responder_2_close_roll30_std</th><th>feature_31__diff50</th><th>feature_58</th><th>&hellip;</th><th>feature_12__lag2904</th><th>feature_67__lag4840</th><th>feature_70__lag6776</th><th>feature_21__lag968</th><th>feature_33__lag3872</th><th>feature_64__rstd30</th><th>feature_47__lag500</th><th>feature_67__lag7744</th><th>feature_75</th><th>responder_4_prev_tail_lag1</th><th>feature_72__lag50</th><th>feature_04__lag5808</th><th>feature_69__lag3872</th><th>feature_67__lag5808</th><th>feature_70__diff50</th><th>feature_01__lag3872</th><th>feature_27__rstd3</th><th>responder_7_same_t_last10_slope</th><th>feature_33__lag4840</th><th>feature_69__lag968</th><th>feature_57__diff50</th><th>feature_69__lag2904</th><th>feature_29__rz14</th><th>feature_27__diff3</th><th>feature_65__lag50</th><th>feature_12__ret50</th><th>feature_00__lag5808</th><th>feature_07__lag4840</th><th>responder_4_same_t_last10_slope</th><th>feature_42__lag100</th><th>feature_65__lag968</th><th>responder_5_prevday_close</th><th>responder_1_same_t_prev3</th><th>responder_2_prevday_close</th><th>feature_30__rstd14</th><th>responder_8_prev_tail_lag1</th><th>time_idx</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>&hellip;</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1_010)\n",
       "┌───────────┬─────────┬─────────┬────────┬───┬──────────────┬─────────────┬─────────────┬──────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ weight ┆ … ┆ responder_2_ ┆ feature_30_ ┆ responder_8 ┆ time_idx │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ ---    ┆   ┆ prevday_clos ┆ _rstd14     ┆ _prev_tail_ ┆ ---      │\n",
       "│ u32       ┆ u32     ┆ u32     ┆ u32    ┆   ┆ e            ┆ ---         ┆ lag1        ┆ u32      │\n",
       "│           ┆         ┆         ┆        ┆   ┆ ---          ┆ u32         ┆ ---         ┆          │\n",
       "│           ┆         ┆         ┆        ┆   ┆ u32          ┆             ┆ u32         ┆          │\n",
       "╞═══════════╪═════════╪═════════╪════════╪═══╪══════════════╪═════════════╪═════════════╪══════════╡\n",
       "│ 0         ┆ 0       ┆ 0       ┆ 0      ┆ … ┆ 0            ┆ 0           ┆ 0           ┆ 0        │\n",
       "└───────────┴─────────┴─────────┴────────┴───┴──────────────┴─────────────┴─────────────┴──────────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_clean.select(pl.all().is_null().sum()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e04edc",
   "metadata": {},
   "source": [
    "## CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a016c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_clean.select(pl.col(G_DATE)).unique().sort([G_DATE])\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5315",
   "metadata": {},
   "source": [
    "## 数据标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41652fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化使用训练集的上限日期 = 1583\n"
     ]
    }
   ],
   "source": [
    "fold_id = 0\n",
    "# 取第一个 fold 的训练集最后一天，作为统计 z-score 的上界\n",
    "stats_hi = int(folds_by_day[fold_id][0][-1])\n",
    "print(f\"标准化使用训练集的上限日期 = {stats_hi}\")\n",
    "\n",
    "# ========== Z-score ==========\n",
    "# 开始计算 stats\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .group_by(G_SYM)\n",
    "            .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in z_cols] +\n",
    "                [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in z_cols])\n",
    ").collect(streaming=True)\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in z_cols] +\n",
    "                    [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in z_cols])\n",
    ").collect(streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fd348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2038473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing date: 1500\n",
      "processing date: 1501\n",
      "processing date: 1502\n",
      "processing date: 1503\n",
      "processing date: 1504\n",
      "processing date: 1505\n",
      "processing date: 1506\n",
      "processing date: 1507\n",
      "processing date: 1508\n",
      "processing date: 1509\n",
      "processing date: 1510\n",
      "processing date: 1511\n",
      "processing date: 1512\n",
      "processing date: 1513\n",
      "processing date: 1514\n",
      "processing date: 1515\n",
      "processing date: 1516\n",
      "processing date: 1517\n",
      "processing date: 1518\n",
      "processing date: 1519\n",
      "processing date: 1520\n",
      "processing date: 1521\n",
      "processing date: 1522\n",
      "processing date: 1523\n",
      "processing date: 1524\n",
      "processing date: 1525\n",
      "processing date: 1526\n",
      "processing date: 1527\n",
      "processing date: 1528\n",
      "processing date: 1529\n",
      "processing date: 1530\n",
      "processing date: 1531\n",
      "processing date: 1532\n",
      "processing date: 1533\n",
      "processing date: 1534\n",
      "processing date: 1535\n",
      "processing date: 1536\n",
      "processing date: 1537\n",
      "processing date: 1538\n",
      "processing date: 1539\n",
      "processing date: 1540\n",
      "processing date: 1541\n",
      "processing date: 1542\n",
      "processing date: 1543\n",
      "processing date: 1544\n",
      "processing date: 1545\n",
      "processing date: 1546\n",
      "processing date: 1547\n",
      "processing date: 1548\n",
      "processing date: 1549\n",
      "processing date: 1550\n",
      "processing date: 1551\n",
      "processing date: 1552\n",
      "processing date: 1553\n",
      "processing date: 1554\n",
      "processing date: 1555\n",
      "processing date: 1556\n",
      "processing date: 1557\n",
      "processing date: 1558\n",
      "processing date: 1559\n",
      "processing date: 1560\n",
      "processing date: 1561\n",
      "processing date: 1562\n",
      "processing date: 1563\n",
      "processing date: 1564\n",
      "processing date: 1565\n",
      "processing date: 1566\n",
      "processing date: 1567\n",
      "processing date: 1568\n",
      "processing date: 1569\n",
      "processing date: 1570\n",
      "processing date: 1571\n",
      "processing date: 1572\n",
      "processing date: 1573\n",
      "processing date: 1574\n",
      "processing date: 1575\n",
      "processing date: 1576\n",
      "processing date: 1577\n",
      "processing date: 1578\n",
      "processing date: 1579\n",
      "processing date: 1580\n",
      "processing date: 1581\n",
      "processing date: 1582\n",
      "processing date: 1583\n",
      "processing date: 1584\n",
      "processing date: 1585\n",
      "processing date: 1586\n",
      "processing date: 1587\n",
      "processing date: 1588\n",
      "processing date: 1589\n",
      "processing date: 1590\n",
      "processing date: 1591\n",
      "processing date: 1592\n",
      "processing date: 1593\n",
      "processing date: 1594\n",
      "processing date: 1595\n",
      "processing date: 1596\n",
      "processing date: 1597\n",
      "processing date: 1598\n",
      "processing date: 1599\n",
      "processing date: 1600\n"
     ]
    }
   ],
   "source": [
    "# 逐日处理\n",
    "\n",
    "z_prefix = \"az://jackson/js_exp/exp/v1/tft/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "eps = 1e-6\n",
    "for d in all_days:\n",
    "    print(f\"processing date: {d}\")\n",
    "    lf_day = lf_clean.filter(pl.col(G_DATE) == d)\n",
    "    \n",
    "    lf_day_z = (\n",
    "        lf_day.join(lf_stats_glb.lazy(), how=\"cross\").join(lf_stats_sym.lazy(), on=G_SYM, how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "        \n",
    "    )\n",
    "    for c in z_cols:\n",
    "        mu_sym, std_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "        mu_glb, std_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "        mu_use, std_use = f\"mu_{c}_use\", f\"std_{c}_use\"\n",
    "        z_c = f\"z_{c}\"\n",
    "        \n",
    "        lf_day_z = lf_day_z.with_columns([\n",
    "            pl.when(pl.col(mu_sym).is_null()).then(pl.col(mu_glb)).otherwise(pl.col(mu_sym)).alias(mu_use),\n",
    "            pl.when(pl.col(std_sym).is_null() | (pl.col(std_sym) == 0)).then(pl.col(std_glb)).otherwise(pl.col(std_sym)).alias(std_use),   \n",
    "        ]).with_columns(\n",
    "            ((pl.col(c) - pl.col(mu_use)) / (pl.col(std_use) + eps)).alias(z_c)\n",
    "        ).drop([mu_sym, std_sym, mu_glb, std_glb, mu_use, std_use])\n",
    "    \n",
    "    out_cols = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *[f\"z_{c}\" for c in z_cols]] \n",
    "    lf_day_z = lf_day_z.select([c for c in out_cols if c in lf_day_z.collect_schema().names()])\n",
    "    lf_day_z = lf_day_z.sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    # 写出\n",
    "    out_path = f\"{z_prefix}/z_shard_{d:04d}.parquet\"\n",
    "    \n",
    "    lf_day_z.collect(streaming=True).write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fd72919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一折训练集日期区间 = [1500, 1583]\n"
     ]
    }
   ],
   "source": [
    "# 明确第一折训练日期区间\n",
    "train_start_date = int(folds_by_day[fold_id][0][0])\n",
    "train_end_date   = int(folds_by_day[fold_id][0][-1])\n",
    "print(f\"第一折训练集日期区间 = [{train_start_date}, {train_end_date}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6a335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting IPCA on date: 1500, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1500.parquet\n",
      "fitting IPCA on date: 1501, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1501.parquet\n",
      "fitting IPCA on date: 1502, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1502.parquet\n",
      "fitting IPCA on date: 1503, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1503.parquet\n",
      "fitting IPCA on date: 1504, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1504.parquet\n",
      "fitting IPCA on date: 1505, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1505.parquet\n",
      "fitting IPCA on date: 1506, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1506.parquet\n",
      "fitting IPCA on date: 1507, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1507.parquet\n",
      "fitting IPCA on date: 1508, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1508.parquet\n",
      "fitting IPCA on date: 1509, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1509.parquet\n",
      "fitting IPCA on date: 1510, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1510.parquet\n",
      "fitting IPCA on date: 1511, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1511.parquet\n",
      "fitting IPCA on date: 1512, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1512.parquet\n",
      "fitting IPCA on date: 1513, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1513.parquet\n",
      "fitting IPCA on date: 1514, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1514.parquet\n",
      "fitting IPCA on date: 1515, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1515.parquet\n",
      "fitting IPCA on date: 1516, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1516.parquet\n",
      "fitting IPCA on date: 1517, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1517.parquet\n",
      "fitting IPCA on date: 1518, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1518.parquet\n",
      "fitting IPCA on date: 1519, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1519.parquet\n",
      "fitting IPCA on date: 1520, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1520.parquet\n",
      "fitting IPCA on date: 1521, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1521.parquet\n",
      "fitting IPCA on date: 1522, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1522.parquet\n",
      "fitting IPCA on date: 1523, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1523.parquet\n",
      "fitting IPCA on date: 1524, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1524.parquet\n",
      "fitting IPCA on date: 1525, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1525.parquet\n",
      "fitting IPCA on date: 1526, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1526.parquet\n",
      "fitting IPCA on date: 1527, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1527.parquet\n",
      "fitting IPCA on date: 1528, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1528.parquet\n",
      "fitting IPCA on date: 1529, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1529.parquet\n",
      "fitting IPCA on date: 1530, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1530.parquet\n",
      "fitting IPCA on date: 1531, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1531.parquet\n",
      "fitting IPCA on date: 1532, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1532.parquet\n",
      "fitting IPCA on date: 1533, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1533.parquet\n",
      "fitting IPCA on date: 1534, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1534.parquet\n",
      "fitting IPCA on date: 1535, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1535.parquet\n",
      "fitting IPCA on date: 1536, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1536.parquet\n",
      "fitting IPCA on date: 1537, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1537.parquet\n",
      "fitting IPCA on date: 1538, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1538.parquet\n",
      "fitting IPCA on date: 1539, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1539.parquet\n",
      "fitting IPCA on date: 1540, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1540.parquet\n",
      "fitting IPCA on date: 1541, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1541.parquet\n",
      "fitting IPCA on date: 1542, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1542.parquet\n",
      "fitting IPCA on date: 1543, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1543.parquet\n",
      "fitting IPCA on date: 1544, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1544.parquet\n",
      "fitting IPCA on date: 1545, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1545.parquet\n",
      "fitting IPCA on date: 1546, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1546.parquet\n",
      "fitting IPCA on date: 1547, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1547.parquet\n",
      "fitting IPCA on date: 1548, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1548.parquet\n",
      "fitting IPCA on date: 1549, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1549.parquet\n",
      "fitting IPCA on date: 1550, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1550.parquet\n",
      "fitting IPCA on date: 1551, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1551.parquet\n",
      "fitting IPCA on date: 1552, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1552.parquet\n",
      "fitting IPCA on date: 1553, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1553.parquet\n",
      "fitting IPCA on date: 1554, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1554.parquet\n",
      "fitting IPCA on date: 1555, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1555.parquet\n",
      "fitting IPCA on date: 1556, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1556.parquet\n",
      "fitting IPCA on date: 1557, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1557.parquet\n",
      "fitting IPCA on date: 1558, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1558.parquet\n",
      "fitting IPCA on date: 1559, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1559.parquet\n",
      "fitting IPCA on date: 1560, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1560.parquet\n",
      "fitting IPCA on date: 1561, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1561.parquet\n",
      "fitting IPCA on date: 1562, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1562.parquet\n",
      "fitting IPCA on date: 1563, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1563.parquet\n",
      "fitting IPCA on date: 1564, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1564.parquet\n",
      "fitting IPCA on date: 1565, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1565.parquet\n",
      "fitting IPCA on date: 1566, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1566.parquet\n",
      "fitting IPCA on date: 1567, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1567.parquet\n",
      "fitting IPCA on date: 1568, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1568.parquet\n",
      "fitting IPCA on date: 1569, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1569.parquet\n",
      "fitting IPCA on date: 1570, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1570.parquet\n",
      "fitting IPCA on date: 1571, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1571.parquet\n",
      "fitting IPCA on date: 1572, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1572.parquet\n",
      "fitting IPCA on date: 1573, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1573.parquet\n",
      "fitting IPCA on date: 1574, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1574.parquet\n",
      "fitting IPCA on date: 1575, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1575.parquet\n",
      "fitting IPCA on date: 1576, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1576.parquet\n",
      "fitting IPCA on date: 1577, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1577.parquet\n",
      "fitting IPCA on date: 1578, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1578.parquet\n",
      "fitting IPCA on date: 1579, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1579.parquet\n",
      "fitting IPCA on date: 1580, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1580.parquet\n",
      "fitting IPCA on date: 1581, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1581.parquet\n",
      "fitting IPCA on date: 1582, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1582.parquet\n",
      "fitting IPCA on date: 1583, path: az://jackson/js_exp/exp/v1/tft/z_shards/z_shard_1583.parquet\n"
     ]
    }
   ],
   "source": [
    "# Incremental PCA\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import joblib\n",
    "\n",
    "pca_cols = [f\"z_{c}\" for c in z_cols[100:]]\n",
    "keep_cols = [f\"z_{c}\" for c in z_cols[:100]]\n",
    "max_components = min(100, len(pca_cols))\n",
    "\n",
    "ipca = IncrementalPCA(n_components=max_components)\n",
    "\n",
    "for d in range(train_start_date, train_end_date + 1):\n",
    "    path = f\"{z_prefix}/z_shard_{d:04d}.parquet\"\n",
    "    print(f\"fitting IPCA on date: {d}, path: {path}\")\n",
    "    \n",
    "    df_day = pl.read_parquet(path, storage_options=storage_options).select(pca_cols).to_pandas()\n",
    "    df_day.fillna(0.0, inplace=True)\n",
    "    X = df_day.to_numpy()\n",
    "    ipca.partial_fit(X)  # 增量拟合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d51f1ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 拟合完成后，看看累计方差占比，决定真正保留的维度k (95%)\n",
    "cum = ipca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "k = int(np.searchsorted(cum, 0.5)) + 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abbc6884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.searchsorted(cum, 0.65)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63242dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "ipca.components_ = IncrementalPCA(n_components=k)\n",
    "ipca.mean_ = ipca.mean_ # 保持不变\n",
    "ipca.n_components_ = k\n",
    "ipca.explained_variance_ = ipca.explained_variance_ratio_[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c7c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dict = f\"{TFT_LOCAL_ROOT}/ipca_{train_start_date}_{train_end_date}_k{k}_{_now()}.joblib\"\n",
    "joblib.dump(\n",
    "    {\"ipca\": ipca, \"pca_cols\": pca_cols, \"cum_ratio\": cum, \"chosen_k\": k}, \n",
    "    pca_dict\n",
    ")\n",
    "print(f\"IPCA 模型已保存至: {pca_dict}\")\n",
    "print(f\"保留主成分维度 k = {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a798e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b907e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按7天合并\n",
    "ten_days_prefix = \"az://jackson/js_exp/exp/v1/tft/z_ten_chunks\"; ensure_dir_az(ten_days_prefix)\n",
    "\n",
    "chunk_size = 10\n",
    "data_start_date = 1400\n",
    "data_end_date = 1600\n",
    "for lo in range(data_start_date, data_end_date+1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, data_end_date)\n",
    "    paths = [f\"{z_prefix}/z_shard_{d:04d}.parquet\" for d in range(lo, hi + 1)]\n",
    "    print(f\"merging date range: {lo} ~ {hi}, num files = {len(paths)}\")\n",
    "    \n",
    "    lf = pl.scan_parquet(paths, storage_options=storage_options)\n",
    "    \n",
    "    df = lf.collect(streaming=True).sort([G_SYM, \"time_idx\"]).rechunk()\n",
    "    \n",
    "    out_path = f\"{ten_days_prefix}/z__chunk_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    df.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028cc3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_REALS = [c for c in lf_clean.collect_schema().names() if c not in (G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_bucket\", WEIGHT_COL, TARGET_COL)]\n",
    "\n",
    "KNOWN_CATEGORIES = [\"time_bucket\"] if 'time_bucket' in lf_clean.collect_schema().names() else []\n",
    "\n",
    "UNSCALE_COLS = KNOWN_REALS\n",
    "\n",
    "TRAIN_COLS = [c for c in lf_clean.collect_schema().names() if c not in (G_DATE, G_TIME)]\n",
    "\n",
    "# 定义 identity scalers\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8f841",
   "metadata": {},
   "source": [
    "## try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                          \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in KNOWN_CATEGORIES else None,\n",
    "                          },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction == val_start_idx + ENC_LEN) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c11547",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "# ========== 8) 训练（按 CV 折） ========== 先取第一折\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "#for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "####################################\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[fold_id]\n",
    "####################################\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    lf_clean\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ") \n",
    "\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51eb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建训练集 timeseries dataset\n",
    "\n",
    "t_data = pdf_data[TRAIN_COLS]\n",
    "train_df = t_data.loc[t_data[\"time_idx\"] <= train_end_idx]\n",
    "\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, min_prediction_length=PRED_LEN,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    static_categoricals=[G_SYM],\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                        \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in pdf_data.columns else None},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a95f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 验证集复用 train_ds 的所有 encoders/normalizer（不泄漏）\n",
    "\n",
    "val_df = t_data.loc[t_data[\"time_idx\"].between(val_start_idx, val_end_idx, inclusive=\"both\")]\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    val_df,\n",
    "    min_prediction_idx=val_start_idx+ENC_LEN,\n",
    "    stop_randomization=True,\n",
    "    predict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0412450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE*4,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 callbacks/logger/trainer\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, check_on_train_epoch_end=False),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, dirpath=ckpt_dir_fold.as_posix(), filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\", save_on_train_epoch_end=False),\n",
    "            LearningRateMonitor(logging_interval=\"step\"),]\n",
    "RUN_NAME = (f\"f{fold_id}\"f\"_E{MAX_EPOCHS}\"f\"_lr{LR}\"f\"_bs{BATCH_SIZE}\"f\"_enc{ENC_LEN}_dec{DEC_LEN}\"f\"_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "logger = TensorBoardLogger(save_dir=LOGS_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=3,\n",
    "                    accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    enable_model_summary=True,\n",
    "                    gradient_clip_val=1.0,\n",
    "                    gradient_clip_algorithm=\"norm\",\n",
    "                    fast_dev_run=False,\n",
    "                    limit_train_batches=50,\n",
    "                    limit_val_batches=25,\n",
    "                    val_check_interval=0.25,\n",
    "                    num_sanity_val_steps=2,\n",
    "                    log_every_n_steps=10,\n",
    "                    callbacks=callbacks,\n",
    "                    logger=logger,\n",
    "                    accumulate_grad_batches=1,\n",
    "                    deterministic=False\n",
    "                    )\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[RMSE()],\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (torch.square(y_true - y_pred) * w).sum()\n",
    "den = (torch.square(y_true) * w).sum()  \n",
    "wr2 = 1 - num / den\n",
    "print(f\"wr2 after training: {wr2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
