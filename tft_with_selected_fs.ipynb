{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2317e57f",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4586e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-20 20:13:05] imports ok\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "from pipeline.stream_input_local import ShardedBatchStream  \n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c956339",
   "metadata": {},
   "source": [
    "## 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cd586",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c7e58",
   "metadata": {},
   "source": [
    "读入选出的特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64883d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入筛选的所有特征列\n",
    "\n",
    "path = Path(\"/mnt/data/js/exp/v1/models/tune/selected_covariant_features.txt\")\n",
    "filted_cols = path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "selected_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1400-1698__cv2-g7-r4__seed42__top1000__1760906660__range830-1698__range830-1698__cv2-g7-r4__1760912739.csv\")\n",
    "\n",
    "df_cov_cols = selected_features[selected_features['feature'].isin(filted_cols)].copy()\n",
    "\n",
    "# 我们这里重新归一化一下\n",
    "df_cov_cols[\"mean_gain\"] = (df_cov_cols['mean_gain'] / df_cov_cols['mean_gain'].sum()).astype(np.float32)\n",
    "df_e_features = df_cov_cols.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] ready\n"
     ]
    }
   ],
   "source": [
    "# ========== 1) 初始化配置 ==========\n",
    "\n",
    "# 所有列\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "COV_FEATURES = df_e_features['feature'].tolist() # 含有常数列\n",
    "\n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 0\n",
    "TRAIN_TO_VAL = 8\n",
    "ENC_LEN      = 10\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 512\n",
    "LR           = 1e-3\n",
    "HIDDEN       = 32\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.2\n",
    "MAX_EPOCHS   = 30\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "\n",
    "start_date, end_date = (1500, 1560)  # 仅用于本次实验\n",
    "\n",
    "\n",
    "print(\"[config] ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481d04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需不需要标准化\n",
    "already_z_cols = [c for c in COV_FEATURES if any(k in c for k in [\"__cs_z\", \"__rz\", \"__csrank\"])]\n",
    "no_z_grp_static_cols = [c for c in [\"feature_09\", \"feature_10\", \"feature_11\"] if c in COV_FEATURES]\n",
    "do_z_cols = [c for c in COV_FEATURES if c not in (already_z_cols + no_z_grp_static_cols)] + [\"time_pos\"]\n",
    "\n",
    "# 全部列： G_SYM, G_DATE, G_TIME, TARGET_COL, WEIGHT_COL, TIME_FEATURES, COV_FEATURES\n",
    "# 全部列： G_SYM, G_DATE, G_TIME, TARGET_COL, WEIGHT_COL, “time_index”, \"time_sin\", \"time_cos\", \"time_bucket\", no_z_grp_static_cols, already_z_cols, do_z_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300bbba",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *COV_FEATURES])\n",
    "    .filter(pl.col(G_DATE).is_between(start_date, end_date, closed=\"both\"))\n",
    ")\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1aaf",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d516",
   "metadata": {},
   "source": [
    "### 添加全局时间序列号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "        .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "chunk_size = 30\n",
    "for lo in range(start_date, end_date + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, end_date)\n",
    "    print(f\"processing date range: {lo} ~ {hi}\")\n",
    "    \n",
    "    lf_chunk = lf_data.filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    \n",
    "    lf_grid_chunk = (\n",
    "        grid_df.lazy().filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    )\n",
    "    \n",
    "    lf_joined = (\n",
    "        lf_chunk.join(lf_grid_chunk, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "    \n",
    "    out_path = f\"{container_prefix}/panel_clean_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    \n",
    "    lf_joined.sink_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb09de",
   "metadata": {},
   "source": [
    "## 导入新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11120fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新读入数据\n",
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "data_paths = fs.glob(f\"{container_prefix}/*.parquet\")\n",
    "data_paths = [f\"az://{p}\" for p in data_paths] \n",
    "lf_with_idx = pl.scan_parquet(data_paths, storage_options=storage_options).sort([G_SYM, G_DATE, G_TIME])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e04edc",
   "metadata": {},
   "source": [
    "## CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a016c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cv] total 1 folds\n",
      "[(array([1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540,\n",
      "       1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551,\n",
      "       1552, 1553], dtype=int32), array([1554, 1555, 1556, 1557], dtype=int32))]\n"
     ]
    }
   ],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_with_idx.select(pl.col(G_DATE)).unique().sort([G_DATE])\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "\n",
    "print(f\"[cv] total {len(folds_by_day)} folds\")\n",
    "\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "print(folds_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5315",
   "metadata": {},
   "source": [
    "## 数据标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d932b1",
   "metadata": {},
   "source": [
    "下面我们以一折为例，验证成功后，再使用多折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13dab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计标准化使用训练集的日期范围 = 1530 ~ 1553\n"
     ]
    }
   ],
   "source": [
    "fold_id = 0\n",
    "# 取第一个 fold 的训练集最后一天，作为本折统计 z-score 的上界\n",
    "train_lo, train_hi = folds_by_day[fold_id][0][0], folds_by_day[fold_id][0][-1]\n",
    "val_lo, val_hi = folds_by_day[fold_id][1][0], folds_by_day[fold_id][1][-1]\n",
    "\n",
    "print(f\"统计标准化使用训练集的日期范围 = {train_lo} ~ {train_hi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f563a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集的各特征量的中位数\n",
    "lf_tr = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_median = (\n",
    "    lf_tr\n",
    "    .group_by([G_SYM, \"time_pos\"])\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_st\") for col in COV_FEATURES\n",
    "    ])\n",
    ").sort([G_SYM, \"time_pos\"])\n",
    "\n",
    "glb_median = (\n",
    "    lf_tr\n",
    "    .group_by(\"time_pos\")\n",
    "    .agg([\n",
    "        pl.col(col).median().alias(f\"{col}_median_t\") for col in COV_FEATURES\n",
    "    ])\n",
    ").sort(\"time_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1dfd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用于本折全部数据 trian + val\n",
    "lf_all = lf_with_idx.filter(pl.col(G_DATE).is_between(train_lo, val_hi, closed=\"both\"))\n",
    "lf_all = lf_all.join(grp_median, on=[G_SYM, \"time_pos\"], how=\"left\").join(glb_median, on=[\"time_pos\"], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "# 逐列用中位数替换缺失值\n",
    "fill_exprs = []\n",
    "\n",
    "for col in COV_FEATURES:\n",
    "    fill_exprs.append(\n",
    "        pl.coalesce([\n",
    "            pl.col(col),\n",
    "            pl.col(f\"{col}_median_st\"),\n",
    "            pl.col(f\"{col}_median_t\")\n",
    "        ]).alias(col)\n",
    "    )\n",
    "\n",
    "lf_all_imputed = lf_all.with_columns(fill_exprs)\n",
    "\n",
    "# 去掉中位数列\n",
    "drop_cols = [f\"{col}_median_st\" for col in COV_FEATURES]\n",
    "drop_cols += [f\"{col}_median_t\" for col in COV_FEATURES]\n",
    "lf_all_imputed = lf_all_imputed.drop(drop_cols)\n",
    "\n",
    "# 显式检查：若仍有缺失，直接报错\n",
    "remain = lf_all_imputed.select([pl.col(c).is_null().sum().alias(c) for c in COV_FEATURES]).collect()\n",
    "bad = {c: remain[c][0] for c in COV_FEATURES if remain[c][0] > 0}\n",
    "if bad:\n",
    "    raise ValueError(f\"补值失败：以下列在 (symbol_id,time_pos) 和 time_pos 层级都无中位数 -> {bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41652fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Z-score ==========\n",
    "# 开始计算 stats\n",
    "lf_tr_imputed = lf_all_imputed.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "grp_stats = (\n",
    "    lf_tr_imputed\n",
    "    .group_by(G_SYM)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_grp_{c}\") for c in do_z_cols] +\n",
    "        [pl.col(c).std(ddof=1).alias(f\"std_grp_{c}\") for c in do_z_cols])\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_stats = (\n",
    "    lf_tr_imputed\n",
    "    .select([pl.col(c).mean().alias(f\"mu_glb_{c}\") for c in do_z_cols] +\n",
    "            [pl.col(c).std(ddof=1).alias(f\"std_glb_{c}\") for c in do_z_cols])\n",
    ").collect(streaming=True)\n",
    "\n",
    "glb_row = glb_stats.to_dicts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 逐日处理本折所有数据\n",
    "z_prefix = \"az://jackson/js_exp/exp/v1/tft/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "min_std = 1e-5\n",
    "eps = 1e-6\n",
    "# 预处理全局均值/方差的兜底，避免在行级判断\n",
    "glb_mu = {c: glb_row[f\"mu_glb_{c}\"] for c in do_z_cols}\n",
    "glb_std = {}\n",
    "for c in do_z_cols:\n",
    "    s = glb_row[f\"std_glb_{c}\"]\n",
    "    if s is None or (isinstance(s, float) and math.isnan(s)) or s <= 0:\n",
    "        s = min_std\n",
    "    glb_std[c] = s\n",
    "z_done_cols = [f\"z_{c}\" for c in do_z_cols]\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    lf_day = lf_all_imputed.filter(pl.col(G_DATE) == d)\n",
    "\n",
    "    lf_day_z = lf_day.join(grp_stats.lazy(), on=G_SYM, how=\"left\")\n",
    "\n",
    "    # 1) 先把所有“常量”变成具名列，避免 anonymous literal\n",
    "    const_exprs = []\n",
    "    for c in do_z_cols:\n",
    "        const_exprs += [\n",
    "            pl.lit(glb_mu[c]).alias(f\"__mu_glb_{c}\"),\n",
    "            pl.lit(glb_std[c] if glb_std[c] > 0 else min_std).alias(f\"__std_glb_{c}\"),\n",
    "        ]\n",
    "    lf_day_z = lf_day_z.with_columns(const_exprs)\n",
    "\n",
    "    # 2) 计算 z，并用 clip 裁剪\n",
    "    exprs = []\n",
    "    for c in do_z_cols:\n",
    "        mu_grp  = pl.col(f\"mu_grp_{c}\")\n",
    "        std_grp = pl.col(f\"std_grp_{c}\")\n",
    "        mu_use = pl.coalesce([mu_grp, pl.col(f\"__mu_glb_{c}\")]).cast(pl.Float32)\n",
    "\n",
    "        std_tmp = pl.when(std_grp.is_null() | (std_grp <= 0))\\\n",
    "                    .then(pl.col(f\"__std_glb_{c}\"))\\\n",
    "                    .otherwise(std_grp)\n",
    "        std_use = pl.when(std_tmp < min_std).then(min_std).otherwise(std_tmp).cast(pl.Float32)\n",
    "\n",
    "        z = ((pl.col(c).cast(pl.Float32) - mu_use) / (std_use + eps)).clip(-8.0, 8.0).alias(f\"z_{c}\")\n",
    "        exprs.append(z)\n",
    "\n",
    "    lf_day_z = lf_day_z.with_columns(exprs)\n",
    "\n",
    "    \n",
    "    keep = [\n",
    "        \"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL,\n",
    "        \"time_sin\", \"time_cos\", \"time_bucket\", *no_z_grp_static_cols, *already_z_cols, \n",
    "        *z_done_cols\n",
    "    ]\n",
    "    \n",
    "    # 与实际可用列取交集（避免不存在的列）\n",
    "    avail = set(lf_day_z.collect_schema().names())\n",
    "    keep = [c for c in keep if c in avail]\n",
    "\n",
    "    # 去重但保持顺序（避免 DuplicateError）\n",
    "    keep = list(dict.fromkeys(keep))\n",
    "    \n",
    "    lf_out = lf_day_z.select(keep).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "    out_path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    lf_out.collect(streaming=True).write_parquet(\n",
    "        out_path, storage_options=storage_options, compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"wrote z-scored data for day {d} to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc45f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查缺失值\n",
    "z_paths = fs.glob(f\"{z_prefix}/*.parquet\")\n",
    "z_paths = [f\"az://{p}\" for p in z_paths]\n",
    "lx = pl.scan_parquet(z_paths, storage_options=storage_options).sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "\n",
    "df_null = lx.select(\n",
    "    [pl.col(c).null_count().alias(c) for c in [*z_done_cols, *already_z_cols]]\n",
    ").collect()\n",
    "\n",
    "df_null = df_null.to_pandas()\n",
    "df_null.rename(columns={0: \"null_count\"}, inplace=True)\n",
    "\n",
    "df_null = df_null.T\n",
    "df_null.rename(columns={0: \"null_count\"}, inplace=True)\n",
    "df_null.sort_values(by=\"null_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52252703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查标准差\n",
    "stds = lx.select(\n",
    "    [pl.col(c).std().alias(c) for c in [*already_z_cols, *z_done_cols]]\n",
    ").collect(streaming=True)\n",
    "\n",
    "pd_stds = stds.to_pandas().T\n",
    "\n",
    "pd_stds.sort_values(by=0, ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf18c8",
   "metadata": {},
   "source": [
    "## PCA 降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "411ee887",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_done_cols = [f\"z_{c}\" for c in do_z_cols]\n",
    "\n",
    "keep = [\n",
    "    \"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL,\n",
    "    \"time_sin\", \"time_cos\", \"time_bucket\", *no_z_grp_static_cols, *already_z_cols, \n",
    "    *z_done_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79a0bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定各个特征列处理方案：\n",
    "# already_z_cols = csrank_cols + other_already_z_cols\n",
    "\n",
    "# 挑选出不需要PCA的列：看z_done_cols 和other_already_z_cols中哪些 由COV_FEATURES[:200] 衍生的\n",
    "topk_cols = COV_FEATURES[:200]\n",
    "\n",
    "csrank_cols = [c for c in already_z_cols if \"__csrank\" in c]\n",
    "other_already_z_cols = [c for c in already_z_cols if c not in csrank_cols]\n",
    "no_pca_cols_from_other_already = [c for c in other_already_z_cols if c in topk_cols]\n",
    "no_pca_cols_from_z_done = [f\"z_{c}\" for c in do_z_cols if c in topk_cols]\n",
    "no_pca_cols = [\"time_idx\", G_SYM, G_DATE, G_TIME, WEIGHT_COL, TARGET_COL,\n",
    "        \"time_sin\", \"time_cos\", \"time_bucket\", \"z_time_pos\"] + no_z_grp_static_cols + csrank_cols + no_pca_cols_from_other_already + no_pca_cols_from_z_done\n",
    "no_pca_cols = list(dict.fromkeys(no_pca_cols))  # 去重且保持顺序\n",
    "\n",
    "pca_cols = [c for c in keep if c not in no_pca_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9b6a335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting IPCA on date: 1530 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1530.parquet\n",
      "fitting IPCA on date: 1531 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1531.parquet\n",
      "fitting IPCA on date: 1532 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1532.parquet\n",
      "fitting IPCA on date: 1533 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1533.parquet\n",
      "fitting IPCA on date: 1534 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1534.parquet\n",
      "fitting IPCA on date: 1535 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1535.parquet\n",
      "fitting IPCA on date: 1536 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1536.parquet\n",
      "fitting IPCA on date: 1537 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1537.parquet\n",
      "fitting IPCA on date: 1538 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1538.parquet\n",
      "fitting IPCA on date: 1539 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1539.parquet\n",
      "fitting IPCA on date: 1540 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1540.parquet\n",
      "fitting IPCA on date: 1541 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1541.parquet\n",
      "fitting IPCA on date: 1542 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1542.parquet\n",
      "fitting IPCA on date: 1543 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1543.parquet\n",
      "fitting IPCA on date: 1544 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1544.parquet\n",
      "fitting IPCA on date: 1545 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1545.parquet\n",
      "fitting IPCA on date: 1546 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1546.parquet\n",
      "fitting IPCA on date: 1547 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1547.parquet\n",
      "fitting IPCA on date: 1548 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1548.parquet\n",
      "fitting IPCA on date: 1549 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1549.parquet\n",
      "fitting IPCA on date: 1550 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1550.parquet\n",
      "fitting IPCA on date: 1551 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1551.parquet\n",
      "fitting IPCA on date: 1552 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1552.parquet\n",
      "fitting IPCA on date: 1553 from the path: az://jackson/js_exp/exp/v1/tft/z_shards/z_1553.parquet\n"
     ]
    }
   ],
   "source": [
    "# 训练集上计算 PCA\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import joblib\n",
    "\n",
    "z_prefix = \"az://jackson/js_exp/exp/v1/tft/z_shards\"; ensure_dir_az(z_prefix)\n",
    "\n",
    "\n",
    "max_components = min(200, len(pca_cols))\n",
    "\n",
    "ipca = IncrementalPCA(n_components=max_components)\n",
    "\n",
    "for d in range(train_lo, train_hi + 1): \n",
    "    path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    print(f\"fitting IPCA on date: {d} from the path: {path}\")\n",
    "    \n",
    "    df_day = pl.read_parquet(path, storage_options=storage_options).select(pca_cols)\n",
    "    X = df_day.to_numpy()\n",
    "    ipca.partial_fit(X)  # 增量拟合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c79e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合完成后，看看累计方差占比，决定真正保留的维度k (95%)\n",
    "cum = ipca.explained_variance_ratio_.cumsum()\n",
    "tau =0.7\n",
    "k = int(np.searchsorted(cum, tau)) + 1\n",
    "\n",
    "k_final = min(k, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d51f1ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPCA 模型已保存至: /mnt/data/js/exp/v1/tft/fold_0_ipca_1530_1553_k60_2025-10-20 20:22:03.joblib\n",
      "保留主成分0.7维度 k = 60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 保存模型\n",
    "# —— 裁剪参数到前 k 维（不要重新 new 模型）——\n",
    "ipca.components_              = ipca.components_[:k_final]\n",
    "ipca.explained_variance_      = ipca.explained_variance_[:k_final]\n",
    "ipca.explained_variance_ratio_= ipca.explained_variance_ratio_[:k_final]\n",
    "ipca.singular_values_         = ipca.singular_values_[:k_final]\n",
    "ipca.n_components_            = k_final\n",
    "\n",
    "\n",
    "pca_path = f\"{TFT_LOCAL_ROOT}/fold_{fold_id}_ipca_{train_lo}_{train_hi}_k{k_final}_{_now()}.joblib\"\n",
    "joblib.dump(\n",
    "    {\"ipca\": ipca, \"pca_cols\": pca_cols, \"cum_ratio\": cum, \"chosen_k\": k_final}, \n",
    "    pca_path\n",
    ")\n",
    "print(f\"IPCA 模型已保存至: {pca_path}\")\n",
    "print(f\"保留主成分{tau}维度 k = {k_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d16efc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1530.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1531.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1532.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1533.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1534.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1535.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1536.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1537.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1538.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1539.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1540.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1541.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1542.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1543.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1544.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1545.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1546.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1547.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1548.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1549.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1550.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1551.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1552.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1553.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1554.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1555.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1556.parquet\n",
      "written PCA features to: az://jackson/js_exp/exp/v1/tft/feat_pca_shards/pca_shard_1557.parquet\n",
      "[2025-10-20 20:23:06] all done\n"
     ]
    }
   ],
   "source": [
    "# 本折全部数据进行PCA变换\n",
    "\n",
    "pca_prefix = \"az://jackson/js_exp/exp/v1/tft/feat_pca_shards\"; ensure_dir_az(pca_prefix)\n",
    "keep_cols = no_pca_cols\n",
    "\n",
    "pca_k = ipca.n_components_\n",
    "pca_out_cols = [f\"PC{i+1}\" for i in range(pca_k)]\n",
    "\n",
    "for d in range(train_lo, val_hi + 1):\n",
    "    in_path = f\"{z_prefix}/z_{d:04d}.parquet\"\n",
    "    out_path = f\"{pca_prefix}/pca_shard_{d:04d}.parquet\"\n",
    "    \n",
    "    df = pl.read_parquet(in_path, storage_options=storage_options)\n",
    "    \n",
    "    X = (\n",
    "        df.select(pca_cols).to_numpy()\n",
    "    )\n",
    "    if X.size == 0:\n",
    "        # 当天没有行，直接跳过\n",
    "        continue\n",
    "    if np.isnan(X).any():\n",
    "        raise ValueError(\"PCA transform 输入里仍有 NaN，请检查上游补值/标准化。\")\n",
    "\n",
    "    Z = ipca.transform(X).astype(np.float32)\n",
    "    \n",
    "    # 组装输出\n",
    "    df_pca = pl.DataFrame(Z, schema=pca_out_cols)\n",
    "    \n",
    "    lf_out = pl.concat([\n",
    "        df.select(no_pca_cols),\n",
    "        df_pca\n",
    "    ],\n",
    "    how=\"horizontal\"\n",
    "    ).sort([G_SYM, \"time_idx\"])\n",
    "    \n",
    "    lf_out.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\"\n",
    "    )\n",
    "    print(f\"written PCA features to: {out_path}\")\n",
    "print(f\"[{_now()}] all done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b907e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging date range: 1530 ~ 1539, num files = 10\n",
      "writing to: az://jackson/js_exp/exp/v1/tft/fold_0/feat_pca_ten_days_shards/feat_pca_ten_days_chunk_1530_1539.parquet\n",
      "merging date range: 1540 ~ 1549, num files = 10\n",
      "writing to: az://jackson/js_exp/exp/v1/tft/fold_0/feat_pca_ten_days_shards/feat_pca_ten_days_chunk_1540_1549.parquet\n",
      "merging date range: 1550 ~ 1557, num files = 8\n",
      "writing to: az://jackson/js_exp/exp/v1/tft/fold_0/feat_pca_ten_days_shards/feat_pca_ten_days_chunk_1550_1557.parquet\n",
      "[2025-10-20 20:23:15] all done\n"
     ]
    }
   ],
   "source": [
    "# 按10天合并\n",
    "tft_fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(tft_fold_root)\n",
    "ten_days_prefix = f\"{tft_fold_root}/feat_pca_ten_days_shards\"; ensure_dir_az(ten_days_prefix)\n",
    "\n",
    "chunk_size = 10\n",
    "\n",
    "for lo in range(train_lo, val_hi + 1, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, folds_by_day[fold_id][1][-1])\n",
    "    paths = [f\"{pca_prefix}/pca_shard_{d:04d}.parquet\" for d in range(lo, hi + 1)]\n",
    "    \n",
    "    print(f\"merging date range: {lo} ~ {hi}, num files = {len(paths)}\")\n",
    "    \n",
    "    lf = pl.scan_parquet(paths, storage_options=storage_options)\n",
    "    \n",
    "    df = lf.collect(streaming=True).sort([G_SYM, \"time_idx\"]).rechunk()\n",
    "    \n",
    "    out_path = f\"{ten_days_prefix}/feat_pca_ten_days_chunk_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    df.write_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028cc3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0f8cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取处理好的数据\n",
    "tft_fold_root = f\"az://jackson/js_exp/exp/v1/tft/fold_{fold_id}\"; ensure_dir_az(tft_fold_root)\n",
    "ten_days_prefix = f\"{tft_fold_root}/feat_pca_ten_days_shards\"; ensure_dir_az(ten_days_prefix)\n",
    "\n",
    "\n",
    "lf_clean = pl.scan_parquet(f\"{ten_days_prefix}/*.parquet\", storage_options=storage_options)\n",
    "lf_clean = lf_clean.sort([G_SYM, \"time_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "933b1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_clean_train = lf_clean.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "\n",
    "sym_med = lf_clean_train.group_by(G_SYM).agg(\n",
    "    [pl.col(c).median().alias(f\"median_{c}\") for c in no_z_grp_static_cols]\n",
    ").collect()\n",
    "\n",
    "glb_med = lf_clean_train.select(\n",
    "    [pl.col(c).median().alias(f\"glb_median_{c}\") for c in no_z_grp_static_cols]\n",
    ").collect()\n",
    "\n",
    "\n",
    "lf_pure = (\n",
    "    lf_clean.join(sym_med.lazy(), on=G_SYM, how=\"left\")\n",
    "    .join(glb_med.lazy(), how=\"cross\")\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.coalesce([\n",
    "                pl.col(f\"median_{c}\"),\n",
    "                pl.col(f\"glb_median_{c}\")\n",
    "            ]).alias(c)\n",
    "            for c in no_z_grp_static_cols\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "    .drop([f\"median_{c}\" for c in no_z_grp_static_cols] + [f\"glb_median_{c}\" for c in no_z_grp_static_cols])\n",
    "    .sort([G_SYM, \"time_idx\"])\n",
    ")\n",
    "\n",
    "\n",
    "# 对Feature_09, 10, 11 标准化\n",
    "EPS = 1e-6\n",
    "cols = no_z_grp_static_cols\n",
    "# 训练期统计 -> 字典\n",
    "stats = (\n",
    "    lf_pure.filter(pl.col(G_DATE).is_between(train_lo, train_hi, closed=\"both\"))\n",
    "           .select([pl.mean(c).alias(f\"mean_{c}\") for c in cols] +\n",
    "                   [pl.std(c).alias(f\"std_{c}\")  for c in cols])\n",
    "           .collect(streaming=True)\n",
    ")\n",
    "mu  = {c: float(stats[0, f\"mean_{c}\"]) for c in cols}\n",
    "sd  = {c: max(float(stats[0, f\"std_{c}\"]) or 0.0, EPS) for c in cols}\n",
    "\n",
    "\n",
    "lf_final = lf_pure.with_columns([\n",
    "    ((pl.col(c) - pl.lit(mu[c])) / pl.lit(sd[c])).alias(c)   # 原地覆盖\n",
    "    for c in cols\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4c16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "KNOWN_CATEGORIES = [\"time_bucket\"]\n",
    "\n",
    "STATIC_REALS = [c for c in no_z_grp_static_cols]\n",
    "\n",
    "\n",
    "KNOWN_REALS = [c for c in lf_final.collect_schema().names() if c not in ([G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_bucket\", WEIGHT_COL, TARGET_COL] + STATIC_REALS)]\n",
    "\n",
    "\n",
    "TRAIN_COLS = KNOWN_REALS + STATIC_REALS + KNOWN_CATEGORIES + [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8f841",
   "metadata": {},
   "source": [
    "## try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                          \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in KNOWN_CATEGORIES else None,\n",
    "                          },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction == val_start_idx + ENC_LEN) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c11547",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b07cdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train 1530..1553 (24 days), val 1554..1557 (4 days)\n"
     ]
    }
   ],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "# ========== 训练（按 CV 折） ========== 先取第一折\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "#for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "\n",
    "####################################\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[fold_id]\n",
    "####################################\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    lf_final\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ") \n",
    "\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(str).astype(\"category\")\n",
    "pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(str).astype(\"category\")\n",
    "pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62859c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATIC 与 KNOWN 不重叠\n",
    "assert not (set(STATIC_REALS) & set(KNOWN_REALS))\n",
    "# 这些列都在数据里\n",
    "need = set(KNOWN_REALS) | set(STATIC_REALS) | set(KNOWN_CATEGORIES) | {G_SYM, \"time_idx\", TARGET_COL}\n",
    "missing = [c for c in need if c not in pdf_data.columns]\n",
    "assert not missing, f\"missing columns: {missing}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a51eb04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train idx up to 52271, val idx 52272..56143\n"
     ]
    }
   ],
   "source": [
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ede7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "# 定义 identity scalers\n",
    "UNSCALE_COLS = KNOWN_REALS #+ STATIC_REALS\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "\n",
    "# 构建训练集 timeseries dataset\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    pdf_data.loc[pdf_data[\"time_idx\"] <= train_end_idx],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, min_prediction_length=PRED_LEN,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    static_categoricals=[G_SYM],\n",
    "    #static_reals=STATIC_REALS,\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                        \"time_bucket\": NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")\n",
    "\n",
    "# 验证集复用 train_ds 的所有 encoders/normalizer（不泄漏）\n",
    "\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    pdf_data.loc[pdf_data[\"time_idx\"].between(val_start_idx, val_end_idx, inclusive=\"both\")],\n",
    "    min_prediction_idx=val_start_idx+ENC_LEN,\n",
    "    stop_randomization=True,\n",
    "    predict=False\n",
    ")\n",
    "\n",
    "#del pdf_data, lf_clean, lf_with_idx; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0412450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] train_loader batches = 1723\n",
      "[debug] val_loader batches = 144\n"
     ]
    }
   ],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE*2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=2,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 555    | train\n",
      "3  | prescalers                         | ModuleDict                      | 8.5 K  | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 635 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 635 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 33     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.294     Total estimated model params size (MB)\n",
      "7837      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9852270e0c405696bd79d7e202c5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0549194ad7e744fba01b9dfa0e5ad1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e54537ec844302be163cf9489a8f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eeff9a2b6e4c089c7ffd95c5f61d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226f1b422cf64f3ebd5b83ec56fb59ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a8e78aa8fc41d8af235b58aecdc67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b167dae9f770422dae348c9fd45e5841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 8.6 callbacks/logger/trainer\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, dirpath=ckpt_dir_fold.as_posix(), filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\", save_on_train_epoch_end=False),\n",
    "            ] # LearningRateMonitor(logging_interval=\"step\"),\n",
    "RUN_NAME = f\"without_static_f{fold_id}_E{MAX_EPOCHS}_lr{LR:g}_bs{BATCH_SIZE}_enc{ENC_LEN}_dec{DEC_LEN}_{datetime.now():%Y%m%d-%H%M%S}\"\n",
    "\n",
    "TEMP_LOG_DIR = Path(\"./tft_logs\")\n",
    "TEMP_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=TEMP_LOG_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=3,\n",
    "                    accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    enable_model_summary=True,\n",
    "                    gradient_clip_val=1.0,\n",
    "                    gradient_clip_algorithm=\"norm\",\n",
    "                    #fast_dev_run=1,\n",
    "                    limit_train_batches=1.0,\n",
    "                    limit_val_batches=1.0,\n",
    "                    val_check_interval=0.5,\n",
    "                    num_sanity_val_steps=0,\n",
    "                    log_every_n_steps=200,\n",
    "                    callbacks=callbacks,\n",
    "                    logger=logger,\n",
    "                    #accumulate_grad_batches=1,\n",
    "                    )\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[],\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    reduce_on_plateau_patience=3, \n",
    ")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a0c6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323975ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (torch.square(y_true - y_pred) * w).sum()\n",
    "den = (torch.square(y_true) * w).sum()  \n",
    "wr2 = 1 - num / den\n",
    "print(f\"wr2 after training: {wr2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
