{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2317e57f",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "from pipeline.stream_input_local import ShardedBatchStream  \n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c956339",
   "metadata": {},
   "source": [
    "## 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cd586",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入筛选的特征列\n",
    "df_ranking_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1300-1500__cv3-g7-r4__seed42__top1000__1760299442__range1000-1600__range1000-1600__cv2-g7-r4__1760347190.csv\")\n",
    "ls_topk_features = df_ranking_features['feature'].tolist()[:500]\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "TO_Z_INPUTS = [c for c in ls_topk_features if c not in TIME_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1) 统一配置 ==========\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 0\n",
    "TRAIN_TO_VAL = 6\n",
    "ENC_LEN      = 10\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 512\n",
    "LR           = 1e-5\n",
    "HIDDEN       = 16\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.1\n",
    "MAX_EPOCHS   = 30\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "print(\"[config] ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300bbba",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_date = 1400\n",
    "data_end_date = 1600\n",
    "\n",
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *TO_Z_INPUTS])\n",
    "    .filter(pl.col(G_DATE).is_between(data_start_date, data_end_date, closed=\"both\"))\n",
    ")\n",
    "lf_data = lf_data.sort([G_SYM, G_DATE, G_TIME])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1aaf",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d516",
   "metadata": {},
   "source": [
    "### 添加全局时间序列号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "        .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_prefix = \"az://jackson/js_exp/exp/v1/tft/panel_clean_shards\"; ensure_dir_az(container_prefix)\n",
    "chunk_size = 30\n",
    "for lo in range(data_start_date, data_end_date, chunk_size):\n",
    "    hi = min(lo + chunk_size - 1, data_end_date)\n",
    "    print(f\"processing date range: {lo} ~ {hi}\")\n",
    "    \n",
    "    lf_chunk = lf_data.filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    \n",
    "    lf_grid_chunk = (\n",
    "        grid_df.lazy().filter(pl.col(G_DATE).is_between(lo, hi, closed=\"both\"))\n",
    "    )\n",
    "    \n",
    "    lf_joined = (\n",
    "        lf_chunk.join(lf_grid_chunk, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "    )\n",
    "    \n",
    "    out_path = f\"{container_prefix}/panel_clean_{lo:04d}_{hi:04d}.parquet\"\n",
    "    print(f\"writing to: {out_path}\")\n",
    "    \n",
    "    lf_joined.sink_parquet(\n",
    "        out_path,\n",
    "        storage_options=storage_options,\n",
    "        compression=\"zstd\",\n",
    "    )\n",
    "print(f\"[{_now()}] all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb09de",
   "metadata": {},
   "source": [
    "### 导入新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新读入数据\n",
    "data_paths = fs.glob(f\"{container_prefix}/*.parquet\")\n",
    "data_paths = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in data_paths]\n",
    "lf_ready = pl.scan_parquet(data_paths, storage_options=storage_options).sort([G_SYM, G_DATE, G_TIME])\n",
    "\n",
    "# 明确要标准化的列\n",
    "z_cols = [c for c in TO_Z_INPUTS if c in lf_ready.collect_schema().names()]\n",
    "\n",
    "# ========== 连续特征清洗 ==========\n",
    "inf2null_exprs  = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c) for c in z_cols]\n",
    "ffill_exprs = [pl.col(c).forward_fill().over(G_SYM).fill_null(0.0).alias(c) for c in z_cols]\n",
    "\n",
    "\n",
    "lf_clean = (\n",
    "    lf_ready.with_columns(inf2null_exprs).with_columns(ffill_exprs) #.with_columns(isna_flag_exprs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e04edc",
   "metadata": {},
   "source": [
    "## CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_clean.select(pl.col(G_DATE)).unique().sort(by=G_DATE)\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5315",
   "metadata": {},
   "source": [
    "## 折内数据标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41652fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold_id = 0\n",
    "# 取第一个 fold 的训练集最后一天，作为统计 z-score 的上界\n",
    "stats_hi = int(folds_by_day[fold_id][0][-1])\n",
    "print(f\"标准化使用训练集的上限日期 = {stats_hi}\")\n",
    "\n",
    "\n",
    "# ========== Z-score ==========\n",
    "# 开始计算 z-score\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .group_by(G_SYM)\n",
    "            .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in z_cols] +\n",
    "                [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in z_cols])\n",
    ")\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in z_cols] +\n",
    "                    [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in z_cols])\n",
    ")\n",
    "\n",
    "stats_prefix = \"az://jackson/js_exp/exp/v1/tft/stats\"; ensure_dir_az(stats_prefix)\n",
    "# 保存统计结果\n",
    "lf_stats_sym.collect().write_parquet(\n",
    "    f\"{container_prefix}/fold_{fold_id}_stats_by_symbol.parquet\",\n",
    "    storage_options=storage_options,\n",
    "    compression=\"zstd\",\n",
    ")\n",
    "\n",
    "lf_stats_glb.collect().write_parquet(\n",
    "    f\"{container_prefix}/fold_{fold_id}_stats_by_global.parquet\",\n",
    "    storage_options=storage_options,\n",
    "    compression=\"zstd\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\").join(lf_stats_sym, on=G_SYM, how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "eps = 1e-6\n",
    "#Z_COLS, NAMARK_COLS = [], [f\"{c}__isna\" for c in RAW_FEATURES]\n",
    "\n",
    "Z_COLS = []\n",
    "for c in z_cols:\n",
    "    mu_sym, std_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_glb, std_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    mu_use, std_use = f\"{c}_mu_use\", f\"{c}_std_use\"\n",
    "    z_name = f\"{c}_z\"\n",
    "\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_sym).is_null()).then(pl.col(mu_glb)).otherwise(pl.col(mu_sym)).alias(mu_use),\n",
    "        pl.when(pl.col(std_sym).is_null() | (pl.col(std_sym) == 0)).then(pl.col(std_glb)).otherwise(pl.col(std_sym)).alias(std_use),\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(mu_use)) / (pl.col(std_use) + eps)).alias(z_name)\n",
    "    ).drop([mu_glb, std_glb, mu_sym, std_sym, mu_use, std_use])\n",
    "\n",
    "    Z_COLS.append(z_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_COLS = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL, *TIME_FEATURES] + Z_COLS \n",
    "lf_out = lf_z.select(OUT_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0575179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存lf_out到本地\n",
    "clean_path_local = P(\"local\", \"tft/panel/clean.parquet\")\n",
    "Path(clean_path_local).parent.mkdir(parents=True, exist_ok=True)\n",
    "lf_out.collect(streaming=True).write_parquet(clean_path_local, compression=\"zstd\")\n",
    "print(f\"[{_now()}] cleaned data saved to {clean_path_local}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028cc3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean 数据导入\n",
    "clean_path_local = P(\"local\", \"tft/panel/clean.parquet\")\n",
    "lf_clean = pl.scan_parquet(clean_path_local)\n",
    "lf_clean.limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_REALS = [c for c in lf_clean.collect_schema().names() if c not in (G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_bucket\", WEIGHT_COL, TARGET_COL)]\n",
    "\n",
    "KNOWN_CATEGORIES = [\"time_bucket\"] if 'time_bucket' in lf_clean.collect_schema().names() else []\n",
    "\n",
    "UNSCALE_COLS = KNOWN_REALS\n",
    "\n",
    "TRAIN_COLS = [c for c in lf_clean.collect_schema().names() if c not in (G_DATE, G_TIME)]\n",
    "\n",
    "# 定义 identity scalers\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8f841",
   "metadata": {},
   "source": [
    "## try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                          \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in KNOWN_CATEGORIES else None,\n",
    "                          },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction == val_start_idx + ENC_LEN) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c11547",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "# ========== 8) 训练（按 CV 折） ========== 先取第一折\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "#for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "####################################\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[fold_id]\n",
    "####################################\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    lf_clean\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ") \n",
    "\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51eb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建训练集 timeseries dataset\n",
    "\n",
    "t_data = pdf_data[TRAIN_COLS]\n",
    "train_df = t_data.loc[t_data[\"time_idx\"] <= train_end_idx]\n",
    "\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, min_prediction_length=PRED_LEN,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    static_categoricals=[G_SYM],\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                        \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in pdf_data.columns else None},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a95f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 验证集复用 train_ds 的所有 encoders/normalizer（不泄漏）\n",
    "\n",
    "val_df = t_data.loc[t_data[\"time_idx\"].between(val_start_idx, val_end_idx, inclusive=\"both\")]\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    val_df,\n",
    "    min_prediction_idx=val_start_idx+ENC_LEN,\n",
    "    stop_randomization=True,\n",
    "    predict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0412450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE*4,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 callbacks/logger/trainer\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, check_on_train_epoch_end=False),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, dirpath=ckpt_dir_fold.as_posix(), filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\", save_on_train_epoch_end=False),\n",
    "            LearningRateMonitor(logging_interval=\"step\"),]\n",
    "RUN_NAME = (f\"f{fold_id}\"f\"_E{MAX_EPOCHS}\"f\"_lr{LR}\"f\"_bs{BATCH_SIZE}\"f\"_enc{ENC_LEN}_dec{DEC_LEN}\"f\"_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "logger = TensorBoardLogger(save_dir=LOGS_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=3,\n",
    "                    accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    enable_model_summary=True,\n",
    "                    gradient_clip_val=1.0,\n",
    "                    gradient_clip_algorithm=\"norm\",\n",
    "                    fast_dev_run=False,\n",
    "                    limit_train_batches=50,\n",
    "                    limit_val_batches=25,\n",
    "                    val_check_interval=0.25,\n",
    "                    num_sanity_val_steps=2,\n",
    "                    log_every_n_steps=10,\n",
    "                    callbacks=callbacks,\n",
    "                    logger=logger,\n",
    "                    accumulate_grad_batches=1,\n",
    "                    deterministic=False\n",
    "                    )\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[RMSE()],\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (torch.square(y_true - y_pred) * w).sum()\n",
    "den = (torch.square(y_true) * w).sum()  \n",
    "wr2 = 1 - num / den\n",
    "print(f\"wr2 after training: {wr2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
