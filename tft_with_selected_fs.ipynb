{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2317e57f",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4586e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-13 10:28:15] imports ok\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库 ──────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# ── 第三方 ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import lightning as L\n",
    "import lightning.pytorch as lp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.data import TorchNormalizer, GroupNormalizer\n",
    "\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "from pipeline.stream_input_local import ShardedBatchStream  # 使用下方给你的新版类\n",
    "from pipeline.wr2 import WR2\n",
    "\n",
    "# ---- 性能/兼容开关（仅一次）----\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import time as _t\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "def _now() -> str:\n",
    "    return _t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c956339",
   "metadata": {},
   "source": [
    "## 定义工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# 滑动窗划分\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days):\n",
    "            break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cd586",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed63ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入筛选的特征列\n",
    "df_ranking_features = pd.read_csv(\"/mnt/data/js/exp/v1/models/tune/feature_importance__fixed__fixed__mm_full_train__features__fs__1300-1500__cv3-g7-r4__seed42__top1000__1760299442__range1000-1600__range1000-1600__cv2-g7-r4__1760347190.csv\")\n",
    "ls_topk_features = df_ranking_features['feature'].tolist()[:600]\n",
    "TIME_FEATURES = [\"time_bucket\", \"time_pos\", \"time_sin\", \"time_cos\"]\n",
    "TO_Z_INPUTS = [c for c in ls_topk_features if c not in TIME_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ef8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] ready\n"
     ]
    }
   ],
   "source": [
    "# ========== 1) 统一配置 ==========\n",
    "G_SYM, G_DATE, G_TIME = cfg[\"keys\"]          # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "TARGET_COL = cfg[\"target\"]                   # e.g. \"responder_6\"\n",
    "WEIGHT_COL = cfg[\"weight\"]                   # 允许为 None\n",
    "\n",
    "# 训练 & CV 超参\n",
    "N_SPLITS     = 1\n",
    "GAP_DAYS     = 0\n",
    "TRAIN_TO_VAL = 8\n",
    "ENC_LEN      = 10\n",
    "DEC_LEN      = 1\n",
    "PRED_LEN     = DEC_LEN\n",
    "BATCH_SIZE   = 512\n",
    "LR           = 1e-5\n",
    "HIDDEN       = 16\n",
    "HEADS        = 1\n",
    "DROPOUT      = 0.1\n",
    "MAX_EPOCHS   = 30\n",
    "\n",
    "# 数据路径\n",
    "PANEL_DIR_AZ   = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "\n",
    "TFT_LOCAL_ROOT = P(\"local\", \"tft\"); ensure_dir_local(TFT_LOCAL_ROOT)\n",
    "\n",
    "LOCAL_CLEAN_DIR = f\"{TFT_LOCAL_ROOT}/clean\"; ensure_dir_local(LOCAL_CLEAN_DIR)\n",
    "CKPTS_DIR = Path(TFT_LOCAL_ROOT) / \"ckpts\"; ensure_dir_local(CKPTS_DIR.as_posix())\n",
    "LOGS_DIR  = Path(TFT_LOCAL_ROOT) / \"logs\";  ensure_dir_local(LOGS_DIR.as_posix())\n",
    "\n",
    "print(\"[config] ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300bbba",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_paths = fs.glob(f\"{PANEL_DIR_AZ}/*.parquet\")\n",
    "#data_paths = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "data_start_date = 1500\n",
    "data_end_date = 1550\n",
    "\n",
    "data_paths = fs.glob(\"az://jackson/js_exp/exp/v1/panel_shards/*.parquet\")\n",
    "data_paths =[f\"az://{p}\" for p in data_paths]\n",
    "\n",
    "lf_data = (\n",
    "    pl.scan_parquet(data_paths, storage_options=storage_options)\n",
    "    .select([*cfg['keys'], WEIGHT_COL, TARGET_COL, *TIME_FEATURES, *TO_Z_INPUTS])\n",
    "    .filter(pl.col(G_DATE).is_between(data_start_date, data_end_date, closed=\"both\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04763c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-13 10:33:19] data saved: /mnt/data/js/exp/v1/tft/clean/data_1500_1550.parquet\n"
     ]
    }
   ],
   "source": [
    "# 存到本地\n",
    "\n",
    "local_source_path = f\"{LOCAL_CLEAN_DIR}/data_{data_start_date}_{data_end_date}.parquet\"\n",
    "\n",
    "# 直接流式写出；大数据更省内存更快\n",
    "lf_data.sink_parquet(\n",
    "    local_source_path,\n",
    "    compression=\"zstd\",      # 或 \"snappy\"，看你下游需求\n",
    "    statistics=True          # 便于后续再做 pruning\n",
    ")\n",
    "\n",
    "print(f\"[{_now()}] data saved: {local_source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb441d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新从本地读数据\n",
    "\n",
    "lf_data = pl.scan_parquet(local_source_path).sort([G_SYM, G_DATE, G_TIME])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1aaf",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d516",
   "metadata": {},
   "source": [
    "### 添加全局时间序列号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5fc2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_grid = (\n",
    "    lf_data.select([G_DATE, G_TIME]).unique()\n",
    "        .sort([G_DATE, G_TIME])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "\n",
    "lf_with_time_idx = (\n",
    "    lf_data.join(lf_grid, on=[G_DATE, G_TIME], how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6993ab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 606)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>weight</th><th>responder_6</th><th>time_bucket</th><th>time_pos</th><th>time_sin</th><th>time_cos</th><th>feature_06</th><th>feature_36</th><th>feature_04</th><th>feature_75__rstd14</th><th>feature_60__cs_z</th><th>feature_59</th><th>responder_0_close_roll30_std</th><th>feature_59__rstd30</th><th>feature_07</th><th>feature_61__lag900</th><th>feature_60</th><th>feature_61__lag1936</th><th>responder_6_prevday_std</th><th>responder_8_prev_tail_lag10</th><th>feature_61__ret50</th><th>feature_61__lag6776</th><th>feature_25__diff50</th><th>feature_76__rstd7</th><th>feature_48</th><th>responder_5_prevday_std</th><th>feature_60__rstd30</th><th>responder_5_prevday_mean</th><th>feature_51__rmean14</th><th>responder_1_close_roll14_std</th><th>feature_37__rstd30</th><th>responder_2_close_roll30_std</th><th>feature_31__diff50</th><th>feature_58</th><th>&hellip;</th><th>feature_15__rmean30</th><th>feature_48__rmean30</th><th>feature_05__lag968</th><th>feature_42__ewm10</th><th>feature_20__lag1936</th><th>feature_51__ewm5</th><th>feature_42__lag5808</th><th>responder_4_close_roll30_std</th><th>feature_04__rmean30</th><th>feature_69__ewm50</th><th>feature_46__rstd30</th><th>feature_12__ewm50</th><th>feature_13__ewm5</th><th>responder_3_close_roll7_mean</th><th>responder_7_prev_tail_d1</th><th>responder_4_same_t_prev3</th><th>responder_4_overnight_gap</th><th>feature_01__rstd30</th><th>feature_37__rmean7</th><th>responder_4_prev_tail_lag10</th><th>feature_42__lag2904</th><th>feature_46__rmean30</th><th>feature_20__lag900</th><th>feature_26__rz14</th><th>feature_59__rmean30</th><th>feature_57__ewm50</th><th>responder_7_prev_tail_d10</th><th>responder_0_prev_tail_lag967</th><th>feature_78__diff50</th><th>feature_51__rmean30</th><th>feature_71__ewm50</th><th>feature_21__lag2904</th><th>feature_39__lag2904</th><th>feature_47__ewm5</th><th>feature_39__rmean14</th><th>feature_20__lag6776</th><th>time_idx</th></tr><tr><td>i32</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>u8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>1500</td><td>0</td><td>4.426604</td><td>-1.314976</td><td>0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>-0.159566</td><td>-2.922452</td><td>2.150813</td><td>0.649587</td><td>-0.850858</td><td>1.746195</td><td>0.908512</td><td>0.517499</td><td>-0.829518</td><td>-1.370043</td><td>0.641324</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.172714</td><td>0.898012</td><td>-0.224687</td><td>0.665439</td><td>1.725063</td><td>0.434996</td><td>0.832047</td><td>-0.01</td><td>-0.181106</td><td>0.580871</td><td>0.186617</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.851687</td><td>-0.412686</td><td>0.568624</td><td>-1.191733</td><td>0.593553</td><td>-0.405449</td><td>-0.050556</td><td>0.234993</td><td>-0.2984</td><td>0.148307</td><td>0.336496</td><td>0.171171</td><td>0.69607</td><td>0.072969</td><td>0.044967</td><td>-0.1679</td><td>0.172663</td><td>0.546177</td><td>-0.57305</td><td>0.358798</td><td>-0.955042</td><td>-0.79962</td><td>0.532322</td><td>0.001837</td><td>-0.501739</td><td>-0.661193</td><td>-0.214477</td><td>-0.258034</td><td>0.33619</td><td>-0.282391</td><td>0.070346</td><td>-0.076776</td><td>-1.046247</td><td>-0.699493</td><td>-0.651832</td><td>0.462466</td><td>0</td></tr><tr><td>0</td><td>1500</td><td>1</td><td>4.426604</td><td>-0.671698</td><td>0</td><td>1.0</td><td>0.006491</td><td>0.999979</td><td>1.069108</td><td>-2.794829</td><td>2.031462</td><td>0.836284</td><td>1.438851</td><td>1.23963</td><td>0.908512</td><td>0.65714</td><td>-0.089751</td><td>-1.370043</td><td>0.988123</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.188648</td><td>0.9046</td><td>-0.224687</td><td>0.759381</td><td>1.759171</td><td>0.434996</td><td>0.841773</td><td>-0.01</td><td>-0.07661</td><td>0.580871</td><td>0.227577</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.86166</td><td>-0.354852</td><td>0.485492</td><td>-1.264243</td><td>0.593553</td><td>0.161152</td><td>-0.050556</td><td>0.234993</td><td>-0.225145</td><td>0.13006</td><td>0.48841</td><td>0.126291</td><td>1.061163</td><td>0.072969</td><td>0.044967</td><td>-0.150434</td><td>0.172663</td><td>0.54851</td><td>-0.471738</td><td>0.358798</td><td>-0.955042</td><td>-0.732828</td><td>0.532322</td><td>3.414578</td><td>-0.441202</td><td>-0.546575</td><td>-0.214477</td><td>-0.258034</td><td>0.58457</td><td>-0.238503</td><td>0.122192</td><td>-0.076776</td><td>-1.046247</td><td>-0.69222</td><td>-0.71471</td><td>0.462466</td><td>1</td></tr><tr><td>0</td><td>1500</td><td>2</td><td>4.426604</td><td>-0.290113</td><td>0</td><td>2.0</td><td>0.012981</td><td>0.999916</td><td>1.138704</td><td>-2.443655</td><td>2.43745</td><td>0.953172</td><td>1.20742</td><td>2.128175</td><td>0.908512</td><td>0.721343</td><td>0.062938</td><td>-1.370043</td><td>1.320846</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.203366</td><td>0.911017</td><td>-0.224687</td><td>0.779975</td><td>1.77037</td><td>0.434996</td><td>0.868895</td><td>-0.01</td><td>0.06524</td><td>0.580871</td><td>0.258462</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.871606</td><td>-0.292985</td><td>0.565012</td><td>-1.323569</td><td>0.593553</td><td>0.552869</td><td>-0.050556</td><td>0.234993</td><td>-0.171501</td><td>0.118531</td><td>0.644536</td><td>0.084969</td><td>1.313277</td><td>0.072969</td><td>0.044967</td><td>-0.113657</td><td>0.172663</td><td>0.529534</td><td>-0.415042</td><td>0.358798</td><td>-0.955042</td><td>-0.656717</td><td>0.532322</td><td>2.338139</td><td>-0.400401</td><td>-0.456623</td><td>-0.214477</td><td>-0.258034</td><td>0.598722</td><td>-0.179174</td><td>0.173759</td><td>-0.076776</td><td>-1.046247</td><td>-0.622791</td><td>-0.815185</td><td>0.462466</td><td>2</td></tr><tr><td>0</td><td>1500</td><td>3</td><td>4.426604</td><td>-0.338634</td><td>0</td><td>3.0</td><td>0.019471</td><td>0.99981</td><td>1.024881</td><td>-1.752336</td><td>2.53054</td><td>1.020966</td><td>1.261943</td><td>2.167717</td><td>0.908512</td><td>0.848355</td><td>0.152608</td><td>-1.370043</td><td>1.082904</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.217116</td><td>0.917273</td><td>-0.224687</td><td>0.786966</td><td>1.202953</td><td>0.434996</td><td>0.933084</td><td>-0.01</td><td>0.177734</td><td>0.580871</td><td>0.275996</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.875016</td><td>-0.222378</td><td>0.368624</td><td>-1.372108</td><td>0.593553</td><td>0.663555</td><td>-0.050556</td><td>0.234993</td><td>-0.110447</td><td>0.107719</td><td>0.702746</td><td>0.037482</td><td>1.491972</td><td>0.072969</td><td>0.044967</td><td>-0.11141</td><td>0.172663</td><td>0.521723</td><td>-0.349781</td><td>0.358798</td><td>-0.955042</td><td>-0.585588</td><td>0.532322</td><td>1.832468</td><td>-0.297947</td><td>-0.344714</td><td>-0.214477</td><td>-0.258034</td><td>0.362049</td><td>-0.138715</td><td>0.225399</td><td>-0.076776</td><td>-1.046247</td><td>-0.594974</td><td>-0.885424</td><td>0.462466</td><td>3</td></tr><tr><td>0</td><td>1500</td><td>4</td><td>4.426604</td><td>-0.530085</td><td>0</td><td>4.0</td><td>0.025961</td><td>0.999663</td><td>1.954541</td><td>-2.193815</td><td>2.395972</td><td>1.006895</td><td>0.862566</td><td>1.732435</td><td>0.908512</td><td>0.956367</td><td>0.468781</td><td>-1.370043</td><td>1.062898</td><td>-1.257957</td><td>0.821905</td><td>-0.156409</td><td>-0.23007</td><td>0.92338</td><td>-0.224687</td><td>0.70098</td><td>1.503926</td><td>0.434996</td><td>0.976965</td><td>-0.01</td><td>0.301686</td><td>0.580871</td><td>0.290789</td><td>1.597364</td><td>-0.026319</td><td>-1.144345</td><td>&hellip;</td><td>-0.877326</td><td>-0.166216</td><td>0.227686</td><td>-1.411823</td><td>0.593553</td><td>0.866701</td><td>-0.050556</td><td>0.234993</td><td>-0.049175</td><td>0.098634</td><td>0.79892</td><td>0.022148</td><td>1.581803</td><td>0.072969</td><td>0.044967</td><td>-0.0174</td><td>0.172663</td><td>0.516557</td><td>-0.295851</td><td>0.358798</td><td>-0.955042</td><td>-0.503418</td><td>0.532322</td><td>1.514958</td><td>-0.202671</td><td>-0.248388</td><td>-0.214477</td><td>-0.258034</td><td>0.310126</td><td>-0.080733</td><td>0.276998</td><td>-0.076776</td><td>-1.046247</td><td>-0.50747</td><td>-0.925505</td><td>0.462466</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 606)\n",
       "┌───────────┬─────────┬─────────┬──────────┬───┬─────────────┬─────────────┬────────────┬──────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ weight   ┆ … ┆ feature_47_ ┆ feature_39_ ┆ feature_20 ┆ time_idx │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ ---      ┆   ┆ _ewm5       ┆ _rmean14    ┆ __lag6776  ┆ ---      │\n",
       "│ i32       ┆ i32     ┆ i32     ┆ f32      ┆   ┆ ---         ┆ ---         ┆ ---        ┆ i64      │\n",
       "│           ┆         ┆         ┆          ┆   ┆ f32         ┆ f32         ┆ f32        ┆          │\n",
       "╞═══════════╪═════════╪═════════╪══════════╪═══╪═════════════╪═════════════╪════════════╪══════════╡\n",
       "│ 0         ┆ 1500    ┆ 0       ┆ 4.426604 ┆ … ┆ -0.699493   ┆ -0.651832   ┆ 0.462466   ┆ 0        │\n",
       "│ 0         ┆ 1500    ┆ 1       ┆ 4.426604 ┆ … ┆ -0.69222    ┆ -0.71471    ┆ 0.462466   ┆ 1        │\n",
       "│ 0         ┆ 1500    ┆ 2       ┆ 4.426604 ┆ … ┆ -0.622791   ┆ -0.815185   ┆ 0.462466   ┆ 2        │\n",
       "│ 0         ┆ 1500    ┆ 3       ┆ 4.426604 ┆ … ┆ -0.594974   ┆ -0.885424   ┆ 0.462466   ┆ 3        │\n",
       "│ 0         ┆ 1500    ┆ 4       ┆ 4.426604 ┆ … ┆ -0.50747    ┆ -0.925505   ┆ 0.462466   ┆ 4        │\n",
       "└───────────┴─────────┴─────────┴──────────┴───┴─────────────┴─────────────┴────────────┴──────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_with_time_idx.limit(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e04edc",
   "metadata": {},
   "source": [
    "## CV 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a016c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_ready = lf_with_time_idx\n",
    "\n",
    "# ==========  CV 划分 ==========\n",
    "all_days = (\n",
    "    lf_ready.select(pl.col(G_DATE)).unique().sort(by=G_DATE)\n",
    "    .collect(streaming=True)[G_DATE].to_numpy()\n",
    ")\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5315",
   "metadata": {},
   "source": [
    "## 折内数据标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c5174a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symbol_id',\n",
       " 'date_id',\n",
       " 'time_id',\n",
       " 'weight',\n",
       " 'responder_6',\n",
       " 'time_bucket',\n",
       " 'time_pos',\n",
       " 'time_sin',\n",
       " 'time_cos',\n",
       " 'feature_06',\n",
       " 'feature_36',\n",
       " 'feature_04',\n",
       " 'feature_75__rstd14',\n",
       " 'feature_60__cs_z',\n",
       " 'feature_59',\n",
       " 'responder_0_close_roll30_std',\n",
       " 'feature_59__rstd30',\n",
       " 'feature_07',\n",
       " 'feature_61__lag900',\n",
       " 'feature_60',\n",
       " 'feature_61__lag1936',\n",
       " 'responder_6_prevday_std',\n",
       " 'responder_8_prev_tail_lag10',\n",
       " 'feature_61__ret50',\n",
       " 'feature_61__lag6776',\n",
       " 'feature_25__diff50',\n",
       " 'feature_76__rstd7',\n",
       " 'feature_48',\n",
       " 'responder_5_prevday_std',\n",
       " 'feature_60__rstd30',\n",
       " 'responder_5_prevday_mean',\n",
       " 'feature_51__rmean14',\n",
       " 'responder_1_close_roll14_std',\n",
       " 'feature_37__rstd30',\n",
       " 'responder_2_close_roll30_std',\n",
       " 'feature_31__diff50',\n",
       " 'feature_58',\n",
       " 'feature_76__rstd14',\n",
       " 'feature_59__ewm50',\n",
       " 'feature_24__lag6776',\n",
       " 'responder_0_prevday_std',\n",
       " 'feature_61__lag5808',\n",
       " 'responder_3_prevday_std',\n",
       " 'responder_8_prevday_mean',\n",
       " 'feature_26__diff50',\n",
       " 'feature_22__diff50',\n",
       " 'feature_01__ewm50',\n",
       " 'feature_60__csrank',\n",
       " 'responder_7_close_roll14_std',\n",
       " 'feature_20__diff50',\n",
       " 'responder_3_prevday_mean',\n",
       " 'feature_08__ewm50',\n",
       " 'feature_25__ret50',\n",
       " 'feature_61__diff50',\n",
       " 'responder_7_prev_tail_lag967',\n",
       " 'feature_58__ewm50',\n",
       " 'feature_24__lag900',\n",
       " 'feature_61__lag2904',\n",
       " 'feature_56',\n",
       " 'responder_8_close_roll14_std',\n",
       " 'feature_75__rstd7',\n",
       " 'feature_04__ewm50',\n",
       " 'feature_59__rstd7',\n",
       " 'feature_68',\n",
       " 'feature_51__ewm10',\n",
       " 'feature_30__lag900',\n",
       " 'feature_30__diff50',\n",
       " 'feature_68__ewm10',\n",
       " 'feature_16__rstd30',\n",
       " 'feature_24__diff50',\n",
       " 'feature_08__rmean30',\n",
       " 'feature_61__lag3872',\n",
       " 'feature_30__lag50',\n",
       " 'feature_61__lag4840',\n",
       " 'responder_6_prev_tail_lag50',\n",
       " 'responder_2_close_roll14_std',\n",
       " 'responder_6_prev_tail_lag967',\n",
       " 'feature_22__lag2904',\n",
       " 'feature_31__ret50',\n",
       " 'responder_4_close_roll14_std',\n",
       " 'feature_23__diff50',\n",
       " 'feature_05__rstd30',\n",
       " 'feature_13',\n",
       " 'feature_24__ret50',\n",
       " 'feature_27__ret50',\n",
       " 'feature_30__ret50',\n",
       " 'feature_21__diff50',\n",
       " 'responder_0_prev_tail_lag10',\n",
       " 'feature_22__lag7744',\n",
       " 'feature_08__rstd30',\n",
       " 'feature_66__rmean30',\n",
       " 'feature_21__ret50',\n",
       " 'responder_2_prevday_std',\n",
       " 'feature_38',\n",
       " 'feature_54__ewm10',\n",
       " 'responder_8_close_roll30_std',\n",
       " 'responder_4_prevday_std',\n",
       " 'responder_2_prev_tail_lag6',\n",
       " 'feature_25__rz7',\n",
       " 'responder_8_prevday_std',\n",
       " 'feature_22',\n",
       " 'feature_05__ewm50',\n",
       " 'feature_20__lag500',\n",
       " 'responder_7_prevday_std',\n",
       " 'responder_5_prev_tail_lag2',\n",
       " 'feature_22__lag900',\n",
       " 'responder_8_prev_tail_lag967',\n",
       " 'responder_7_prevday_mean',\n",
       " 'responder_1_prev2day_close',\n",
       " 'feature_04__ewm10',\n",
       " 'feature_07__rstd30',\n",
       " 'responder_0_close_roll3_std',\n",
       " 'responder_2_prev_tail_lag50',\n",
       " 'feature_22__lag6776',\n",
       " 'feature_23__lag100',\n",
       " 'feature_60__ewm10',\n",
       " 'feature_26__ret50',\n",
       " 'feature_20__ret50',\n",
       " 'feature_06__ewm10',\n",
       " 'feature_33__ewm50',\n",
       " 'feature_02__ewm50',\n",
       " 'feature_52',\n",
       " 'feature_59__rstd14',\n",
       " 'feature_28__ret50',\n",
       " 'feature_29__ret50',\n",
       " 'feature_28__lag500',\n",
       " 'responder_1_prevday_std',\n",
       " 'feature_16__rmean7',\n",
       " 'feature_22__lag1936',\n",
       " 'responder_6_prev_tail_lag950',\n",
       " 'responder_2_prev_tail_lag100',\n",
       " 'feature_61__lag100',\n",
       " 'responder_1_close_roll30_std',\n",
       " 'feature_02__rmean14',\n",
       " 'feature_61',\n",
       " 'responder_2_prev_tail_lag10',\n",
       " 'feature_16__ewm10',\n",
       " 'feature_30__lag1936',\n",
       " 'responder_8_prev_tail_lag500',\n",
       " 'feature_24__lag100',\n",
       " 'feature_08__lag500',\n",
       " 'feature_43__ewm50',\n",
       " 'responder_7_prevday_close_minus_mean',\n",
       " 'feature_38__ewm50',\n",
       " 'feature_33__rmean3',\n",
       " 'responder_5_prev_tail_lag967',\n",
       " 'responder_2_close_roll14_mean',\n",
       " 'responder_0_prev_tail_lag900',\n",
       " 'feature_69__rstd30',\n",
       " 'feature_19__rstd30',\n",
       " 'feature_23__ret50',\n",
       " 'responder_2_close_roll7_std',\n",
       " 'feature_61__rz30',\n",
       " 'responder_3_prev_tail_lag967',\n",
       " 'feature_15__ewm50',\n",
       " 'responder_6_close_roll7_std',\n",
       " 'feature_38__rstd30',\n",
       " 'feature_40__ewm50',\n",
       " 'responder_0_close_roll14_std',\n",
       " 'feature_36__ewm5',\n",
       " 'responder_6_prev_tail_d1',\n",
       " 'feature_61__lag7744',\n",
       " 'feature_01__rmean30',\n",
       " 'feature_47',\n",
       " 'feature_06__ewm50',\n",
       " 'feature_07__rmean30',\n",
       " 'responder_5_close_roll30_std',\n",
       " 'responder_7_prev_tail_lag500',\n",
       " 'responder_8_close_roll7_std',\n",
       " 'responder_3_prev_tail_lag950',\n",
       " 'feature_61__rstd30',\n",
       " 'feature_31__lag100',\n",
       " 'feature_44__ewm10',\n",
       " 'feature_04__ewm5',\n",
       " 'feature_05__rmean30',\n",
       " 'responder_0_prev_tail_lag500',\n",
       " 'feature_47__rstd30',\n",
       " 'feature_27__lag1936',\n",
       " 'feature_56__diff50',\n",
       " 'feature_22__lag3872',\n",
       " 'feature_61__lag500',\n",
       " 'feature_01__lag100',\n",
       " 'feature_06__rmean14',\n",
       " 'feature_55__rmean14',\n",
       " 'feature_28__diff50',\n",
       " 'responder_8_prevday_close_minus_mean',\n",
       " 'feature_13__rmean7',\n",
       " 'feature_22__ret50',\n",
       " 'feature_01__diff50',\n",
       " 'responder_0_prev_tail_lag100',\n",
       " 'feature_21__lag950',\n",
       " 'feature_05__lag100',\n",
       " 'feature_24__lag7744',\n",
       " 'responder_6_prevday_mean',\n",
       " 'feature_08__rstd7',\n",
       " 'feature_68__rmean14',\n",
       " 'feature_55__ewm50',\n",
       " 'responder_6_prev_tail_lag900',\n",
       " 'feature_59__rstd3',\n",
       " 'feature_05__rmean14',\n",
       " 'feature_56__rmean3',\n",
       " 'feature_76',\n",
       " 'responder_1_prevday_mean',\n",
       " 'feature_25__lag4840',\n",
       " 'feature_58__rstd30',\n",
       " 'responder_6_prevday_close_minus_mean',\n",
       " 'feature_45__rstd30',\n",
       " 'feature_37__diff50',\n",
       " 'feature_54__rmean14',\n",
       " 'responder_3_prev_tail_lag100',\n",
       " 'feature_39',\n",
       " 'feature_66__diff50',\n",
       " 'feature_56__ewm5',\n",
       " 'feature_22__lag5808',\n",
       " 'feature_29__diff50',\n",
       " 'feature_22__rstd30',\n",
       " 'feature_24',\n",
       " 'feature_07__rmean3',\n",
       " 'feature_21__lag4840',\n",
       " 'responder_2_prev_tail_d1',\n",
       " 'feature_51',\n",
       " 'responder_4_prevday_mean',\n",
       " 'feature_24__lag5808',\n",
       " 'responder_4_prev_tail_lag900',\n",
       " 'feature_06__diff10',\n",
       " 'feature_00__ewm50',\n",
       " 'responder_7_prev_tail_lag950',\n",
       " 'feature_07__ewm50',\n",
       " 'feature_61__ewm50',\n",
       " 'feature_38__lag100',\n",
       " 'feature_66__ewm50',\n",
       " 'feature_61__lag968',\n",
       " 'feature_05__diff50',\n",
       " 'feature_58__rmean3',\n",
       " 'feature_17__rstd30',\n",
       " 'responder_6_prev_tail_lag100',\n",
       " 'feature_08',\n",
       " 'responder_3_close_roll14_std',\n",
       " 'responder_4_prev_tail_lag950',\n",
       " 'feature_31__lag900',\n",
       " 'feature_23__rz7',\n",
       " 'feature_66__ewm10',\n",
       " 'responder_7_prev_tail_lag900',\n",
       " 'responder_4_close_roll3_std',\n",
       " 'feature_30__lag2904',\n",
       " 'feature_16__rmean14',\n",
       " 'feature_08__lag100',\n",
       " 'responder_1_prev_tail_lag950',\n",
       " 'feature_68__rstd30',\n",
       " 'feature_36__rstd30',\n",
       " 'responder_5_prev_tail_lag10',\n",
       " 'feature_53__rmean14',\n",
       " 'responder_1_close_roll7_std',\n",
       " 'feature_32__ewm50',\n",
       " 'feature_39__ewm50',\n",
       " 'feature_37__lag100',\n",
       " 'feature_01__rmean7',\n",
       " 'responder_7_same_t_prev1',\n",
       " 'feature_38__rmean30',\n",
       " 'feature_07__ewm10',\n",
       " 'feature_42__ewm50',\n",
       " 'feature_31__lag950',\n",
       " 'feature_36__lag1',\n",
       " 'feature_48__rstd30',\n",
       " 'feature_01__rmean14',\n",
       " 'feature_68__ewm50',\n",
       " 'feature_56__ewm10',\n",
       " 'feature_22__rstd3',\n",
       " 'feature_54__ewm50',\n",
       " 'feature_08__rmean14',\n",
       " 'feature_27__diff50',\n",
       " 'feature_19__ewm50',\n",
       " 'feature_00__rmean30',\n",
       " 'feature_47__ewm10',\n",
       " 'feature_20__lag950',\n",
       " 'feature_01',\n",
       " 'feature_59__ewm10',\n",
       " 'responder_7_same_t_prev5',\n",
       " 'feature_50__lag2904',\n",
       " 'responder_5_close_roll7_std',\n",
       " 'feature_17__ewm50',\n",
       " 'responder_2_prev_tail_lag950',\n",
       " 'responder_4_prev_tail_lag50',\n",
       " 'feature_66__rstd30',\n",
       " 'responder_8_prev_tail_d10',\n",
       " 'feature_71__rmean7',\n",
       " 'feature_29__lag500',\n",
       " 'feature_31__rstd30',\n",
       " 'feature_18__ewm50',\n",
       " 'feature_18__rmean7',\n",
       " 'feature_37__ewm50',\n",
       " 'feature_61__diff10',\n",
       " 'feature_01__rmean3',\n",
       " 'feature_23__lag3872',\n",
       " 'feature_53__ewm5',\n",
       " 'feature_61__lag950',\n",
       " 'feature_21__rstd30',\n",
       " 'feature_05__ewm5',\n",
       " 'feature_44__ewm50',\n",
       " 'feature_52__rstd30',\n",
       " 'feature_45',\n",
       " 'responder_8_prev_tail_lag950',\n",
       " 'responder_4_prev_tail_lag500',\n",
       " 'feature_42__rmean30',\n",
       " 'feature_07__rmean14',\n",
       " 'feature_07__ewm5',\n",
       " 'responder_7_prev_tail_lag100',\n",
       " 'feature_50__ewm50',\n",
       " 'responder_4_close_roll7_mean',\n",
       " 'feature_35__ewm50',\n",
       " 'feature_66__rmean14',\n",
       " 'feature_45__rmean14',\n",
       " 'responder_8_prev_tail_lag900',\n",
       " 'feature_01__ewm10',\n",
       " 'feature_66__ewm5',\n",
       " 'feature_47__ewm50',\n",
       " 'feature_34__ewm50',\n",
       " 'feature_29__lag1936',\n",
       " 'feature_56__rmean14',\n",
       " 'feature_53__ewm50',\n",
       " 'responder_0_prev2day_close',\n",
       " 'feature_49__rstd30',\n",
       " 'feature_04__rstd30',\n",
       " 'responder_5_prev_tail_lag31',\n",
       " 'feature_26__rstd30',\n",
       " 'responder_1_close_roll3_mean',\n",
       " 'responder_4_prev_tail_lag967',\n",
       " 'feature_06__rmean30',\n",
       " 'feature_47__rmean30',\n",
       " 'feature_29__rstd30',\n",
       " 'feature_38__diff50',\n",
       " 'responder_4_close_roll3_mean',\n",
       " 'feature_22__rz3',\n",
       " 'feature_49__ewm50',\n",
       " 'responder_3_prev_tail_d10',\n",
       " 'feature_29__lag4840',\n",
       " 'feature_20__ewm50',\n",
       " 'feature_61__lag50',\n",
       " 'feature_24__lag2904',\n",
       " 'responder_6_prev_tail_d5',\n",
       " 'feature_21__rz7',\n",
       " 'feature_55__rmean7',\n",
       " 'responder_4_same_t_prev1',\n",
       " 'feature_38__rmean3',\n",
       " 'feature_25__lag7744',\n",
       " 'feature_61__rz7',\n",
       " 'feature_30__lag4840',\n",
       " 'feature_36__ewm10',\n",
       " 'feature_37__lag500',\n",
       " 'feature_19__rmean14',\n",
       " 'responder_3_prev_tail_lag11',\n",
       " 'feature_61__ret10',\n",
       " 'feature_76__rmean14',\n",
       " 'responder_2_prev_tail_lag2',\n",
       " 'responder_8_prev_tail_d1',\n",
       " 'feature_13__rmean14',\n",
       " 'feature_08__lag50',\n",
       " 'responder_4_prev_tail_d10',\n",
       " 'feature_58__rmean7',\n",
       " 'feature_29__lag5808',\n",
       " 'feature_76__ewm5',\n",
       " 'feature_37__rmean30',\n",
       " 'feature_08__lag2904',\n",
       " 'feature_01__ewm5',\n",
       " 'feature_28__lag1936',\n",
       " 'feature_03__rmean30',\n",
       " 'responder_5_prev_tail_d10',\n",
       " 'feature_58__rmean30',\n",
       " 'feature_00__rmean14',\n",
       " 'feature_53__lag6776',\n",
       " 'feature_56__rstd30',\n",
       " 'responder_5_prev_tail_lag50',\n",
       " 'responder_1_close_roll7_mean',\n",
       " 'feature_37__lag900',\n",
       " 'responder_7_prev_tail_lag50',\n",
       " 'responder_4_prev_tail_d5',\n",
       " 'responder_8_prev_tail_lag31',\n",
       " 'feature_55__ewm10',\n",
       " 'feature_26__lag6776',\n",
       " 'responder_3_prev_tail_lag900',\n",
       " 'feature_50__lag6776',\n",
       " 'feature_62__ewm50',\n",
       " 'feature_20__rstd30',\n",
       " 'responder_0_overnight_gap',\n",
       " 'responder_7_close_roll14_mean',\n",
       " 'feature_21__lag3872',\n",
       " 'feature_29__lag7744',\n",
       " 'feature_20__lag3872',\n",
       " 'feature_29__lag2904',\n",
       " 'feature_38__lag900',\n",
       " 'feature_45__rmean30',\n",
       " 'responder_0_prev_tail_lag50',\n",
       " 'feature_33__ewm10',\n",
       " 'feature_57__rmean30',\n",
       " 'responder_0_close_roll7_std',\n",
       " 'feature_27__lag7744',\n",
       " 'feature_01__lag5808',\n",
       " 'responder_0_prev_tail_lag950',\n",
       " 'feature_65__rstd30',\n",
       " 'feature_25__rz3',\n",
       " 'feature_06__rstd30',\n",
       " 'feature_15__ewm10',\n",
       " 'feature_14__ewm50',\n",
       " 'feature_46__ewm10',\n",
       " 'responder_3_prev_tail_lag10',\n",
       " 'feature_42__lag968',\n",
       " 'responder_8_prev_tail_lag11',\n",
       " 'feature_28__lag5808',\n",
       " 'responder_5_prev_tail_lag900',\n",
       " 'feature_56__rmean30',\n",
       " 'feature_29__lag3872',\n",
       " 'feature_25__lag100',\n",
       " 'feature_42__lag6776',\n",
       " 'responder_1_prev_tail_lag500',\n",
       " 'feature_30__lag5808',\n",
       " 'feature_07__rz30',\n",
       " 'responder_7_same_t_prev2',\n",
       " 'responder_2_prev_tail_lag31',\n",
       " 'responder_5_prev_tail_lag11',\n",
       " 'feature_01__lag900',\n",
       " 'feature_21__lag900',\n",
       " 'feature_53__rmean30',\n",
       " 'feature_08__rstd14',\n",
       " 'feature_50__ewm10',\n",
       " 'feature_50__rstd30',\n",
       " 'feature_18__rmean30',\n",
       " 'feature_20__lag2904',\n",
       " 'responder_3_prev_tail_lag6',\n",
       " 'responder_0_prev_tail_lag2',\n",
       " 'feature_53',\n",
       " 'responder_5_close_roll14_std',\n",
       " 'responder_1_prev_tail_d30',\n",
       " 'feature_47__rmean7',\n",
       " 'responder_5_prev_tail_lag100',\n",
       " 'feature_38__rmean14',\n",
       " 'feature_05',\n",
       " 'responder_2_prev_tail_lag900',\n",
       " 'responder_8_prev_tail_lag100',\n",
       " 'responder_8_prev_tail_lag6',\n",
       " 'feature_23__lag2904',\n",
       " 'responder_3_prev_tail_lag31',\n",
       " 'feature_49',\n",
       " 'feature_72__ewm50',\n",
       " 'responder_5_prev_tail_lag950',\n",
       " 'feature_36__ewm50',\n",
       " 'feature_31__rstd3',\n",
       " 'feature_42__lag1936',\n",
       " 'feature_50__lag968',\n",
       " 'feature_26__lag5808',\n",
       " 'feature_21__lag1936',\n",
       " 'feature_01__lag968',\n",
       " 'feature_24__lag3872',\n",
       " 'responder_1_prevday_close_minus_mean',\n",
       " 'feature_29',\n",
       " 'responder_2_prev_tail_lag11',\n",
       " 'feature_29__lag6776',\n",
       " 'feature_19__rmean30',\n",
       " 'responder_3_prev_tail_lag50',\n",
       " 'feature_39__ewm10',\n",
       " 'feature_41__rmean30',\n",
       " 'feature_20__lag4840',\n",
       " 'feature_04__rmean3',\n",
       " 'feature_31__diff10',\n",
       " 'feature_15__rmean14',\n",
       " 'feature_21__rz3',\n",
       " 'responder_7_prev_tail_lag6',\n",
       " 'feature_23__lag4840',\n",
       " 'feature_08__lag5808',\n",
       " 'feature_23__rstd14',\n",
       " 'responder_7_close_roll7_mean',\n",
       " 'feature_31__lag1936',\n",
       " 'feature_30__lag7744',\n",
       " 'responder_4_prev_tail_d1',\n",
       " 'responder_6_prev_tail_lag2',\n",
       " 'feature_60__ewm5',\n",
       " 'feature_54__rmean30',\n",
       " 'responder_2_prevday_mean',\n",
       " 'feature_65__ewm50',\n",
       " 'feature_42__lag3872',\n",
       " 'feature_26__rz7',\n",
       " 'responder_3_prev_tail_lag500',\n",
       " 'feature_20__lag100',\n",
       " 'responder_3_prev_tail_d1',\n",
       " 'responder_1_close_roll3_std',\n",
       " 'responder_6_prev2day_close',\n",
       " 'responder_6_prev_tail_lag500',\n",
       " 'feature_38__ewm5',\n",
       " 'responder_6_prev_tail_lag31',\n",
       " 'responder_7_overnight_gap',\n",
       " 'responder_1_prev_tail_lag900',\n",
       " 'feature_05__rmean3',\n",
       " 'feature_31__lag4840',\n",
       " 'responder_4_prevday_close_minus_mean',\n",
       " 'feature_05__lag50',\n",
       " 'feature_33__rmean7',\n",
       " 'feature_56__rmean7',\n",
       " 'responder_0_prev_tail_d10',\n",
       " 'feature_26__rstd3',\n",
       " 'feature_13__ewm50',\n",
       " 'responder_6_close_roll30_mean',\n",
       " 'responder_0_prev_tail_d30',\n",
       " 'responder_2_prev_tail_lag500',\n",
       " 'feature_19__rmean7',\n",
       " 'responder_1_prev_tail_d10',\n",
       " 'responder_4_prev_tail_lag6',\n",
       " 'responder_4_prev_tail_lag100',\n",
       " 'feature_37__lag50',\n",
       " 'feature_23__rstd30',\n",
       " 'feature_07__diff50',\n",
       " 'responder_4_prev_tail_lag31',\n",
       " 'responder_8_prev_tail_d5',\n",
       " 'feature_05__lag5808',\n",
       " 'feature_76__rmean30',\n",
       " 'responder_7_close_roll30_std',\n",
       " 'responder_4_close_roll7_std',\n",
       " 'responder_7_prev_tail_d30',\n",
       " 'responder_1_prev_tail_lag100',\n",
       " 'responder_4_same_t_prev7',\n",
       " 'feature_31__rz7',\n",
       " 'feature_66__rmean7',\n",
       " 'responder_6_overnight_gap',\n",
       " 'responder_5_close_roll3_std',\n",
       " 'responder_8_prev2day_close',\n",
       " 'responder_0_prev_tail_d5',\n",
       " 'responder_0_prev_tail_lag6',\n",
       " 'responder_0_prevday_mean',\n",
       " 'responder_1_prev_tail_d5',\n",
       " 'feature_55__ewm5',\n",
       " 'feature_74__rstd30',\n",
       " 'feature_07__rmean7',\n",
       " 'feature_31__rz3',\n",
       " 'feature_43__rmean30',\n",
       " 'feature_08__rmean3',\n",
       " 'responder_5_prev_tail_lag6',\n",
       " 'responder_4_prevday_close',\n",
       " 'responder_4_prev_tail_lag11',\n",
       " 'feature_58__ewm5',\n",
       " 'feature_05__ewm10',\n",
       " 'feature_23__lag1936',\n",
       " 'responder_0_close_roll14_mean',\n",
       " 'responder_8_prev_tail_lag50',\n",
       " 'responder_6_prev_tail_d30',\n",
       " 'feature_58__lag50',\n",
       " 'responder_7_prev_tail_lag31',\n",
       " 'responder_1_overnight_gap',\n",
       " 'responder_3_prev2day_close',\n",
       " 'responder_3_close_roll30_std',\n",
       " 'feature_53__lag1936',\n",
       " 'feature_37__ewm10',\n",
       " 'feature_65__rmean30',\n",
       " 'feature_38__ewm10',\n",
       " 'feature_71__rmean30',\n",
       " 'feature_57__ewm10',\n",
       " 'feature_48__rstd14',\n",
       " 'responder_3_close_roll3_std',\n",
       " 'feature_03__ewm50',\n",
       " 'feature_30__rstd30',\n",
       " 'feature_36__rmean30',\n",
       " 'feature_31__lag5808',\n",
       " 'feature_39__rmean30',\n",
       " 'feature_01__lag7744',\n",
       " 'feature_23__lag7744',\n",
       " 'responder_5_prev2day_close',\n",
       " 'feature_50__lag7744',\n",
       " 'feature_27__ret10',\n",
       " 'feature_38__ret50',\n",
       " 'feature_29__rz30',\n",
       " 'feature_05__lag1936',\n",
       " 'feature_58__ewm10',\n",
       " 'feature_15__rmean30',\n",
       " 'feature_48__rmean30',\n",
       " 'feature_05__lag968',\n",
       " 'feature_42__ewm10',\n",
       " 'feature_20__lag1936',\n",
       " 'feature_51__ewm5',\n",
       " 'feature_42__lag5808',\n",
       " 'responder_4_close_roll30_std',\n",
       " 'feature_04__rmean30',\n",
       " 'feature_69__ewm50',\n",
       " 'feature_46__rstd30',\n",
       " 'feature_12__ewm50',\n",
       " 'feature_13__ewm5',\n",
       " 'responder_3_close_roll7_mean',\n",
       " 'responder_7_prev_tail_d1',\n",
       " 'responder_4_same_t_prev3',\n",
       " 'responder_4_overnight_gap',\n",
       " 'feature_01__rstd30',\n",
       " 'feature_37__rmean7',\n",
       " 'responder_4_prev_tail_lag10',\n",
       " 'feature_42__lag2904',\n",
       " 'feature_46__rmean30',\n",
       " 'feature_20__lag900',\n",
       " 'feature_26__rz14',\n",
       " 'feature_59__rmean30',\n",
       " 'feature_57__ewm50',\n",
       " 'responder_7_prev_tail_d10',\n",
       " 'responder_0_prev_tail_lag967',\n",
       " 'feature_78__diff50',\n",
       " 'feature_51__rmean30',\n",
       " 'feature_71__ewm50',\n",
       " 'feature_21__lag2904',\n",
       " 'feature_39__lag2904',\n",
       " 'feature_47__ewm5',\n",
       " 'feature_39__rmean14',\n",
       " 'feature_20__lag6776',\n",
       " 'time_idx']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_ready.collect_schema().names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41652fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化使用训练集的上限日期 = 1539\n"
     ]
    }
   ],
   "source": [
    "# 明确要标准化的列\n",
    "z_cols = [c for c in TO_Z_INPUTS if c in lf_ready.collect_schema().names()]\n",
    "\n",
    "# 取第一个 fold 的训练集最后一天，作为统计 z-score 的上界\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"标准化使用训练集的上限日期 = {stats_hi}\")\n",
    "\n",
    "# ========== 4) 连续特征清洗 + Z-score ==========\n",
    "inf2null_exprs  = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c) for c in z_cols]\n",
    "#isna_flag_exprs = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\") for c in z_cols]\n",
    "\n",
    "ffill_exprs = [pl.col(c).forward_fill().over(G_SYM).fill_null(0.0).alias(c) for c in z_cols]\n",
    "lf_clean = (\n",
    "    lf_ready.with_columns(inf2null_exprs).with_columns(ffill_exprs) #.with_columns(isna_flag_exprs)\n",
    ")\n",
    "\n",
    "\n",
    "# 开始计算 z-score\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .group_by(G_SYM)\n",
    "            .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in z_cols] +\n",
    "                [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in z_cols])\n",
    ")\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(G_DATE) <= stats_hi)\n",
    "            .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in z_cols] +\n",
    "                    [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in z_cols])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00fb3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\").join(lf_stats_sym, on=G_SYM, how=\"left\").sort([G_SYM, \"time_idx\"])\n",
    "\n",
    "eps = 1e-6\n",
    "#Z_COLS, NAMARK_COLS = [], [f\"{c}__isna\" for c in RAW_FEATURES]\n",
    "\n",
    "Z_COLS = []\n",
    "for c in z_cols:\n",
    "    mu_sym, std_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_glb, std_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    mu_use, std_use = f\"{c}_mu_use\", f\"{c}_std_use\"\n",
    "    z_name = f\"{c}_z\"\n",
    "\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_sym).is_null()).then(pl.col(mu_glb)).otherwise(pl.col(mu_sym)).alias(mu_use),\n",
    "        pl.when(pl.col(std_sym).is_null() | (pl.col(std_sym) == 0)).then(pl.col(std_glb)).otherwise(pl.col(std_sym)).alias(std_use),\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(mu_use)) / (pl.col(std_use) + eps)).alias(z_name)\n",
    "    ).drop([mu_glb, std_glb, mu_sym, std_sym, mu_use, std_use])\n",
    "\n",
    "    Z_COLS.append(z_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d34d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_COLS = [G_SYM, G_DATE, G_TIME, \"time_idx\", WEIGHT_COL, TARGET_COL, *TIME_FEATURES] + Z_COLS \n",
    "lf_out = lf_z.select(OUT_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0575179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-13 10:53:47] cleaned data saved to /mnt/data/js/exp/v1/tft/panel/clean.parquet\n"
     ]
    }
   ],
   "source": [
    "# 保存lf_out到本地\n",
    "clean_path_local = P(\"local\", \"tft/panel/clean.parquet\")\n",
    "Path(clean_path_local).parent.mkdir(parents=True, exist_ok=True)\n",
    "lf_out.collect(streaming=True).write_parquet(clean_path_local, compression=\"zstd\")\n",
    "print(f\"[{_now()}] cleaned data saved to {clean_path_local}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028cc3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a5b5e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 606)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>time_idx</th><th>weight</th><th>responder_6</th><th>time_bucket</th><th>time_pos</th><th>time_sin</th><th>time_cos</th><th>feature_06_z</th><th>feature_36_z</th><th>feature_04_z</th><th>feature_75__rstd14_z</th><th>feature_60__cs_z_z</th><th>feature_59_z</th><th>responder_0_close_roll30_std_z</th><th>feature_59__rstd30_z</th><th>feature_07_z</th><th>feature_61__lag900_z</th><th>feature_60_z</th><th>feature_61__lag1936_z</th><th>responder_6_prevday_std_z</th><th>responder_8_prev_tail_lag10_z</th><th>feature_61__ret50_z</th><th>feature_61__lag6776_z</th><th>feature_25__diff50_z</th><th>feature_76__rstd7_z</th><th>feature_48_z</th><th>responder_5_prevday_std_z</th><th>feature_60__rstd30_z</th><th>responder_5_prevday_mean_z</th><th>feature_51__rmean14_z</th><th>responder_1_close_roll14_std_z</th><th>feature_37__rstd30_z</th><th>responder_2_close_roll30_std_z</th><th>feature_31__diff50_z</th><th>&hellip;</th><th>feature_58__ewm10_z</th><th>feature_15__rmean30_z</th><th>feature_48__rmean30_z</th><th>feature_05__lag968_z</th><th>feature_42__ewm10_z</th><th>feature_20__lag1936_z</th><th>feature_51__ewm5_z</th><th>feature_42__lag5808_z</th><th>responder_4_close_roll30_std_z</th><th>feature_04__rmean30_z</th><th>feature_69__ewm50_z</th><th>feature_46__rstd30_z</th><th>feature_12__ewm50_z</th><th>feature_13__ewm5_z</th><th>responder_3_close_roll7_mean_z</th><th>responder_7_prev_tail_d1_z</th><th>responder_4_same_t_prev3_z</th><th>responder_4_overnight_gap_z</th><th>feature_01__rstd30_z</th><th>feature_37__rmean7_z</th><th>responder_4_prev_tail_lag10_z</th><th>feature_42__lag2904_z</th><th>feature_46__rmean30_z</th><th>feature_20__lag900_z</th><th>feature_26__rz14_z</th><th>feature_59__rmean30_z</th><th>feature_57__ewm50_z</th><th>responder_7_prev_tail_d10_z</th><th>responder_0_prev_tail_lag967_z</th><th>feature_78__diff50_z</th><th>feature_51__rmean30_z</th><th>feature_71__ewm50_z</th><th>feature_21__lag2904_z</th><th>feature_39__lag2904_z</th><th>feature_47__ewm5_z</th><th>feature_39__rmean14_z</th><th>feature_20__lag6776_z</th></tr><tr><td>i32</td><td>i32</td><td>i32</td><td>i64</td><td>f32</td><td>f32</td><td>u8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>1500</td><td>0</td><td>0</td><td>4.426604</td><td>-1.314976</td><td>0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>-0.180863</td><td>-3.584806</td><td>2.17891</td><td>2.437274</td><td>-1.09928</td><td>2.820399</td><td>-1.801402</td><td>-0.038881</td><td>-0.950069</td><td>-1.429659</td><td>0.977931</td><td>-1.291016</td><td>1.156231</td><td>-0.001653</td><td>-0.005114</td><td>0.955494</td><td>-5.083159</td><td>2.984312</td><td>3.362084</td><td>0.685297</td><td>1.701188</td><td>-0.05778</td><td>1.095647</td><td>-0.537389</td><td>0.764006</td><td>0.713745</td><td>-3.067613</td><td>&hellip;</td><td>-1.30399</td><td>-0.130877</td><td>-2.700641</td><td>0.640619</td><td>-1.858955</td><td>0.786799</td><td>0.096759</td><td>-0.10717</td><td>-1.125324</td><td>-0.421482</td><td>0.751405</td><td>-0.090083</td><td>0.87785</td><td>2.06421</td><td>-0.187472</td><td>0.232704</td><td>-0.322004</td><td>0.217516</td><td>1.079885</td><td>-1.206706</td><td>0.318356</td><td>-1.315562</td><td>-1.312192</td><td>0.646878</td><td>-0.010013</td><td>-2.694227</td><td>-1.229489</td><td>-1.162042</td><td>-0.332934</td><td>0.703254</td><td>0.756416</td><td>0.652802</td><td>-0.05723</td><td>-1.377247</td><td>-1.191844</td><td>-0.97941</td><td>0.246667</td></tr><tr><td>0</td><td>1500</td><td>1</td><td>1</td><td>4.426604</td><td>-0.671698</td><td>0</td><td>1.0</td><td>0.006491</td><td>0.999979</td><td>1.14151</td><td>-3.426875</td><td>2.059115</td><td>3.548316</td><td>1.965287</td><td>2.002331</td><td>-1.801402</td><td>0.453161</td><td>-0.12851</td><td>-1.429659</td><td>1.501385</td><td>-1.291016</td><td>1.156231</td><td>-0.001653</td><td>-0.005929</td><td>0.960884</td><td>-5.083159</td><td>3.588061</td><td>3.42849</td><td>0.685297</td><td>1.737849</td><td>-0.05778</td><td>1.548055</td><td>-0.537389</td><td>1.165513</td><td>0.713745</td><td>-3.067613</td><td>&hellip;</td><td>-1.376462</td><td>-0.145171</td><td>-2.320478</td><td>0.539652</td><td>-1.967689</td><td>0.786799</td><td>2.004433</td><td>-0.10717</td><td>-1.125324</td><td>-0.310679</td><td>0.709988</td><td>1.306695</td><td>0.78464</td><td>2.980228</td><td>-0.187472</td><td>0.232704</td><td>-0.301718</td><td>0.217516</td><td>1.094302</td><td>-1.019356</td><td>0.318356</td><td>-1.315562</td><td>-1.222878</td><td>0.646878</td><td>12.438131</td><td>-2.368961</td><td>-1.094448</td><td>-1.162042</td><td>-0.332934</td><td>1.222962</td><td>0.975032</td><td>0.842836</td><td>-0.05723</td><td>-1.377247</td><td>-1.179356</td><td>-1.073758</td><td>0.246667</td></tr><tr><td>0</td><td>1500</td><td>2</td><td>2</td><td>4.426604</td><td>-0.290113</td><td>0</td><td>2.0</td><td>0.012981</td><td>0.999916</td><td>1.216413</td><td>-2.992304</td><td>2.466613</td><td>4.243918</td><td>1.655539</td><td>3.437272</td><td>-1.801402</td><td>0.67939</td><td>0.041061</td><td>-1.429659</td><td>2.003592</td><td>-1.291016</td><td>1.156231</td><td>-0.001653</td><td>-0.006682</td><td>0.966135</td><td>-5.083159</td><td>3.720413</td><td>3.450294</td><td>0.685297</td><td>1.840076</td><td>-0.05778</td><td>2.162183</td><td>-0.537389</td><td>1.468257</td><td>0.713745</td><td>-3.067613</td><td>&hellip;</td><td>-1.435757</td><td>-0.159426</td><td>-1.913798</td><td>0.636232</td><td>-2.056653</td><td>0.786799</td><td>3.323298</td><td>-0.10717</td><td>-1.125324</td><td>-0.229539</td><td>0.683817</td><td>2.742198</td><td>0.698821</td><td>3.612778</td><td>-0.187472</td><td>0.232704</td><td>-0.259004</td><td>0.217516</td><td>0.977031</td><td>-0.914513</td><td>0.318356</td><td>-1.315562</td><td>-1.121104</td><td>0.646878</td><td>8.511765</td><td>-2.149735</td><td>-0.988469</td><td>-1.162042</td><td>-0.332934</td><td>1.252574</td><td>1.270561</td><td>1.031847</td><td>-0.05723</td><td>-1.377247</td><td>-1.060132</td><td>-1.224522</td><td>0.246667</td></tr><tr><td>0</td><td>1500</td><td>3</td><td>3</td><td>4.426604</td><td>-0.338634</td><td>0</td><td>3.0</td><td>0.019471</td><td>0.99981</td><td>1.09391</td><td>-2.136811</td><td>2.560048</td><td>4.647358</td><td>1.728513</td><td>3.50113</td><td>-1.801402</td><td>1.126933</td><td>0.140645</td><td>-1.429659</td><td>1.644447</td><td>-1.291016</td><td>1.156231</td><td>-0.001653</td><td>-0.007386</td><td>0.971254</td><td>-5.083159</td><td>3.765344</td><td>2.345562</td><td>0.685297</td><td>2.082021</td><td>-0.05778</td><td>2.649216</td><td>-0.537389</td><td>1.640126</td><td>0.713745</td><td>-3.067613</td><td>&hellip;</td><td>-1.484271</td><td>-0.164314</td><td>-1.449671</td><td>0.39771</td><td>-2.129441</td><td>0.786799</td><td>3.695964</td><td>-0.10717</td><td>-1.125324</td><td>-0.137191</td><td>0.659277</td><td>3.277414</td><td>0.600195</td><td>4.061122</td><td>-0.187472</td><td>0.232704</td><td>-0.256394</td><td>0.217516</td><td>0.928757</td><td>-0.793831</td><td>0.318356</td><td>-1.315562</td><td>-1.02599</td><td>0.646878</td><td>6.667307</td><td>-1.59925</td><td>-0.85662</td><td>-1.162042</td><td>-0.332934</td><td>0.757361</td><td>1.4721</td><td>1.221126</td><td>-0.05723</td><td>-1.377247</td><td>-1.012365</td><td>-1.329914</td><td>0.246667</td></tr><tr><td>0</td><td>1500</td><td>4</td><td>4</td><td>4.426604</td><td>-0.530085</td><td>0</td><td>4.0</td><td>0.025961</td><td>0.999663</td><td>2.094465</td><td>-2.683133</td><td>2.424981</td><td>4.563622</td><td>1.193983</td><td>2.798179</td><td>-1.801402</td><td>1.507529</td><td>0.491775</td><td>-1.429659</td><td>1.61425</td><td>-1.291016</td><td>1.156231</td><td>-0.001653</td><td>-0.008048</td><td>0.976251</td><td>-5.083159</td><td>3.212726</td><td>2.931542</td><td>0.685297</td><td>2.247417</td><td>-0.05778</td><td>3.185857</td><td>-0.537389</td><td>1.78513</td><td>0.713745</td><td>-3.067613</td><td>&hellip;</td><td>-1.523965</td><td>-0.167624</td><td>-1.080498</td><td>0.226535</td><td>-2.188995</td><td>0.786799</td><td>4.37993</td><td>-0.10717</td><td>-1.125324</td><td>-0.044513</td><td>0.638655</td><td>4.161686</td><td>0.568348</td><td>4.286509</td><td>-0.187472</td><td>0.232704</td><td>-0.147207</td><td>0.217516</td><td>0.89683</td><td>-0.694102</td><td>0.318356</td><td>-1.315562</td><td>-0.916114</td><td>0.646878</td><td>5.509173</td><td>-1.087329</td><td>-0.743131</td><td>-1.162042</td><td>-0.332934</td><td>0.648716</td><td>1.760923</td><td>1.410254</td><td>-0.05723</td><td>-1.377247</td><td>-0.862105</td><td>-1.390056</td><td>0.246667</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 606)\n",
       "┌───────────┬─────────┬─────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ time_idx ┆ … ┆ feature_39 ┆ feature_47 ┆ feature_39 ┆ feature_20 │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ ---      ┆   ┆ __lag2904_ ┆ __ewm5_z   ┆ __rmean14_ ┆ __lag6776_ │\n",
       "│ i32       ┆ i32     ┆ i32     ┆ i64      ┆   ┆ z          ┆ ---        ┆ z          ┆ z          │\n",
       "│           ┆         ┆         ┆          ┆   ┆ ---        ┆ f32        ┆ ---        ┆ ---        │\n",
       "│           ┆         ┆         ┆          ┆   ┆ f32        ┆            ┆ f32        ┆ f32        │\n",
       "╞═══════════╪═════════╪═════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0         ┆ 1500    ┆ 0       ┆ 0        ┆ … ┆ -1.377247  ┆ -1.191844  ┆ -0.97941   ┆ 0.246667   │\n",
       "│ 0         ┆ 1500    ┆ 1       ┆ 1        ┆ … ┆ -1.377247  ┆ -1.179356  ┆ -1.073758  ┆ 0.246667   │\n",
       "│ 0         ┆ 1500    ┆ 2       ┆ 2        ┆ … ┆ -1.377247  ┆ -1.060132  ┆ -1.224522  ┆ 0.246667   │\n",
       "│ 0         ┆ 1500    ┆ 3       ┆ 3        ┆ … ┆ -1.377247  ┆ -1.012365  ┆ -1.329914  ┆ 0.246667   │\n",
       "│ 0         ┆ 1500    ┆ 4       ┆ 4        ┆ … ┆ -1.377247  ┆ -0.862105  ┆ -1.390056  ┆ 0.246667   │\n",
       "└───────────┴─────────┴─────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean 数据导入\n",
    "clean_path_local = P(\"local\", \"tft/panel/clean.parquet\")\n",
    "lf_clean = pl.scan_parquet(clean_path_local)\n",
    "lf_clean.limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4c16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_REALS = [c for c in lf_clean.collect_schema().names() if c not in (G_SYM, G_DATE, G_TIME, \"time_idx\", \"time_bucket\", WEIGHT_COL, TARGET_COL)]\n",
    "\n",
    "KNOWN_CATEGORIES = [\"time_bucket\"] if 'time_bucket' in lf_clean.collect_schema().names() else []\n",
    "\n",
    "UNSCALE_COLS = KNOWN_REALS\n",
    "\n",
    "TRAIN_COLS = [c for c in lf_clean.collect_schema().names() if c not in (G_DATE, G_TIME)]\n",
    "\n",
    "# 定义 identity scalers\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8f841",
   "metadata": {},
   "source": [
    "## try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9e78a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train 1500..1539 (40 days), val 1540..1545 (6 days)\n",
      "[fold 0] train idx up to 38719, val idx 38720..44527\n"
     ]
    }
   ],
   "source": [
    "# 取第一折先试探一下\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[0]\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    pl.scan_parquet(clean_path_local)\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    "    .sort_values([G_SYM, \"time_idx\"])\n",
    ")\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a6b8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pdf_data[TRAIN_COLS]\n",
    "\n",
    "identity_scalers = {name: None for name in UNSCALE_COLS}\n",
    "base_ds = TimeSeriesDataSet(\n",
    "    t_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, \n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, \n",
    "    min_prediction_length=PRED_LEN,\n",
    "    \n",
    "    static_categoricals=[G_SYM],\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                          \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in KNOWN_CATEGORIES else None,\n",
    "                          },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb4d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集，验证集\n",
    "train_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        idx.time_idx_last <= train_end_idx\n",
    "    ),\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "val_ds = base_ds.filter(\n",
    "    lambda idx: (\n",
    "        (idx.time_idx_first_prediction == val_start_idx + ENC_LEN) &\n",
    "        \n",
    "        (idx.time_idx_last <= val_end_idx)\n",
    "    ),\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d27cc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] train_loader batches = 2921\n",
      "[debug] val_loader batches = 1\n"
     ]
    }
   ],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e93a9a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 842.4k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e36a2a3d814098add9727a0d6e77fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 95 steps due to diverging loss.\n",
      "Restoring states from the checkpoint path at /home/admin_ml/Jackson/projects/js/JS/.lr_find_c24391f4-c967-4396-b7db-9d4f1ed942c5.ckpt\n",
      "Restored all states from the checkpoint at /home/admin_ml/Jackson/projects/js/JS/.lr_find_c24391f4-c967-4396-b7db-9d4f1ed942c5.ckpt\n",
      "Learning rate set to 1.1220184543019632e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 1.1220184543019632e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lp.seed_everything(42)\n",
    "trainer = lp.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=HIDDEN // 2,  # set to <= hidden_size\n",
    "    loss=RMSE(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "    \n",
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c11547",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b07cdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train 1500..1539 (40 days), val 1540..1545 (6 days)\n"
     ]
    }
   ],
   "source": [
    "lp.seed_everything(42) \n",
    "\n",
    "# ========== 8) 训练（按 CV 折） ========== 先取第一折\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "#for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "####################################\n",
    "fold_id = 0\n",
    "train_days, val_days = folds_by_day[fold_id]\n",
    "####################################\n",
    "\n",
    "print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "    f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "# 明确日期：\n",
    "train_start_date = int(train_days[0])\n",
    "train_end_date   = int(train_days[-1])\n",
    "val_start_date   = int(val_days[0])\n",
    "val_end_date     = int(val_days[-1])      \n",
    "\n",
    "# 提取数据\n",
    "date_range = (train_start_date, val_end_date)\n",
    "pdf_data = (\n",
    "    lf_clean\n",
    "    .filter(pl.col(G_DATE).is_between(train_start_date, val_end_date, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ") \n",
    "\n",
    "pdf_data[G_SYM] = pdf_data[G_SYM].astype(\"str\")\n",
    "if \"time_bucket\" in pdf_data.columns:\n",
    "    pdf_data[\"time_bucket\"] = pdf_data[\"time_bucket\"].astype(\"str\")\n",
    "pdf_data.sort_values([G_SYM, \"time_idx\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a51eb04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train idx up to 38719, val idx 38720..44527\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 明确 indexes:\n",
    "train_end_idx = pdf_data.loc[pdf_data[G_DATE] == train_end_date, \"time_idx\"].max()\n",
    "val_start_idx = pdf_data.loc[pdf_data[G_DATE] == val_start_date, \"time_idx\"].min()\n",
    "val_end_idx   = pdf_data.loc[pdf_data[G_DATE] == val_end_date, \"time_idx\"].max()\n",
    "assert pd.notna(train_end_idx) and pd.notna(val_start_idx) and pd.notna(val_end_idx), \"train/val idx not found\"\n",
    "train_end_idx, val_start_idx, val_end_idx = int(train_end_idx), int(val_start_idx), int(val_end_idx)\n",
    "print(f\"[fold {fold_id}] train idx up to {train_end_idx}, val idx {val_start_idx}..{val_end_idx}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ede7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建训练集 timeseries dataset\n",
    "\n",
    "t_data = pdf_data[TRAIN_COLS]\n",
    "train_df = t_data.loc[t_data[\"time_idx\"] <= train_end_idx]\n",
    "\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=TARGET_COL,\n",
    "    group_ids=[G_SYM],\n",
    "    weight=WEIGHT_COL,\n",
    "    max_encoder_length=ENC_LEN, min_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN, min_prediction_length=PRED_LEN,\n",
    "    time_varying_known_reals =KNOWN_REALS,\n",
    "    time_varying_known_categoricals=KNOWN_CATEGORIES,\n",
    "    static_categoricals=[G_SYM],\n",
    "    categorical_encoders={G_SYM: NaNLabelEncoder(add_nan=True),\n",
    "                        \"time_bucket\": NaNLabelEncoder(add_nan=True) if \"time_bucket\" in pdf_data.columns else None},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        method=\"standard\", groups=[G_SYM], center=True, scale_by_group=False),\n",
    "    scalers=identity_scalers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1a95f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 验证集复用 train_ds 的所有 encoders/normalizer（不泄漏）\n",
    "\n",
    "val_df = t_data.loc[t_data[\"time_idx\"].between(val_start_idx, val_end_idx, inclusive=\"both\")]\n",
    "val_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds,\n",
    "    val_df,\n",
    "    min_prediction_idx=val_start_idx+ENC_LEN,\n",
    "    stop_randomization=True,\n",
    "    predict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0412450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] train_loader batches = 2921\n",
      "[debug] val_loader batches = 106\n"
     ]
    }
   ],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_loader = train_ds.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "print(f\"[debug] train_loader batches = {n_train_batches}\")\n",
    "assert n_train_batches > 0, \"Empty train dataloader. Check min_prediction_idx/ENC_LEN/date windows.\"\n",
    "\n",
    "val_loader = val_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE*4,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "n_val_batches = len(val_loader)\n",
    "print(f\"[debug] val_loader batches = {n_val_batches}\")\n",
    "assert n_val_batches > 0, \"Empty val dataloader. Check min_prediction_idx/ENC_LEN/date windows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36e91a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 555    | train\n",
      "3  | prescalers                         | ModuleDict                      | 9.6 K  | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 417 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 417 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 17     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "842 K     Trainable params\n",
      "0         Non-trainable params\n",
      "842 K     Total params\n",
      "3.370     Total estimated model params size (MB)\n",
      "17611     Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d070944dca4141bf11e6ef687f01f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 42\u001b[0m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m lp\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     12\u001b[0m                     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m                     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m                     accumulate_grad_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     27\u001b[0m                     )\n\u001b[1;32m     29\u001b[0m tft \u001b[38;5;241m=\u001b[39m TemporalFusionTransformer\u001b[38;5;241m.\u001b[39mfrom_dataset(\n\u001b[1;32m     30\u001b[0m     train_ds,\n\u001b[1;32m     31\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mLR,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     reduce_on_plateau_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:560\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     52\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:598\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    591\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    592\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    594\u001b[0m     ckpt_path,\n\u001b[1;32m    595\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m )\n\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1011\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1053\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1082\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1082\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:437\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:329\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 329\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    332\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:734\u001b[0m, in \u001b[0;36mBaseModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    733\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 734\u001b[0m     log, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m     log\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_log(x, y, out, batch_idx))\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step_outputs\u001b[38;5;241m.\u001b[39mappend(log)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:911\u001b[0m, in \u001b[0;36mBaseModel.step\u001b[0;34m(self, x, y, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/_tft.py:553\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_variables) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# static embeddings will be constant over entire batch\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     static_embedding \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    550\u001b[0m         name: input_vectors[name][:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_variables\n\u001b[1;32m    551\u001b[0m     }\n\u001b[1;32m    552\u001b[0m     static_embedding, static_variable_selection \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 553\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_variable_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m     )\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     static_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    557\u001b[0m         (x_cont\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mhidden_size),\n\u001b[1;32m    558\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    559\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    560\u001b[0m     )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:372\u001b[0m, in \u001b[0;36mVariableSelectionNetwork.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    370\u001b[0m variable_embedding \u001b[38;5;241m=\u001b[39m x[name]\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprescalers:\n\u001b[0;32m--> 372\u001b[0m     variable_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprescalers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m weight_inputs\u001b[38;5;241m.\u001b[39mappend(variable_embedding)\n\u001b[1;32m    374\u001b[0m var_outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_variable_grns[name](variable_embedding))\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility"
     ]
    }
   ],
   "source": [
    "# 8.6 callbacks/logger/trainer\n",
    "ckpt_dir_fold = Path(CKPTS_DIR) / f\"fold_{fold_id}\"\n",
    "ckpt_dir_fold.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, check_on_train_epoch_end=False),\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, dirpath=ckpt_dir_fold.as_posix(), filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_loss:.5f}}\", save_on_train_epoch_end=False),\n",
    "            LearningRateMonitor(logging_interval=\"step\"),]\n",
    "RUN_NAME = (f\"f{fold_id}\"f\"_E{MAX_EPOCHS}\"f\"_lr{LR}\"f\"_bs{BATCH_SIZE}\"f\"_enc{ENC_LEN}_dec{DEC_LEN}\"f\"_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "logger = TensorBoardLogger(save_dir=LOGS_DIR.as_posix(),name=\"tft\",version=RUN_NAME,default_hp_metric=False)\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=3,\n",
    "                    accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    enable_model_summary=True,\n",
    "                    gradient_clip_val=1.0,\n",
    "                    gradient_clip_algorithm=\"norm\",\n",
    "                    fast_dev_run=False,\n",
    "                    limit_train_batches=50,\n",
    "                    limit_val_batches=25,\n",
    "                    val_check_interval=0.25,\n",
    "                    num_sanity_val_steps=2,\n",
    "                    log_every_n_steps=10,\n",
    "                    callbacks=callbacks,\n",
    "                    logger=logger,\n",
    "                    accumulate_grad_batches=1,\n",
    "                    )\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN,\n",
    "    attention_head_size=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN // 2,\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[RMSE()],\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
    "best_path = ckpt_cb.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_path)\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_loader,\n",
    "    return_y=True,\n",
    "    trainer_kwargs=dict(accelerator=\"gpu\")\n",
    ")\n",
    "y_pred = predictions.output\n",
    "y_true, w = predictions.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (w * (y_true - y_pred).pow(2)).sum()\n",
    "den = (w * y_true.pow(2)).sum()\n",
    "\n",
    "wr2 = 1.0 - num / (den + eps)\n",
    "print(f\"wr2 after training: {wr2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (torch.square(y_true - y_pred) * w).sum()\n",
    "den = (torch.square(y_true) * w).sum()  \n",
    "wr2 = 1 - num / den\n",
    "print(f\"wr2 after training: {wr2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (js-uv)",
   "language": "python",
   "name": "js-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
