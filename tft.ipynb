{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34503ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (572149434.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -c \"import torch;print(torch.__version__, torch.version.cuda); import torch; print(torch.cuda.is_available())\"\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库（stdlib） ─────────────────────────────────────────────────────────────\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:cudaMallocAsync\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "# ── 第三方（third-party） ───────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import torch\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import (\n",
    "    TimeSeriesDataSet,\n",
    "    TemporalFusionTransformer,\n",
    ")\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import (\n",
    "    NaNLabelEncoder,\n",
    ")\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "\n",
    "def _now(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a5d780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-03 17:16:03] loaded 787 features from /mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\n"
     ]
    }
   ],
   "source": [
    "# 导入文件 /mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\n",
    "input_file = \"/mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\"\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    data = file.read()\n",
    "lines = data.split('\\n')\n",
    "features = [line.split()[0] for line in lines if line.strip() and not line.startswith('#')]\n",
    "print(f\"[{_now()}] loaded {len(features)} features from {input_file}\")  \n",
    "\n",
    "# ── 本地（local） ────────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad855cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time_pos',\n",
       " 'time_sin',\n",
       " 'time_cos',\n",
       " 'time_bucket',\n",
       " 'feature_36',\n",
       " 'feature_06',\n",
       " 'feature_08__ewm5',\n",
       " 'responder_5_prevday_std',\n",
       " 'responder_3_prevday_std',\n",
       " 'responder_4_prev_tail_d1',\n",
       " 'feature_53__rstd3',\n",
       " 'feature_16__ewm5',\n",
       " 'feature_01__ewm5',\n",
       " 'feature_21__rz3',\n",
       " 'responder_7_prevday_std',\n",
       " 'responder_8_prevday_mean',\n",
       " 'feature_38__ewm5',\n",
       " 'feature_05__ewm5',\n",
       " 'responder_1_close_roll3_std',\n",
       " 'responder_3_prevday_mean',\n",
       " 'feature_37__ewm5',\n",
       " 'responder_4_prevday_std',\n",
       " 'responder_3_prev_tail_d1',\n",
       " 'responder_6_prevday_std',\n",
       " 'responder_2_prevday_mean',\n",
       " 'responder_0_prevday_std',\n",
       " 'responder_3_prev2day_close',\n",
       " 'responder_8_prevday_std',\n",
       " 'responder_2_prev_tail_d1',\n",
       " 'responder_4_prevday_mean',\n",
       " 'feature_04',\n",
       " 'feature_66__ewm5',\n",
       " 'responder_8_close_roll3_std',\n",
       " 'responder_1_prev_tail_d1',\n",
       " 'feature_16',\n",
       " 'responder_7_prevday_mean',\n",
       " 'responder_5_prev2day_close',\n",
       " 'responder_5_close_roll3_mean',\n",
       " 'responder_8_prev2day_close',\n",
       " 'feature_07__ewm5',\n",
       " 'responder_0_close_roll3_mean',\n",
       " 'responder_5_prevday_mean',\n",
       " 'responder_4_prevday_close_minus_mean',\n",
       " 'responder_7_close_roll3_mean',\n",
       " 'feature_69',\n",
       " 'responder_1_prev2day_close',\n",
       " 'feature_41__rstd3',\n",
       " 'responder_5_prev_tail_d1',\n",
       " 'responder_2_close_roll3_mean',\n",
       " 'responder_1_close_roll3_mean',\n",
       " 'responder_3_close_roll3_std',\n",
       " 'responder_5_prev_tail_lag2',\n",
       " 'responder_1_prevday_mean',\n",
       " 'responder_7_close_roll3_std',\n",
       " 'responder_2_prevday_std',\n",
       " 'feature_26__rstd3',\n",
       " 'responder_8_close_roll3_mean',\n",
       " 'feature_42__rstd3',\n",
       " 'feature_27__rz3',\n",
       " 'responder_0_close_roll3_std',\n",
       " 'feature_25__rz3',\n",
       " 'responder_5_close_roll3_std',\n",
       " 'responder_2_close_roll3_std',\n",
       " 'feature_22',\n",
       " 'feature_31__ewm5',\n",
       " 'responder_1_prevday_std',\n",
       " 'feature_61__ret3',\n",
       " 'responder_0_prevday_mean',\n",
       " 'feature_72__ewm5',\n",
       " 'feature_20',\n",
       " 'feature_31__rstd3',\n",
       " 'feature_29__rz3',\n",
       " 'feature_38__rmean3',\n",
       " 'feature_58__ewm5',\n",
       " 'responder_8_prev_tail_d1',\n",
       " 'responder_4_prev_tail_lag2',\n",
       " 'responder_4_close_roll3_mean',\n",
       " 'feature_15__rstd3',\n",
       " 'responder_6_prev_tail_lag2',\n",
       " 'feature_47__ewm5',\n",
       " 'feature_61__rstd3',\n",
       " 'feature_61__rz3',\n",
       " 'feature_22__lag1',\n",
       " 'feature_33__ewm5',\n",
       " 'feature_30__rz3',\n",
       " 'responder_6_prevday_mean',\n",
       " 'feature_19__ewm5',\n",
       " 'responder_5_overnight_gap',\n",
       " 'feature_22__rz3',\n",
       " 'responder_3_close_roll3_mean',\n",
       " 'feature_42__ewm5',\n",
       " 'feature_31__rmean3',\n",
       " 'responder_4_overnight_gap',\n",
       " 'feature_58',\n",
       " 'responder_8_prev_tail_lag2',\n",
       " 'feature_21__rmean3',\n",
       " 'feature_69__ewm5',\n",
       " 'feature_24',\n",
       " 'feature_44__rstd3',\n",
       " 'responder_7_prevday_close_minus_mean',\n",
       " 'feature_13__ewm5',\n",
       " 'responder_4_close_roll3_std',\n",
       " 'responder_6_prev2day_close',\n",
       " 'feature_23__rz3',\n",
       " 'feature_25__diff3',\n",
       " 'responder_3_overnight_gap',\n",
       " 'feature_25__ret3',\n",
       " 'responder_2_prev_tail_lag2',\n",
       " 'responder_4_prev2day_close',\n",
       " 'responder_3_prevday_close_minus_mean',\n",
       " 'feature_27',\n",
       " 'feature_57__ewm5',\n",
       " 'responder_2_overnight_gap',\n",
       " 'feature_37',\n",
       " 'responder_3_prev_tail_lag2',\n",
       " 'responder_0_overnight_gap',\n",
       " 'responder_0_prev2day_close',\n",
       " 'responder_0_prev_tail_d1',\n",
       " 'responder_2_prev2day_close',\n",
       " 'responder_6_prev_tail_d1',\n",
       " 'feature_22__diff3',\n",
       " 'feature_20__ret3',\n",
       " 'feature_21__ewm5',\n",
       " 'feature_32__ewm5',\n",
       " 'feature_20__rz3',\n",
       " 'feature_77__ewm5',\n",
       " 'responder_6_close_roll3_mean',\n",
       " 'feature_20__rstd3',\n",
       " 'feature_24__diff3',\n",
       " 'feature_01__rmean3',\n",
       " 'responder_8_overnight_gap',\n",
       " 'feature_20__lag1',\n",
       " 'feature_68',\n",
       " 'feature_61__diff3',\n",
       " 'feature_24__rz3',\n",
       " 'feature_23__diff3',\n",
       " 'responder_1_overnight_gap',\n",
       " 'responder_6_close_roll3_std',\n",
       " 'feature_67__ewm5',\n",
       " 'feature_20__ewm5',\n",
       " 'responder_7_prev_tail_lag2',\n",
       " 'feature_39__ewm5',\n",
       " 'feature_60',\n",
       " 'feature_31__rz3',\n",
       " 'feature_29',\n",
       " 'feature_26__rz3',\n",
       " 'feature_50__rstd3',\n",
       " 'prev1_same_t_std_9rep',\n",
       " 'responder_7_overnight_gap',\n",
       " 'feature_53__rmean3',\n",
       " 'feature_07',\n",
       " 'feature_61__rmean3',\n",
       " 'feature_28__rstd3',\n",
       " 'feature_00__ewm5',\n",
       " 'feature_28__rz3',\n",
       " 'feature_30__rstd3',\n",
       " 'responder_7_prev_tail_d1',\n",
       " 'feature_24__rstd3',\n",
       " 'feature_21',\n",
       " 'feature_27__ewm5',\n",
       " 'feature_15__ewm5',\n",
       " 'feature_38',\n",
       " 'feature_25',\n",
       " 'feature_28',\n",
       " 'feature_61',\n",
       " 'responder_7_prev2day_close',\n",
       " 'responder_4_same_t_prev1',\n",
       " 'responder_6_prevday_close_minus_mean',\n",
       " 'feature_27__rstd3',\n",
       " 'responder_0_prev_tail_lag1',\n",
       " 'feature_65__ewm5',\n",
       " 'responder_0_prev_tail_lag2',\n",
       " 'feature_68__ewm5',\n",
       " 'feature_21__diff3',\n",
       " 'feature_14__ewm5',\n",
       " 'feature_52__rstd3',\n",
       " 'feature_35__ewm5',\n",
       " 'feature_20__rmean3',\n",
       " 'feature_08__rmean3',\n",
       " 'feature_28__ewm5',\n",
       " 'feature_69__rmean3',\n",
       " 'feature_26',\n",
       " 'responder_1_prev_tail_lag1',\n",
       " 'responder_1_prev_tail_lag2',\n",
       " 'responder_6_overnight_gap',\n",
       " 'feature_22__rstd3',\n",
       " 'feature_72',\n",
       " 'feature_21__rstd3',\n",
       " 'feature_50__ewm5',\n",
       " 'feature_36__ewm5',\n",
       " 'feature_39__rmean3',\n",
       " 'feature_17__diff3',\n",
       " 'responder_5_prevday_close',\n",
       " 'feature_44__ewm5',\n",
       " 'feature_22__rmean3',\n",
       " 'feature_45__ewm5',\n",
       " 'responder_1_prevday_close_minus_mean',\n",
       " 'feature_30',\n",
       " 'feature_37__rmean3',\n",
       " 'feature_68__rmean3',\n",
       " 'feature_23',\n",
       " 'feature_30__lag1',\n",
       " 'feature_25__rstd3',\n",
       " 'feature_16__diff3',\n",
       " 'feature_18__ewm5',\n",
       " 'feature_33__rmean3',\n",
       " 'feature_14',\n",
       " 'responder_3_prev_tail_lag1',\n",
       " 'feature_47__rmean3',\n",
       " 'feature_02__ewm5',\n",
       " 'feature_61__ewm5',\n",
       " 'feature_12__ewm5',\n",
       " 'feature_04__ewm5',\n",
       " 'feature_49__ewm5',\n",
       " 'feature_29__rstd3',\n",
       " 'feature_50',\n",
       " 'feature_39',\n",
       " 'feature_32__rmean3',\n",
       " 'feature_21__lag1',\n",
       " 'responder_5_prevday_close_minus_mean',\n",
       " 'feature_59',\n",
       " 'responder_2_prevday_close',\n",
       " 'feature_57__rmean3',\n",
       " 'feature_47',\n",
       " 'feature_23__rstd3',\n",
       " 'feature_70__ewm5',\n",
       " 'feature_26__cs_z',\n",
       " 'responder_7_prevday_close',\n",
       " 'responder_4_prev_tail_lag1',\n",
       " 'feature_29__diff3',\n",
       " 'responder_8_prevday_close_minus_mean',\n",
       " 'feature_50__rmean3',\n",
       " 'feature_39__rstd3',\n",
       " 'feature_72__rmean3',\n",
       " 'feature_05__rmean3',\n",
       " 'feature_53__ewm5',\n",
       " 'feature_31',\n",
       " 'feature_22__ewm5',\n",
       " 'feature_74__ewm5',\n",
       " 'responder_0_prevday_close_minus_mean',\n",
       " 'feature_42',\n",
       " 'feature_24__ret3',\n",
       " 'feature_72__lag1',\n",
       " 'feature_24__rmean3',\n",
       " 'feature_70',\n",
       " 'responder_4_same_t_last1_mean',\n",
       " 'feature_01',\n",
       " 'feature_29__ret3',\n",
       " 'feature_34__ewm5',\n",
       " 'responder_4_prevday_close',\n",
       " 'feature_29__rmean3',\n",
       " 'feature_53',\n",
       " 'feature_05',\n",
       " 'feature_09',\n",
       " 'responder_2_prev_tail_lag1',\n",
       " 'feature_55__rstd3',\n",
       " 'feature_07__rmean3',\n",
       " 'responder_7_prev_tail_lag1',\n",
       " 'feature_30__rmean3',\n",
       " 'feature_56__ewm5',\n",
       " 'feature_44__rmean3',\n",
       " 'responder_0_prevday_close',\n",
       " 'feature_24__ewm5',\n",
       " 'feature_46__ewm5',\n",
       " 'feature_42__rz3',\n",
       " 'responder_2_prevday_close_minus_mean',\n",
       " 'feature_15__rz3',\n",
       " 'feature_67__rmean3',\n",
       " 'feature_09__lag1',\n",
       " 'feature_31__lag1',\n",
       " 'feature_30__ewm5',\n",
       " 'feature_33',\n",
       " 'responder_7_same_t_prev1',\n",
       " 'feature_60__ewm5',\n",
       " 'feature_60__cs_z',\n",
       " 'feature_35__rmean3',\n",
       " 'feature_50__rz3',\n",
       " 'feature_23__ewm5',\n",
       " 'feature_15',\n",
       " 'feature_71__ewm5',\n",
       " 'responder_6_prev_tail_lag1',\n",
       " 'feature_26__rmean3',\n",
       " 'feature_31__diff3',\n",
       " 'feature_53__lag1',\n",
       " 'feature_42__rmean3',\n",
       " 'feature_27__diff3',\n",
       " 'feature_11',\n",
       " 'feature_52__ewm5',\n",
       " 'feature_34',\n",
       " 'feature_73__rmean3',\n",
       " 'feature_73__ewm5',\n",
       " 'feature_19__rmean3',\n",
       " 'feature_26__ret3',\n",
       " 'feature_27__lag1',\n",
       " 'feature_55__ewm5',\n",
       " 'feature_66__rmean3',\n",
       " 'feature_03__ewm5',\n",
       " 'feature_78__ewm5',\n",
       " 'feature_16__rmean3',\n",
       " 'feature_26__diff3',\n",
       " 'feature_23__lag1',\n",
       " 'feature_61__lag1',\n",
       " 'feature_50__lag1',\n",
       " 'feature_77__rmean3',\n",
       " 'feature_77',\n",
       " 'feature_29__lag1',\n",
       " 'feature_10__rmean3',\n",
       " 'feature_37__lag1',\n",
       " 'feature_28__rmean3',\n",
       " 'feature_12__rmean3',\n",
       " 'feature_58__rmean3',\n",
       " 'feature_29__ewm5',\n",
       " 'feature_55__rmean3',\n",
       " 'responder_3_prevday_close',\n",
       " 'feature_20__diff3',\n",
       " 'feature_36__rmean3',\n",
       " 'feature_08__rstd3',\n",
       " 'feature_25__rmean3',\n",
       " 'feature_38__lag1',\n",
       " 'feature_28__diff3',\n",
       " 'feature_08',\n",
       " 'responder_8_prev_tail_lag1',\n",
       " 'feature_56__rmean3',\n",
       " 'feature_70__rmean3',\n",
       " 'feature_12',\n",
       " 'responder_7_same_t_last1_mean',\n",
       " 'feature_25__ewm5',\n",
       " 'feature_23__ret3',\n",
       " 'feature_52__rmean3',\n",
       " 'feature_78__rmean3',\n",
       " 'feature_55',\n",
       " 'feature_49__rmean3',\n",
       " 'feature_60__csrank',\n",
       " 'feature_10__ewm5',\n",
       " 'feature_27__rmean3',\n",
       " 'feature_04__lag1',\n",
       " 'feature_30__diff3',\n",
       " 'feature_11__rmean3',\n",
       " 'feature_78',\n",
       " 'feature_09__rmean3',\n",
       " 'feature_02__rmean3',\n",
       " 'feature_70__lag1',\n",
       " 'feature_14__rmean3',\n",
       " 'feature_46__rmean3',\n",
       " 'feature_60__rmean3',\n",
       " 'feature_39__lag1',\n",
       " 'feature_09__ewm5',\n",
       " 'prev1_same_t_mean_9rep',\n",
       " 'feature_25__lag1',\n",
       " 'feature_77__lag1',\n",
       " 'feature_22__ret3',\n",
       " 'feature_28__ret3',\n",
       " 'responder_5_prev_tail_lag1',\n",
       " 'feature_53__rz3',\n",
       " 'feature_05__lag1',\n",
       " 'feature_32__rstd3',\n",
       " 'feature_18__rmean3',\n",
       " 'feature_10',\n",
       " 'feature_71__rmean3',\n",
       " 'feature_39__rz3',\n",
       " 'feature_65__rmean3',\n",
       " 'feature_23__rmean3',\n",
       " 'feature_58__lag1',\n",
       " 'feature_76__ewm5',\n",
       " 'feature_34__rmean3',\n",
       " 'feature_36__lag1',\n",
       " 'feature_04__rmean3',\n",
       " 'responder_1_prevday_close',\n",
       " 'feature_67',\n",
       " 'feature_35',\n",
       " 'feature_32__lag1',\n",
       " 'feature_11__ewm5',\n",
       " 'feature_62__rmean3',\n",
       " 'feature_73',\n",
       " 'feature_51',\n",
       " 'feature_11__lag1',\n",
       " 'feature_42__lag1',\n",
       " 'feature_76__rstd3',\n",
       " 'feature_74__lag1',\n",
       " 'feature_13__lag1',\n",
       " 'feature_62__ewm5',\n",
       " 'feature_78__lag1',\n",
       " 'feature_10__lag1',\n",
       " 'feature_17__ewm5',\n",
       " 'feature_13',\n",
       " 'responder_3_same_t_prev1',\n",
       " 'feature_26__lag1',\n",
       " 'feature_26__csrank',\n",
       " 'feature_21__ret3',\n",
       " 'feature_14__lag1',\n",
       " 'feature_69__lag1',\n",
       " 'feature_74',\n",
       " 'feature_16__lag1',\n",
       " 'feature_00__rmean3',\n",
       " 'feature_28__lag1',\n",
       " 'feature_19',\n",
       " 'feature_27__ret3',\n",
       " 'feature_24__lag1',\n",
       " 'responder_6_prevday_close',\n",
       " 'feature_44',\n",
       " 'feature_49',\n",
       " 'feature_52',\n",
       " 'feature_74__rmean3',\n",
       " 'feature_31__ret3',\n",
       " 'feature_66__lag1',\n",
       " 'feature_73__lag1',\n",
       " 'feature_17__rstd3',\n",
       " 'feature_56',\n",
       " 'responder_8_prevday_close',\n",
       " 'feature_67__lag1',\n",
       " 'feature_17',\n",
       " 'responder_6_same_t_prev1',\n",
       " 'feature_12__lag1',\n",
       " 'feature_55__lag1',\n",
       " 'feature_51__ewm5',\n",
       " 'feature_15__rmean3',\n",
       " 'feature_26__ewm5',\n",
       " 'feature_45__rmean3',\n",
       " 'feature_01__lag1',\n",
       " 'feature_07__lag1',\n",
       " 'feature_57',\n",
       " 'feature_71__lag1',\n",
       " 'feature_73__rstd3',\n",
       " 'feature_75__rmean3',\n",
       " 'feature_32',\n",
       " 'responder_3_same_t_last1_mean',\n",
       " 'feature_56__lag1',\n",
       " 'feature_75__cs_z',\n",
       " 'feature_41__rz3',\n",
       " 'responder_1_same_t_prev1',\n",
       " 'feature_06__diff3',\n",
       " 'feature_47__lag1',\n",
       " 'feature_33__lag1',\n",
       " 'feature_34__lag1',\n",
       " 'feature_60__lag1',\n",
       " 'feature_65',\n",
       " 'feature_33__rstd3',\n",
       " 'responder_1_same_t_last1_mean',\n",
       " 'feature_62',\n",
       " 'feature_13__rmean3',\n",
       " 'feature_57__lag1',\n",
       " 'feature_41',\n",
       " 'feature_58__rstd3',\n",
       " 'feature_75__ewm5',\n",
       " 'feature_52__rz3',\n",
       " 'feature_66',\n",
       " 'feature_44__lag1',\n",
       " 'feature_41__lag1',\n",
       " 'feature_41__ewm5',\n",
       " 'feature_06__ewm5',\n",
       " 'feature_54__ewm5',\n",
       " 'feature_49__lag1',\n",
       " 'feature_76__rmean3',\n",
       " 'feature_76__cs_z',\n",
       " 'feature_41__rmean3',\n",
       " 'feature_46__lag1',\n",
       " 'feature_47__rstd3',\n",
       " 'feature_46',\n",
       " 'feature_43__ewm5',\n",
       " 'feature_08__lag1',\n",
       " 'feature_44__rz3',\n",
       " 'feature_18',\n",
       " 'feature_30__ret3',\n",
       " 'feature_15__lag1',\n",
       " 'feature_45',\n",
       " 'feature_52__lag1',\n",
       " 'feature_59__rstd3',\n",
       " 'feature_75__rstd3',\n",
       " 'feature_45__lag1',\n",
       " 'feature_54',\n",
       " 'responder_6_same_t_last1_mean',\n",
       " 'feature_03',\n",
       " 'feature_74__rstd3',\n",
       " 'feature_40__ewm5',\n",
       " 'feature_48__ewm5',\n",
       " 'feature_65__lag1',\n",
       " 'feature_76',\n",
       " 'feature_68__lag1',\n",
       " 'feature_76__diff3',\n",
       " 'feature_43__diff3',\n",
       " 'feature_00__lag1',\n",
       " 'feature_18__lag1',\n",
       " 'feature_14__rstd3',\n",
       " 'feature_59__ewm5',\n",
       " 'feature_16__rstd3',\n",
       " 'feature_12__rstd3',\n",
       " 'feature_78__rstd3',\n",
       " 'feature_77__rstd3',\n",
       " 'feature_48',\n",
       " 'feature_12__diff3',\n",
       " 'feature_03__lag1',\n",
       " 'responder_8_same_t_last1_mean',\n",
       " 'feature_06__rmean3',\n",
       " 'feature_05__rstd3',\n",
       " 'feature_37__rstd3',\n",
       " 'feature_55__rz3',\n",
       " 'feature_47__diff3',\n",
       " 'feature_63__ewm5',\n",
       " 'feature_64__ewm5',\n",
       " 'feature_17__rmean3',\n",
       " 'feature_69__rstd3',\n",
       " 'feature_75__csrank',\n",
       " 'feature_11__rstd3',\n",
       " 'feature_54__rstd3',\n",
       " 'feature_35__lag1',\n",
       " 'feature_71',\n",
       " 'feature_51__rmean3',\n",
       " 'feature_70__rstd3',\n",
       " 'feature_69__ret3',\n",
       " 'feature_71__diff3',\n",
       " 'feature_60__rstd3',\n",
       " 'feature_75',\n",
       " 'feature_34__rstd3',\n",
       " 'feature_76__lag1',\n",
       " 'feature_07__rstd3',\n",
       " 'feature_43',\n",
       " 'feature_05__ret3',\n",
       " 'feature_52__diff3',\n",
       " 'feature_68__rstd3',\n",
       " 'responder_0_same_t_last1_mean',\n",
       " 'feature_39__ret3',\n",
       " 'feature_34__rz3',\n",
       " 'feature_60__diff3',\n",
       " 'feature_78__diff3',\n",
       " 'feature_67__diff3',\n",
       " 'feature_01__ret3',\n",
       " 'responder_8_same_t_prev1',\n",
       " 'feature_65__ret3',\n",
       " 'feature_36__ret3',\n",
       " 'feature_36__rstd3',\n",
       " 'feature_36__rz3',\n",
       " 'feature_46__rz3',\n",
       " 'feature_46__rstd3',\n",
       " 'feature_45__rz3',\n",
       " 'feature_45__rstd3',\n",
       " 'feature_43__rz3',\n",
       " 'feature_43__rstd3',\n",
       " 'feature_43__rmean3',\n",
       " 'responder_3_same_t_last1_slope',\n",
       " 'responder_2_same_t_last1_slope',\n",
       " 'feature_08__diff3',\n",
       " 'feature_07__diff3',\n",
       " 'feature_05__diff3',\n",
       " 'feature_04__diff3',\n",
       " 'feature_03__diff3',\n",
       " 'feature_02__diff3',\n",
       " 'feature_01__diff3',\n",
       " 'feature_37__rz3',\n",
       " 'feature_49__diff3',\n",
       " 'feature_46__diff3',\n",
       " 'feature_45__diff3',\n",
       " 'feature_44__diff3',\n",
       " 'feature_10__diff3',\n",
       " 'feature_09__diff3',\n",
       " 'feature_40__diff3',\n",
       " 'feature_39__diff3',\n",
       " 'feature_38__diff3',\n",
       " 'feature_37__diff3',\n",
       " 'feature_36__diff3',\n",
       " 'feature_35__diff3',\n",
       " 'feature_34__diff3',\n",
       " 'feature_33__diff3',\n",
       " 'feature_32__diff3',\n",
       " 'feature_00__diff3',\n",
       " 'feature_35__rstd3',\n",
       " 'feature_33__rz3',\n",
       " 'feature_32__rz3',\n",
       " 'feature_76__csrank',\n",
       " 'feature_19__diff3',\n",
       " 'feature_18__diff3',\n",
       " 'feature_15__diff3',\n",
       " 'feature_14__diff3',\n",
       " 'feature_13__diff3',\n",
       " 'feature_11__diff3',\n",
       " 'feature_40__rz3',\n",
       " 'feature_40__rstd3',\n",
       " 'feature_40__rmean3',\n",
       " 'feature_38__rz3',\n",
       " 'feature_38__rstd3',\n",
       " 'feature_35__rz3',\n",
       " 'feature_76__rz3',\n",
       " 'feature_77__rz3',\n",
       " 'feature_63__rmean3',\n",
       " 'feature_63__rstd3',\n",
       " 'feature_68__rz3',\n",
       " 'feature_40',\n",
       " 'feature_63',\n",
       " 'feature_48__diff3',\n",
       " 'feature_67__rz3',\n",
       " 'feature_57__rz3',\n",
       " 'feature_58__rz3',\n",
       " 'feature_59__rmean3',\n",
       " 'feature_59__rz3',\n",
       " 'feature_60__rz3',\n",
       " 'feature_62__rstd3',\n",
       " 'feature_62__rz3',\n",
       " 'feature_64',\n",
       " 'responder_0_same_t_prev1',\n",
       " 'feature_42__ret3',\n",
       " 'feature_43__ret3',\n",
       " 'feature_12__ret3',\n",
       " 'feature_13__ret3',\n",
       " 'feature_14__ret3',\n",
       " 'feature_15__ret3',\n",
       " 'feature_16__ret3',\n",
       " 'feature_75__rz3',\n",
       " 'feature_18__ret3',\n",
       " 'feature_19__ret3',\n",
       " 'feature_75__lag1',\n",
       " 'feature_73__rz3',\n",
       " 'feature_67__rstd3',\n",
       " 'feature_40__ret3',\n",
       " 'feature_78__rz3',\n",
       " 'feature_74__rz3',\n",
       " 'feature_17__ret3',\n",
       " 'feature_06__cs_z',\n",
       " 'feature_06__csrank',\n",
       " 'feature_59__cs_z',\n",
       " 'feature_59__csrank',\n",
       " 'feature_04__cs_z',\n",
       " 'feature_04__csrank',\n",
       " 'responder_2_same_t_last1_mean',\n",
       " 'responder_3_same_t_last1_std',\n",
       " 'feature_47__rz3',\n",
       " 'feature_48__rmean3',\n",
       " 'feature_48__rstd3',\n",
       " 'feature_48__rz3',\n",
       " 'feature_49__rstd3',\n",
       " 'feature_49__rz3',\n",
       " 'feature_51__rstd3',\n",
       " 'feature_51__rz3',\n",
       " 'responder_2_same_t_last1_std',\n",
       " 'responder_4_same_t_last1_std',\n",
       " 'responder_5_same_t_last1_mean',\n",
       " 'responder_5_same_t_last1_std',\n",
       " 'responder_6_same_t_last1_std',\n",
       " 'responder_7_same_t_last1_std',\n",
       " 'responder_8_same_t_last1_std',\n",
       " 'responder_0_same_t_last1_slope',\n",
       " 'responder_1_same_t_last1_slope',\n",
       " 'responder_8_same_t_last1_slope',\n",
       " 'feature_54__rz3',\n",
       " 'feature_56__rstd3',\n",
       " 'feature_56__rz3',\n",
       " 'feature_57__rstd3',\n",
       " 'responder_4_same_t_last1_slope',\n",
       " 'responder_5_same_t_last1_slope',\n",
       " 'responder_6_same_t_last1_slope',\n",
       " 'responder_7_same_t_last1_slope',\n",
       " 'feature_54__rmean3',\n",
       " 'feature_67__ret3',\n",
       " 'feature_68__ret3',\n",
       " 'feature_70__ret3',\n",
       " 'feature_71__ret3',\n",
       " 'feature_72__ret3',\n",
       " 'feature_73__ret3',\n",
       " 'feature_74__ret3',\n",
       " 'feature_41__ret3',\n",
       " 'feature_44__ret3',\n",
       " 'feature_45__ret3',\n",
       " 'feature_46__ret3',\n",
       " 'feature_47__ret3',\n",
       " 'feature_48__ret3',\n",
       " 'feature_49__ret3',\n",
       " 'feature_50__ret3',\n",
       " 'feature_51__ret3',\n",
       " 'feature_75__ret3',\n",
       " 'feature_03__rz3',\n",
       " 'feature_60__ret3',\n",
       " 'feature_00__ret3',\n",
       " 'feature_02__ret3',\n",
       " 'feature_03__ret3',\n",
       " 'feature_04__ret3',\n",
       " 'feature_06__ret3',\n",
       " 'feature_66__ret3',\n",
       " 'feature_08__ret3',\n",
       " 'feature_09__ret3',\n",
       " 'feature_10__ret3',\n",
       " 'feature_11__ret3',\n",
       " 'feature_59__lag1',\n",
       " 'feature_62__ret3',\n",
       " 'feature_63__ret3',\n",
       " 'feature_64__ret3',\n",
       " 'feature_07__ret3',\n",
       " 'responder_1_same_t_last1_std',\n",
       " 'feature_63__rz3',\n",
       " 'feature_64__rmean3',\n",
       " 'feature_64__rstd3',\n",
       " 'feature_64__rz3',\n",
       " 'feature_65__rstd3',\n",
       " 'feature_65__rz3',\n",
       " 'feature_52__ret3',\n",
       " 'feature_66__rz3',\n",
       " 'feature_70__rz3',\n",
       " 'feature_71__rstd3',\n",
       " 'feature_71__rz3',\n",
       " 'feature_72__rstd3',\n",
       " 'feature_72__rz3',\n",
       " 'feature_00',\n",
       " 'feature_02',\n",
       " 'feature_66__rstd3',\n",
       " 'feature_53__ret3',\n",
       " 'feature_54__ret3',\n",
       " 'feature_55__ret3',\n",
       " 'feature_56__ret3',\n",
       " 'feature_57__ret3',\n",
       " 'feature_58__ret3',\n",
       " 'feature_59__ret3',\n",
       " 'responder_0_same_t_last1_std',\n",
       " 'feature_33__ret3',\n",
       " 'feature_34__ret3',\n",
       " 'feature_35__ret3',\n",
       " 'feature_37__ret3',\n",
       " 'feature_38__ret3',\n",
       " 'feature_69__rz3',\n",
       " 'responder_2_same_t_prev1',\n",
       " 'responder_5_same_t_prev1',\n",
       " 'feature_32__ret3',\n",
       " 'feature_42__diff3',\n",
       " 'feature_75__diff3',\n",
       " 'feature_77__diff3',\n",
       " 'feature_57__diff3',\n",
       " 'feature_58__diff3',\n",
       " 'feature_16__rz3',\n",
       " 'feature_17__rz3',\n",
       " 'feature_03__rstd3',\n",
       " 'feature_18__rz3',\n",
       " 'feature_19__rstd3',\n",
       " 'feature_19__rz3',\n",
       " 'feature_09__rz3',\n",
       " 'feature_73__diff3',\n",
       " 'feature_74__diff3',\n",
       " 'feature_02__lag1',\n",
       " 'feature_06__lag1',\n",
       " 'feature_18__rstd3',\n",
       " 'feature_50__diff3',\n",
       " 'feature_51__diff3',\n",
       " 'feature_53__diff3',\n",
       " 'feature_54__diff3',\n",
       " 'feature_55__diff3',\n",
       " 'feature_56__diff3',\n",
       " 'feature_59__diff3',\n",
       " 'feature_41__diff3',\n",
       " 'feature_63__diff3',\n",
       " 'feature_64__diff3',\n",
       " 'feature_65__diff3',\n",
       " 'feature_66__diff3',\n",
       " 'feature_68__diff3',\n",
       " 'feature_69__diff3',\n",
       " 'feature_70__diff3',\n",
       " 'feature_72__diff3',\n",
       " 'feature_62__diff3',\n",
       " 'feature_08__rz3',\n",
       " 'feature_09__rstd3',\n",
       " 'feature_76__ret3',\n",
       " 'feature_77__ret3',\n",
       " 'feature_62__lag1',\n",
       " 'feature_63__lag1',\n",
       " 'feature_64__lag1',\n",
       " 'feature_10__rstd3',\n",
       " 'feature_78__ret3',\n",
       " 'feature_00__rstd3',\n",
       " 'feature_00__rz3',\n",
       " 'feature_01__rstd3',\n",
       " 'feature_01__rz3',\n",
       " 'feature_02__rstd3',\n",
       " 'feature_02__rz3',\n",
       " 'feature_03__rmean3',\n",
       " 'feature_43__lag1',\n",
       " 'feature_10__rz3',\n",
       " 'feature_11__rz3',\n",
       " 'feature_12__rz3',\n",
       " 'feature_13__rstd3',\n",
       " 'feature_13__rz3',\n",
       " 'feature_14__rz3',\n",
       " 'feature_04__rstd3',\n",
       " 'feature_04__rz3',\n",
       " 'feature_07__rz3',\n",
       " 'feature_19__lag1',\n",
       " 'feature_40__lag1',\n",
       " 'feature_48__lag1',\n",
       " 'feature_51__lag1',\n",
       " 'feature_54__lag1',\n",
       " 'feature_05__rz3',\n",
       " 'feature_06__rstd3',\n",
       " 'feature_06__rz3',\n",
       " 'feature_17__lag1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "248c3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ---- 关键配置（按需改）----\n",
    "target_col = cfg[\"target\"]                 # e.g. \"responder_6\"\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "weight_col = cfg[\"weight\"]\n",
    "\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "\n",
    "# 严格匹配基础特征：feature_后跟两位数字、无后缀\n",
    "base_features = [f for f in features if re.fullmatch(r\"feature_\\d{2}\", f)]\n",
    "\n",
    "# 响应类历史特征：以 responder_ 开头\n",
    "resp_his_feats = [f for f in features if f.startswith(\"responder_\")]\n",
    "\n",
    "# 其他派生的 feature_*（有后缀），但排除基础特征\n",
    "feat_his_feats = [f for f in features if f.startswith(\"feature_\") and f not in base_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ab8741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_cols = list(dict.fromkeys(base_features + resp_his_feats + feat_his_feats))\n",
    "\n",
    "need_cols = list(dict.fromkeys([g_sym, g_date, g_time, weight_col, target_col] + time_features + feature_cols))\n",
    "\n",
    "# CV & 训练\n",
    "N_SPLITS   = 2\n",
    "GAP_DAYS   = 7\n",
    "TRAIN_TO_VAL = 4               # 训练:验证 = 4:1\n",
    "ENC_LEN    = 10\n",
    "PRED_LEN   = 1\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3\n",
    "HIDDEN     = 64\n",
    "HEADS      = 2\n",
    "DROPOUT    = 0.1\n",
    "MAX_EPOCHS_PER_SHARD = 1\n",
    "CHUNK_DAYS = 20               # 训练分片：每片多少天\n",
    "\n",
    "print(\"config ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d88ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "data_path  = fs.glob(f\"{panel_dir}/*.parquet\")\n",
    "az_path = [f\"az://{p}\" for p in data_path]\n",
    "lf_data = pl.scan_parquet(az_path, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b6ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局 time_idx 仅一次\n",
    "lf_grid = (\n",
    "    lf_data.select([g_date, g_time]).unique()\n",
    "        .sort([g_date, g_time])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\"); ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "\n",
    "lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86cfd570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-03 17:16:03] lazyframe ready\n"
     ]
    }
   ],
   "source": [
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# 接入 time_idx + 只保留所需列（仍是 Lazy）\n",
    "lf0 = (\n",
    "    lf_data.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "        .select(need_cols + [\"time_idx\"])\n",
    "        .sort([g_date, g_time, g_sym])\n",
    ")\n",
    "\n",
    "print(f\"[{_now()}] lazyframe ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b157ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全量天列表\n",
    "all_days = (\n",
    "    lf0.select(pl.col(g_date)).unique().sort(by=g_date)\n",
    "       .collect(streaming=True).get_column(g_date).to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f408ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造滑动时间窗 CV\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0: return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0: return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days): break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a831bc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615,\n",
       "         1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626,\n",
       "         1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637,\n",
       "         1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648,\n",
       "         1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659,\n",
       "         1660], dtype=int32),\n",
       "  array([1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678,\n",
       "         1679, 1680, 1681, 1682], dtype=int32)),\n",
       " (array([1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630,\n",
       "         1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641,\n",
       "         1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652,\n",
       "         1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663,\n",
       "         1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674,\n",
       "         1675], dtype=int32),\n",
       "  array([1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693,\n",
       "         1694, 1695, 1696, 1697], dtype=int32))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8c5edd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats_hi (for global z-score) = 1660; first-fold train days end at this day.\n"
     ]
    }
   ],
   "source": [
    "# 选“标准化统计区间”上界：第1折训练天的最大值（不含验证）\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"stats_hi (for global z-score) = {stats_hi}; first-fold train days end at this day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8918944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1) 连续特征：一次性处理 inf->null、打缺失标记、组内 ffill、兜底 0 ==========\n",
    "# 先做 flag（要基于原始缺失），再做填充；合并成两次 with_columns，避免在 for 循环里多次改列\n",
    "inf2null_exprs = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
    "                for c in feature_cols] # inf -> null 不产生新列\n",
    "flags_exprs    = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\")\n",
    "                for c in feature_cols] # 产生新列\n",
    "fill_exprs     = [pl.col(c).forward_fill().over(g_sym).fill_null(0.0).alias(c)\n",
    "                for c in feature_cols] # 填充，覆盖原列\n",
    "\n",
    "lf_clean = (\n",
    "    lf0.with_columns(inf2null_exprs)         # inf -> null\n",
    "    .with_columns(flags_exprs)            # 缺失标记（基于原始缺失）\n",
    "    .with_columns(fill_exprs)             # 组内 ffill + 兜底 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f66adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为数据标准化做准备\n",
    "\n",
    "# ========== 2) 训练区间（<= stats_hi）按组计算 mu/std（一次） ==========\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .group_by(g_sym)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in feature_cols] +\n",
    "        [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 训练期全局统计（作为回退）\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in feature_cols] +\n",
    "            [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 3) 把全局统计作为常量列加到每行（cross join 方式）\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\")\n",
    "\n",
    "# 4) join per-symbol 统计，并对每个特征做回退 & z-score\n",
    "\n",
    "lf_z = lf_z.join(lf_stats_sym, on=g_sym, how=\"left\")\n",
    "\n",
    "# 对每个特征做回退 & z-score\n",
    "eps = 1e-6\n",
    "z_cols = []\n",
    "for c in feature_cols:\n",
    "    mu_c_sym, std_c_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_c_glb, std_c_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    c_z = f\"{c}_z\"\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_c_sym).is_null()).then(pl.col(mu_c_glb)).otherwise(pl.col(mu_c_sym)).alias(f\"{c}_mu_use\"),\n",
    "        pl.when(pl.col(std_c_sym).is_null() | (pl.col(std_c_sym) == 0)).then(pl.col(std_c_glb)).otherwise(pl.col(std_c_sym)).alias(f\"{c}_std_use\")\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(f\"{c}_mu_use\")) / (pl.col(f\"{c}_std_use\") + eps)).alias(c_z)\n",
    "    ).drop([mu_c_glb, std_c_glb, mu_c_sym, std_c_sym, f\"{c}_mu_use\", f\"{c}_std_use\"])\n",
    "    z_cols.append(c_z)\n",
    "    \n",
    "    \n",
    "# 5) 输出列（z_特征 + isna 标记 + 时间/分类/目标/权重）\n",
    "namark_cols = [f\"{c}__isna\" for c in feature_cols]\n",
    "out_cols = [g_sym, g_date, g_time, \"time_idx\", weight_col, target_col] + time_features + z_cols + namark_cols\n",
    "\n",
    "lf_out = lf_z.select(out_cols).sort([g_date, g_time, g_sym])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2d7622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== 4) ==========\n",
    "# 关键：不要“逐天 collect”，而是每次收集一批天，然后一次性按 day 分区写入，显著减少 IO 次数\n",
    "\n",
    "\n",
    "tft_root = P(\"az\", \"tft\"); ensure_dir_az(tft_root)\n",
    "clean_dir = f\"{tft_root}/clean\"; ensure_dir_az(clean_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_DAYS = 30  # 可根据机器内存/速度调整；比如 10~30 天一块\n",
    "day_list = list(map(int, all_days))\n",
    "day_chunks = [day_list[i:i+CHUNK_DAYS] for i in range(0, len(day_list), CHUNK_DAYS)]\n",
    "\n",
    "\n",
    "for ci, chunk in enumerate(day_chunks, 1):\n",
    "    df_chunk = lf_out.filter(pl.col(g_date).is_in(chunk)).collect()\n",
    "    table = df_chunk.to_arrow()\n",
    "\n",
    "    ds.write_dataset(\n",
    "        data=table,\n",
    "        base_dir=clean_dir,\n",
    "        filesystem=fs,\n",
    "        format=\"parquet\",\n",
    "        partitioning=ds.partitioning(pa.schema([(g_date, pa.int32())])),  # or pa.int64()\n",
    "        existing_data_behavior=\"overwrite_or_ignore\",  # 按需改： \"delete_matching\" / \"overwrite_or_ignore\"\n",
    "    )\n",
    "    print(f\"[{_now()}] chunk {ci}/{len(day_chunks)} -> days {chunk[0]}..{chunk[-1]} written\")\n",
    "\n",
    "print(fs.ls(clean_dir)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "241b3b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1] train 1605..1660 (56 days), val 1668..1682 (15 days)\n",
      "[fold 1] template days=[1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614], template shape=(376552, 1571)\n",
      "template (376552, 1571), val (552728, 1571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /mnt/data/js/exp/v1/tft/ckpts/fold_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | MAE                             | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 520    | train\n",
      "3  | prescalers                         | ModuleDict                      | 25.1 K | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 384    | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.9 M  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 0      | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "20 | output_layer                       | Linear                          | 129    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "7.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.919    Total estimated model params size (MB)\n",
      "23624     Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 0/? [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1098, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 211\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# 创建模型\u001b[39;00m\n\u001b[1;32m    200\u001b[0m tft \u001b[38;5;241m=\u001b[39m TemporalFusionTransformer\u001b[38;5;241m.\u001b[39mfrom_dataset(\n\u001b[1;32m    201\u001b[0m     template,\n\u001b[1;32m    202\u001b[0m     loss\u001b[38;5;241m=\u001b[39mMAE(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     reduce_on_plateau_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    209\u001b[0m )\n\u001b[0;32m--> 211\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# 每折训练后：\u001b[39;00m\n\u001b[1;32m    213\u001b[0m es_cb \u001b[38;5;241m=\u001b[39m callbacks[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# EarlyStopping\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:560\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     52\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:598\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    591\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    592\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    594\u001b[0m     ckpt_path,\n\u001b[1;32m    595\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m )\n\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1011\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1055\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1055\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:458\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:152\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:348\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         closure()\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:177\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    180\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1366\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1337\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1366\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/amp.py:76\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimizer_step\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     73\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# skip scaler logic, as bfloat16 does not require scaler\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:81\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 81\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 226\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    229\u001b[0m     params_with_grad: \u001b[38;5;28mlist\u001b[39m[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     98\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m     optimizer: Steppable,\n\u001b[1;32m    100\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    101\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:329\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 329\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    332\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:716\u001b[0m, in \u001b[0;36mBaseModel.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03mTrain on batch.\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    715\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 716\u001b[0m log, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step_outputs\u001b[38;5;241m.\u001b[39mappend(log)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:911\u001b[0m, in \u001b[0;36mBaseModel.step\u001b[0;34m(self, x, y, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/_tft.py:574\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    565\u001b[0m static_context_variable_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand_static_context(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_context_variable_selection(static_embedding), timesteps\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    569\u001b[0m embeddings_varying_encoder \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    570\u001b[0m     name: input_vectors[name][:, :max_encoder_length]\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_variables\n\u001b[1;32m    572\u001b[0m }\n\u001b[1;32m    573\u001b[0m embeddings_varying_encoder, encoder_sparse_weights \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_variable_selection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings_varying_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic_context_variable_selection\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m )\n\u001b[1;32m    580\u001b[0m embeddings_varying_decoder \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    581\u001b[0m     name: input_vectors[name][:, max_encoder_length:]\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_variables  \u001b[38;5;66;03m# select decoder\u001b[39;00m\n\u001b[1;32m    583\u001b[0m }\n\u001b[1;32m    584\u001b[0m embeddings_varying_decoder, decoder_sparse_weights \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_variable_selection(\n\u001b[1;32m    586\u001b[0m         embeddings_varying_decoder,\n\u001b[1;32m    587\u001b[0m         static_context_variable_selection[:, max_encoder_length:],\n\u001b[1;32m    588\u001b[0m     )\n\u001b[1;32m    589\u001b[0m )\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:374\u001b[0m, in \u001b[0;36mVariableSelectionNetwork.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    372\u001b[0m         variable_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprescalers[name](variable_embedding)\n\u001b[1;32m    373\u001b[0m     weight_inputs\u001b[38;5;241m.\u001b[39mappend(variable_embedding)\n\u001b[0;32m--> 374\u001b[0m     var_outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_variable_grns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_embedding\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    375\u001b[0m var_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(var_outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# calculate variable weights\u001b[39;00m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:277\u001b[0m, in \u001b[0;36mGatedResidualNetwork.forward\u001b[0;34m(self, x, context, residual)\u001b[0m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m--> 277\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:206\u001b[0m, in \u001b[0;36mGateAddNorm.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, skip):\n\u001b[1;32m    205\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglu(x)\n\u001b[0;32m--> 206\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:177\u001b[0m, in \u001b[0;36mAddNorm.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_add:\n\u001b[1;32m    175\u001b[0m     skip \u001b[38;5;241m=\u001b[39m skip \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n\u001b[0;32m--> 177\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2905\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2897\u001b[0m         layer_norm,\n\u001b[1;32m   2898\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2903\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2904\u001b[0m     )\n\u001b[0;32m-> 2905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1098, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "# === ===\n",
    "\n",
    "# 设置随机种子\n",
    "L.seed_everything(int(cfg.get(\"seed\", 42)), workers=True)\n",
    "\n",
    "logs_root = P(\"local\", \"tft/logs\"); ensure_dir_local(Path(logs_root).as_posix())\n",
    "ckpts_root = P(\"local\", \"tft/ckpts\"); ensure_dir_local(Path(ckpts_root).as_posix())\n",
    "\n",
    "\n",
    "class ShardedBatchStream(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        template_tsd,\n",
    "        shard_days,\n",
    "        clean_dir: str,\n",
    "        g_sym: str,\n",
    "        batch_size: int = 1024,\n",
    "        num_workers: int = 8,\n",
    "        shuffle_within_shard: bool = True,\n",
    "        buffer_batches: int = 0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.template = template_tsd\n",
    "        self.days = list(map(int, shard_days))\n",
    "        self.clean_dir = clean_dir\n",
    "        self.g_sym = g_sym\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle_within_shard = shuffle_within_shard\n",
    "        self.buffer_batches = buffer_batches\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = random.Random(self.seed)\n",
    "        days = self.days[:]\n",
    "        rng.shuffle(days)\n",
    "\n",
    "        from collections import deque\n",
    "        buf = deque()\n",
    "\n",
    "        for d in days:\n",
    "            paths = fs.glob(f\"{self.clean_dir}/{d}/*.parquet\")\n",
    "            if not paths:\n",
    "                raise RuntimeError(f\"no data files found for day {d} in {self.clean_dir}\")\n",
    "            paths = [f\"az://{p}\" for p in paths]\n",
    "            pdf = pl.scan_parquet(paths, storage_options=storage_options).collect().to_pandas()\n",
    "            if pdf.empty:\n",
    "                raise RuntimeError(f\"empty data for day {d} in {self.clean_dir}\")\n",
    "            pdf[self.g_sym] = pdf[self.g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "            tsds = TimeSeriesDataSet.from_dataset(\n",
    "                self.template,\n",
    "                data=pdf.sort_values([self.g_sym, \"time_idx\"]),\n",
    "                stop_randomization=False,\n",
    "            )\n",
    "\n",
    "            dl = tsds.to_dataloader(\n",
    "                train=True,\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "                shuffle=self.shuffle_within_shard,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=False,\n",
    "            )\n",
    "\n",
    "            for batch in dl:\n",
    "                if self.buffer_batches > 0:\n",
    "                    buf.append(batch)\n",
    "                    if len(buf) >= self.buffer_batches:\n",
    "                        k = rng.randrange(len(buf))\n",
    "                        if k:\n",
    "                            buf.rotate(-k)\n",
    "                        yield buf.popleft()\n",
    "                else:\n",
    "                    yield batch\n",
    "\n",
    "        while buf:\n",
    "            yield buf.popleft()\n",
    "\n",
    "\n",
    "best_ckpt_paths = []\n",
    "fold_metrics = []\n",
    "\n",
    "# ---- 训练集 & 验证集 folds----\n",
    "for fold_id, (train, val) in enumerate(folds_by_day, start=1):\n",
    "    print(f\"[fold {fold_id}] train {train[0]}..{train[-1]} ({len(train)} days), \"\n",
    "          f\"val {val[0]}..{val[-1]} ({len(val)} days)\")\n",
    "    \n",
    "    days_sorted = np.sort(train)\n",
    "    \n",
    "    # ---- Template（用第一天分片建立，固化 encoders/scalers）----\n",
    "    TEMPLATE_DAYS = min(10, len(days_sorted))   # 你可按需调大/调小，比如 5/7/全部\n",
    "\n",
    "    tmpl_paths = []\n",
    "    for d in days_sorted[:TEMPLATE_DAYS]:\n",
    "        tmpl_paths.extend(fs.glob(f\"{clean_dir}/{d}/*.parquet\"))\n",
    "    tmpl_paths = [f\"az://{p}\" for p in tmpl_paths]\n",
    "    \n",
    "    pdf_tmpl = pl.scan_parquet(tmpl_paths, storage_options=storage_options).collect().to_pandas()\n",
    "    pdf_tmpl[g_sym] = pdf_tmpl[g_sym].astype(\"str\").astype(\"category\")\n",
    "    \n",
    "    print(f\"[fold {fold_id}] template days={list(map(int, days_sorted[:TEMPLATE_DAYS]))}, \"\n",
    "    f\"template shape={pdf_tmpl.shape}\")\n",
    "    \n",
    "    # 验证集\n",
    "    val_paths = []\n",
    "    for d in val:\n",
    "        val_paths.extend(fs.glob(f\"{clean_dir}/{d}/*.parquet\"))\n",
    "    val_paths = [f\"az://{p}\" for p in val_paths]\n",
    "    pdf_val = pl.scan_parquet(val_paths, storage_options=storage_options).collect().to_pandas()\n",
    "    pdf_val[g_sym] = pdf_val[g_sym].astype(\"str\").astype(\"category\")\n",
    "    \n",
    "    print(f\"template {pdf_tmpl.shape}, val {pdf_val.shape}\")\n",
    "    \n",
    "    unknown_reals = time_features + z_cols + namark_cols\n",
    "    \n",
    "    identity_scalers = {name: None for name in unknown_reals} # 我们的自变量连续特征只有unknown_reals      \n",
    "    template = TimeSeriesDataSet(\n",
    "        pdf_tmpl.sort_values([g_sym, \"time_idx\"]),\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_col,\n",
    "        group_ids=[g_sym],\n",
    "        weight=weight_col,\n",
    "        max_encoder_length=ENC_LEN,\n",
    "        max_prediction_length=PRED_LEN,\n",
    "        \n",
    "        static_categoricals=[g_sym],\n",
    "        time_varying_unknown_reals=unknown_reals,\n",
    "        \n",
    "        lags=None,  # 不用自动滞后\n",
    "        \n",
    "        categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "        add_relative_time_idx=False, \n",
    "        add_target_scales=False, \n",
    "        add_encoder_length=False,\n",
    "        \n",
    "        allow_missing_timesteps=True,\n",
    "        \n",
    "        target_normalizer=None,\n",
    "        scalers=identity_scalers,\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        template, data=pdf_val.sort_values([g_sym, \"time_idx\"]), stop_randomization=True\n",
    "    )\n",
    "    val_loader = validation.to_dataloader(\n",
    "        train=False, batch_size=BATCH_SIZE, num_workers=min(8, max(1, os.cpu_count() - 2))\n",
    "    )\n",
    "\n",
    "    len(val_loader), pdf_tmpl.shape, pdf_val.shape\n",
    "    \n",
    "    train_stream = ShardedBatchStream(\n",
    "        template_tsd=template,\n",
    "        shard_days=days_sorted,\n",
    "        clean_dir=clean_dir,\n",
    "        g_sym=g_sym,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=min(8, max(1, os.cpu_count() - 2)),\n",
    "        shuffle_within_shard=True,\n",
    "        buffer_batches=16,   # 0 代表关闭跨分片缓冲打乱；8~64 可微调\n",
    "        seed=42,\n",
    "    )\n",
    "    # 外层 DataLoader 不再做 batch/多进程\n",
    "    train_loader = DataLoader(train_stream, batch_size=None, num_workers=0)\n",
    "    \n",
    "    # 在 callbacks 定义前，先为本折建独立目录\n",
    "    ckpt_dir_fold = Path(ckpts_root) / f\"fold_{fold_id}\"\n",
    "    ensure_dir_local(ckpt_dir_fold.as_posix())\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_RMSE\", mode=\"min\", patience=5),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_RMSE\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            dirpath=ckpt_dir_fold.as_posix(),            # 每折独立目录\n",
    "            filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_RMSE:.5f}}\",  # 文件名含 fold\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ]\n",
    "\n",
    "    # （可选）logger 名字也带上 fold，便于区分\n",
    "    logger = TensorBoardLogger(save_dir=logs_root, name=f\"tft_f{fold_id}\", default_hp_metric=False)\n",
    "    \n",
    "    VAL_EVERY_STEPS = 50\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"gpu\", devices=1, precision=\"bf16-mixed\",\n",
    "        max_epochs=5,\n",
    "        val_check_interval=VAL_EVERY_STEPS,\n",
    "        num_sanity_val_steps=0,\n",
    "        gradient_clip_val=0.5,\n",
    "        log_every_n_steps=50,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        default_root_dir=ckpts_root,\n",
    "    )\n",
    "\n",
    "    # 创建模型\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        template,\n",
    "        loss=MAE(),\n",
    "        logging_metrics=[RMSE()],\n",
    "        learning_rate=float(cfg.get(\"tft\", {}).get(\"lr\", 1e-3)),\n",
    "        hidden_size=int(cfg.get(\"tft\", {}).get(\"hidden_size\", 128)),\n",
    "        attention_head_size=int(cfg.get(\"tft\", {}).get(\"heads\", 4)),\n",
    "        dropout=float(cfg.get(\"tft\", {}).get(\"dropout\", 0.2)),\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    # 每折训练后：\n",
    "    es_cb = callbacks[0]  # EarlyStopping\n",
    "    ckpt_cb = callbacks[1]\n",
    "\n",
    "    print(\"epoch_end_at   :\", trainer.current_epoch)              # 停下时的 epoch 索引（0 基）\n",
    "    print(\"global_step    :\", trainer.global_step)                # 训练过的 step 数\n",
    "    print(\"val_best_score :\", float(ckpt_cb.best_model_score))    # 最优 val_RMSE\n",
    "    print(\"es_stopped_ep  :\", getattr(es_cb, \"stopped_epoch\", None))  # 触发早停的 epoch\n",
    "    print(\"es_wait_count  :\", getattr(es_cb, \"wait_count\", None))     # 连续未提升的验证次数\n",
    "\n",
    "    best_ckpt_paths.append(ckpt_cb.best_model_path)\n",
    "    fold_metrics.append(float(ckpt_cb.best_model_score))  # 这是监控的 val_RMSE\n",
    "\n",
    "    # CV 聚合（简单平均或按验证样本数加权平均）\n",
    "    cv_rmse = np.mean(fold_metrics)  # 或按样本数加权\n",
    "    print(f\"[CV] mean val_RMSE = {cv_rmse:.6f}\")\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
