{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34503ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9a5d780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 导入文件 /mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\\ninput_file = \"/mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\"\\n\\nwith open(input_file, \\'r\\') as file:\\n    data = file.read()\\nlines = data.split(\\'\\n\\')\\nfeatures = [line.split()[0] for line in lines if line.strip() and not line.startswith(\\'#\\')]\\n\\nprint(f\"[{_now()}] loaded {len(features)} features from {input_file}\")  \\n\\nimport re\\n\\n# ---- 关键配置（按需改）----\\ntarget_col = cfg[\"target\"]                 # e.g. \"responder_6\"\\ng_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\\nweight_col = cfg[\"weight\"]\\n\\ntime_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\\n\\ntopK_features = features[:10]\\nprint(f\"[{_now()}] top 10 features: {topK_features}\") \\n\\n\\n\\n# 严格匹配基础特征：feature_后跟两位数字、无后缀\\nbase_features = [f for f in features if re.fullmatch(r\"feature_\\\\d{2}\", f)]\\n# 响应类历史特征：以 responder_ 开头\\nresp_his_feats = [f for f in features if f.startswith(\"responder_\")]\\n# 其他派生的 feature_*（有后缀），但排除基础特征\\nfeat_his_feats = [f for f in features if f.startswith(\"feature_\") and f not in base_features]\\n\\n# 只保留 topK 特征\\nbase_features = [f for f in base_features if f in topK_features]\\nresp_his_feats = [f for f in resp_his_feats if f in topK_features]\\nfeat_his_feats = [f for f in feat_his_feats if f in topK_features]\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# 导入文件 /mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\n",
    "input_file = \"/mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\"\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    data = file.read()\n",
    "lines = data.split('\\n')\n",
    "features = [line.split()[0] for line in lines if line.strip() and not line.startswith('#')]\n",
    "\n",
    "print(f\"[{_now()}] loaded {len(features)} features from {input_file}\")  \n",
    "\n",
    "import re\n",
    "\n",
    "# ---- 关键配置（按需改）----\n",
    "target_col = cfg[\"target\"]                 # e.g. \"responder_6\"\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "weight_col = cfg[\"weight\"]\n",
    "\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "\n",
    "topK_features = features[:10]\n",
    "print(f\"[{_now()}] top 10 features: {topK_features}\") \n",
    "\n",
    "\n",
    "\n",
    "# 严格匹配基础特征：feature_后跟两位数字、无后缀\n",
    "base_features = [f for f in features if re.fullmatch(r\"feature_\\d{2}\", f)]\n",
    "# 响应类历史特征：以 responder_ 开头\n",
    "resp_his_feats = [f for f in features if f.startswith(\"responder_\")]\n",
    "# 其他派生的 feature_*（有后缀），但排除基础特征\n",
    "feat_his_feats = [f for f in features if f.startswith(\"feature_\") and f not in base_features]\n",
    "\n",
    "# 只保留 topK 特征\n",
    "base_features = [f for f in base_features if f in topK_features]\n",
    "resp_his_feats = [f for f in resp_his_feats if f in topK_features]\n",
    "feat_his_feats = [f for f in feat_his_feats if f in topK_features]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4611fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-04 14:20:46] imports ok\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库（stdlib） ─────────────────────────────────────────────────────────────\n",
    "import os\n",
    "\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "# ── 第三方（third-party） ───────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import torch\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, IterableDataset, get_worker_info\n",
    "\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import (\n",
    "    TimeSeriesDataSet,\n",
    "    TemporalFusionTransformer,\n",
    ")\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import (\n",
    "    NaNLabelEncoder,\n",
    ")\n",
    "\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()  # 字符串编码缓存，提速 join/分类\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "\n",
    "# 全局一次性设置（不要放到训练/epoch/折叠循环里）\n",
    "import torch.backends.cudnn as cudnn \n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\") \n",
    "\n",
    "\n",
    "def _now(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38ab8741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ready\n"
     ]
    }
   ],
   "source": [
    "# ---- 关键配置（按需改）----\n",
    "target_col = cfg[\"target\"]                 # e.g. \"responder_6\"\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "weight_col = cfg[\"weight\"]\n",
    "\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "\n",
    "feature_cols = list([\"feature_36\"])\n",
    "need_cols = list(dict.fromkeys([g_sym, g_date, g_time, weight_col, target_col] + time_features + feature_cols))\n",
    "\n",
    "# CV & 训练\n",
    "N_SPLITS   = 2\n",
    "GAP_DAYS   = 7\n",
    "TRAIN_TO_VAL = 4               # 训练:验证 = 4:1\n",
    "ENC_LEN    = 10\n",
    "PRED_LEN   = 1\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "HIDDEN     = 64\n",
    "HEADS      = 2\n",
    "DROPOUT    = 0.1\n",
    "MAX_EPOCHS_PER_SHARD = 1\n",
    "CHUNK_DAYS = 20               # 训练分片：每片多少天\n",
    "\n",
    "print(\"config ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d88ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-04 14:20:46] lazyframe ready\n",
      "stats_hi (for global z-score) = 1660; first-fold train days end at this day.\n"
     ]
    }
   ],
   "source": [
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "data_path  = fs.glob(f\"{panel_dir}/*.parquet\")\n",
    "az_path = [f\"az://{p}\" for p in data_path]\n",
    "lf_data = pl.scan_parquet(az_path, storage_options=storage_options)\n",
    "\n",
    "\n",
    "# 全局 time_idx 仅一次\n",
    "lf_grid = (\n",
    "    lf_data.select([g_date, g_time]).unique()\n",
    "        .sort([g_date, g_time])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\"); ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "\n",
    "lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")\n",
    "\n",
    "\n",
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# 接入 time_idx + 只保留所需列（仍是 Lazy）\n",
    "lf0 = (\n",
    "    lf_data.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "        .select(need_cols + [\"time_idx\"])\n",
    "        .sort([g_date, g_time, g_sym])\n",
    ")\n",
    "\n",
    "print(f\"[{_now()}] lazyframe ready\")\n",
    "\n",
    "\n",
    "# 全量天列表\n",
    "all_days = (\n",
    "    lf0.select(pl.col(g_date)).unique().sort(by=g_date)\n",
    "       .collect(streaming=True).get_column(g_date).to_numpy()\n",
    ")\n",
    "\n",
    "\n",
    "# 构造滑动时间窗 CV\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0: return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0: return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days): break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "\n",
    "\n",
    "# 选“标准化统计区间”上界：第1折训练天的最大值（不含验证）\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"stats_hi (for global z-score) = {stats_hi}; first-fold train days end at this day.\")\n",
    "\n",
    "\n",
    "# ========== 1) 连续特征：一次性处理 inf->null、打缺失标记、组内 ffill、兜底 0 ==========\n",
    "# 先做 flag（要基于原始缺失），再做填充；合并成两次 with_columns，避免在 for 循环里多次改列\n",
    "inf2null_exprs = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
    "                for c in feature_cols] # inf -> null 不产生新列\n",
    "flags_exprs    = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\")\n",
    "                for c in feature_cols] # 产生新列\n",
    "fill_exprs     = [pl.col(c).forward_fill().over(g_sym).fill_null(0.0).alias(c)\n",
    "                for c in feature_cols] # 填充，覆盖原列\n",
    "\n",
    "lf_clean = (\n",
    "    lf0.with_columns(inf2null_exprs)         # inf -> null\n",
    "    .with_columns(flags_exprs)            # 缺失标记（基于原始缺失）\n",
    "    .with_columns(fill_exprs)             # 组内 ffill + 兜底 0\n",
    ")\n",
    "\n",
    "# 为数据标准化做准备\n",
    "\n",
    "# ========== 2) 训练区间（<= stats_hi）按组计算 mu/std（一次） ==========\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .group_by(g_sym)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in feature_cols] +\n",
    "        [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 训练期全局统计（作为回退）\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in feature_cols] +\n",
    "            [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 3) 把全局统计作为常量列加到每行（cross join 方式）\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\")\n",
    "\n",
    "# 4) join per-symbol 统计，并对每个特征做回退 & z-score\n",
    "\n",
    "lf_z = lf_z.join(lf_stats_sym, on=g_sym, how=\"left\")\n",
    "\n",
    "# 对每个特征做回退 & z-score\n",
    "eps = 1e-6\n",
    "z_cols = []\n",
    "for c in feature_cols:\n",
    "    mu_c_sym, std_c_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_c_glb, std_c_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    c_z = f\"{c}_z\"\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_c_sym).is_null()).then(pl.col(mu_c_glb)).otherwise(pl.col(mu_c_sym)).alias(f\"{c}_mu_use\"),\n",
    "        pl.when(pl.col(std_c_sym).is_null() | (pl.col(std_c_sym) == 0)).then(pl.col(std_c_glb)).otherwise(pl.col(std_c_sym)).alias(f\"{c}_std_use\")\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(f\"{c}_mu_use\")) / (pl.col(f\"{c}_std_use\") + eps)).alias(c_z)\n",
    "    ).drop([mu_c_glb, std_c_glb, mu_c_sym, std_c_sym, f\"{c}_mu_use\", f\"{c}_std_use\"])\n",
    "    z_cols.append(c_z)\n",
    "    \n",
    "    \n",
    "# 5) 输出列（z_特征 + isna 标记 + 时间/分类/目标/权重）\n",
    "namark_cols = [f\"{c}__isna\" for c in feature_cols]\n",
    "out_cols = [g_sym, g_date, g_time, \"time_idx\", weight_col, target_col] + time_features + z_cols + namark_cols\n",
    "\n",
    "lf_out = lf_z.select(out_cols).sort([g_date, g_time, g_sym])\n",
    "\n",
    "\n",
    "# \n",
    "# 关键：不要“逐天 collect”，而是每次收集一批天，然后一次性按 day 分区写入，显著减少 IO 次数\n",
    "\n",
    "\n",
    "tft_root = P(\"az\", \"tft\"); ensure_dir_az(tft_root)\n",
    "clean_dir = f\"{tft_root}/clean\"; ensure_dir_az(clean_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_DAYS = 30\n",
    "day_list   = list(map(int, all_days))\n",
    "day_chunks = [day_list[i:i+CHUNK_DAYS] for i in range(0, len(day_list), CHUNK_DAYS)]\n",
    "\n",
    "for ci, chunk in enumerate(day_chunks, 1):\n",
    "    df_chunk = lf_out.filter(pl.col(g_date).is_in(chunk)).collect()\n",
    "    table = df_chunk.to_arrow()\n",
    "\n",
    "    chunk_dir = f\"{clean_dir}/chunk_{chunk[0]:04d}_{chunk[-1]:04d}\"  # e.g. chunk_20240101_20240130\n",
    "    ds.write_dataset(\n",
    "        data=table,\n",
    "        base_dir=chunk_dir,\n",
    "        filesystem=fs,\n",
    "        format=\"parquet\",\n",
    "        partitioning=None,                         # ← 不再按天分区\n",
    "        existing_data_behavior=\"overwrite_or_ignore\",\n",
    "        basename_template=\"data-{i}.parquet\",      # 少量大文件\n",
    "        max_rows_per_file=50_000_000,              # 按行数切文件，防止超大\n",
    "    )\n",
    "    print(f\"[{_now()}] chunk {ci}/{len(day_chunks)} -> days {chunk[0]}..{chunk[-1]} written\")\n",
    "\n",
    "print(fs.ls(clean_dir)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52bd15b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prep] 4 chunks; 4 files total\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# ========= 预分组：按 chunk 归并所有 parquet =========\n",
    "# 1) 列出 chunk 目录（加 az:// 前缀并排序）\n",
    "entries = fs.ls(clean_dir)\n",
    "chunk_dirs = []\n",
    "for e in entries:\n",
    "    if isinstance(e, str):\n",
    "        path = e\n",
    "    else:\n",
    "        path = e.get(\"name\") or e.get(\"path\") or e.get(\"Key\") or str(e)\n",
    "    if path.rstrip(\"/\").split(\"/\")[-1].startswith(\"chunk_\"):\n",
    "        chunk_dirs.append(path if path.startswith(\"az://\") else f\"az://{path}\")\n",
    "chunk_dirs = sorted(chunk_dirs)\n",
    "\n",
    "\n",
    "# 2) 构建 {chunk_dir: [该目录下所有 parquet 文件]}\n",
    "chunk2paths = defaultdict(list)\n",
    "for cdir in chunk_dirs:\n",
    "    paths = fs.glob(f\"{cdir}/*.parquet\")\n",
    "    paths = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in paths]\n",
    "    if not paths:\n",
    "        print(f\"[WARN] empty chunk: {cdir}\")\n",
    "    chunk2paths[cdir] = paths\n",
    "\n",
    "\n",
    "# 3) 展平为 all_paths（模板/验证时 lazy 扫描用）\n",
    "all_paths = [p for plist in chunk2paths.values() for p in plist]\n",
    "print(f\"[prep] {len(chunk_dirs)} chunks; {len(all_paths)} files total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0e03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6e8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1] train 1605..1660 (56 days), val 1668..1682 (15 days)\n",
      "[fold 1] template days=[1605, 1606, 1607, 1608, 1609], template shape=(188760, 10)\n",
      "template (188760, 10), val (552728, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loader] chunk 1/1: az://jackson/js_exp/exp/v1/tft/clean/chunk_1695_1698 -> rows=150,040\n",
      "[loader] chunk 1/1: az://jackson/js_exp/exp/v1/tft/clean/chunk_1605_1634 -> rows=1,123,848\n",
      "[loader] chunk 1/1: az://jackson/js_exp/exp/v1/tft/clean/chunk_1635_1664 -> rows=1,107,392\n",
      "[loader] chunk 1/1: az://jackson/js_exp/exp/v1/tft/clean/chunk_1665_1694 -> rows=1,117,072\n",
      "Epoch 0: |          | 6827/? [06:38<00:00, 17.12it/s, v_num=3, train_loss_step=0.953, val_loss=0.996, train_loss_epoch=1.190]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 6827/? [06:42<00:00, 16.96it/s, v_num=3, train_loss_step=0.953, val_loss=0.996, train_loss_epoch=1.190]\n",
      "epoch_end_at   : 1\n",
      "global_step    : 6827\n",
      "val_best_score : 0.9767875075340271\n",
      "es_stopped_ep  : 0\n",
      "es_wait_count  : 0\n",
      "[CV] mean val_RMSE = 0.976788\n",
      "[fold 2] train 1620..1675 (56 days), val 1683..1697 (15 days)\n",
      "[fold 2] template days=[1620, 1621, 1622, 1623, 1624], template shape=(185856, 10)\n",
      "template (185856, 10), val (565312, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loader] chunk 1/1: az://jackson/js_exp/exp/v1/tft/clean/chunk_1695_1698 -> rows=150,040\n",
      "[loader] chunk 1/1: az://jackson/js_exp/exp/v1/tft/clean/chunk_1605_1634 -> rows=1,123,848\n",
      "[loader] chunk 1/1: az://jackson/js_exp/exp/v1/tft/clean/chunk_1665_1694 -> rows=1,117,072\n",
      "[loader] chunk 1/1: az://jackson/js_exp/exp/v1/tft/clean/chunk_1635_1664 -> rows=1,107,392\n",
      "Epoch 0: |          | 5731/? [06:40<00:00, 14.30it/s, v_num=0, train_loss_step=0.451, val_loss=0.984, train_loss_epoch=1.110]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 5731/? [06:44<00:00, 14.18it/s, v_num=0, train_loss_step=0.451, val_loss=0.984, train_loss_epoch=1.110]\n",
      "epoch_end_at   : 1\n",
      "global_step    : 5731\n",
      "val_best_score : 0.9435896873474121\n",
      "es_stopped_ep  : 0\n",
      "es_wait_count  : 0\n",
      "[CV] mean val_RMSE = 0.960189\n",
      "[done] best_ckpts = ['/mnt/data/js/exp/v1/tft/ckpts/fold_1/fold1-tft-best-epoch=00-val_RMSE=0.97679.ckpt', '/mnt/data/js/exp/v1/tft/ckpts/fold_2/fold2-tft-best-epoch=00-val_RMSE=0.94359.ckpt']\n"
     ]
    }
   ],
   "source": [
    "unknown_reals = z_cols + namark_cols + time_features  # 为了快速验证，暂用这个\n",
    "# [FIX] 如果 weight_col 为 None，则不加入；否则加入\n",
    "cols = [g_sym, \"time_idx\", target_col, *unknown_reals] if not weight_col else [g_sym, \"time_idx\", weight_col, target_col, *unknown_reals]\n",
    "\n",
    "\n",
    "\n",
    "from pipeline.stream_input import ShardedBatchStream\n",
    "\n",
    "# ========= 训练 =========\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "tft_root = P(\"local\", \"tft\"); ensure_dir_local(tft_root)\n",
    "ckpts_root = Path(tft_root) / \"ckpts\"; ensure_dir_local(ckpts_root.as_posix())\n",
    "logs_root  = Path(tft_root) / \"logs\";  ensure_dir_local(logs_root.as_posix())\n",
    "\n",
    "\n",
    "\n",
    "for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "    print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "        f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "    # ---- Template（用前 N 天构建编码器/缩放器）----\n",
    "    days_sorted = np.sort(train_days)\n",
    "    TEMPLATE_DAYS = min(5, len(days_sorted))\n",
    "    tmpl_days = list(map(int, days_sorted[:TEMPLATE_DAYS]))\n",
    "\n",
    "    pdf_tmpl = (\n",
    "        pl.scan_parquet(all_paths, storage_options=storage_options)\n",
    "        .filter(pl.col(g_date).is_in(tmpl_days))\n",
    "        .select(cols)\n",
    "        .collect(streaming=True)\n",
    "        .to_pandas()\n",
    "    )\n",
    "    pdf_tmpl[g_sym] = pdf_tmpl[g_sym].astype(\"str\")\n",
    "    pdf_tmpl.sort_values([g_sym, \"time_idx\"], inplace=True)\n",
    "    print(f\"[fold {fold_id}] template days={tmpl_days}, template shape={pdf_tmpl.shape}\")\n",
    "\n",
    "    # ---- 验证集 ----\n",
    "    pdf_val = (\n",
    "        pl.scan_parquet(all_paths, storage_options=storage_options)\n",
    "        .filter(pl.col(g_date).is_in(list(map(int, val_days))))\n",
    "        .select(cols)\n",
    "        .collect(streaming=True)\n",
    "        .to_pandas()\n",
    "    )\n",
    "    pdf_val[g_sym] = pdf_val[g_sym].astype(\"str\")\n",
    "    pdf_val.sort_values([g_sym, \"time_idx\"], inplace=True)\n",
    "    print(f\"template {pdf_tmpl.shape}, val {pdf_val.shape}\")\n",
    "\n",
    "    # ---- TimeSeries template ----\n",
    "    identity_scalers = {name: None for name in unknown_reals}  # 连续特征直接透传（你已做 z/标准化）\n",
    "    template = TimeSeriesDataSet(\n",
    "        pdf_tmpl,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_col,\n",
    "        group_ids=[g_sym],\n",
    "        weight=weight_col,                        # None 时 TSD 会忽略\n",
    "        max_encoder_length=ENC_LEN,\n",
    "        max_prediction_length=PRED_LEN,\n",
    "        static_categoricals=[g_sym],\n",
    "        time_varying_unknown_reals=unknown_reals,\n",
    "        lags=None,\n",
    "        categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "        add_relative_time_idx=False,\n",
    "        add_target_scales=False,\n",
    "        add_encoder_length=False,\n",
    "        allow_missing_timesteps=True,\n",
    "        target_normalizer=None,\n",
    "        scalers=identity_scalers,\n",
    "    )\n",
    "\n",
    "    # ---- 验证 Loader ----\n",
    "    validation = TimeSeriesDataSet.from_dataset(template, data=pdf_val, stop_randomization=True)\n",
    "    val_loader = validation.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=BATCH_SIZE * 2,\n",
    "        num_workers=min(8, max(1, os.cpu_count() - 2)),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "    )\n",
    "\n",
    "    # ---- 训练流（按 chunk 一次读取）----\n",
    "    train_stream = ShardedBatchStream(\n",
    "        template_tsd=template,\n",
    "        chunk_dirs=chunk_dirs,\n",
    "        chunk2paths=chunk2paths,\n",
    "        g_sym=g_sym,\n",
    "        batch_size=512,\n",
    "        buffer_batches=0,\n",
    "        seed=42,\n",
    "        cols=cols,\n",
    "        print_every_chunks=1,   # 每处理一个 chunk 打印一次\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_stream,\n",
    "        batch_size=None,\n",
    "        num_workers=min(8, max(1, os.cpu_count() - 2)),\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "        multiprocessing_context=\"spawn\",   # 保险：远端 FS + 多进程\n",
    "    )\n",
    "\n",
    "    # ---- callbacks/logger/trainer ----\n",
    "    ckpt_dir_fold = Path(ckpts_root) / f\"fold_{fold_id}\"\n",
    "    ensure_dir_local(ckpt_dir_fold.as_posix())\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_RMSE\", mode=\"min\", patience=2),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_RMSE\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            dirpath=ckpt_dir_fold.as_posix(),\n",
    "            filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_RMSE:.5f}}\",\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ]\n",
    "    logger = TensorBoardLogger(save_dir=logs_root, name=f\"tft_f{fold_id}\", default_hp_metric=False)\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"gpu\", devices=1, precision=\"bf16-mixed\",\n",
    "        max_epochs=1,\n",
    "        num_sanity_val_steps=0,\n",
    "        gradient_clip_val=0.5,\n",
    "        log_every_n_steps=50,          # 更频繁\n",
    "        enable_progress_bar=True,      # 显示进度条\n",
    "        enable_model_summary=False,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        default_root_dir=ckpts_root,\n",
    "        # --- 如需“烟测/快速定位”，可临时打开 ---\n",
    "        # limit_train_batches=200,\n",
    "        # val_check_interval=400,\n",
    "    )\n",
    "\n",
    "    # ---- 模型并训练 ----\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        template,\n",
    "        loss=MAE(),\n",
    "        logging_metrics=[RMSE()],\n",
    "        learning_rate=float(cfg.get(\"tft\", {}).get(\"lr\", 1e-3)),\n",
    "        hidden_size=int(cfg.get(\"tft\", {}).get(\"hidden_size\", 64)),\n",
    "        attention_head_size=int(cfg.get(\"tft\", {}).get(\"heads\", 2)),\n",
    "        dropout=float(cfg.get(\"tft\", {}).get(\"dropout\", 0.1)),\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    # ---- 记录结果 ----\n",
    "    es_cb, ckpt_cb = callbacks[0], callbacks[1]\n",
    "    print(\"epoch_end_at   :\", trainer.current_epoch)\n",
    "    print(\"global_step    :\", trainer.global_step)\n",
    "    print(\"val_best_score :\", float(ckpt_cb.best_model_score))\n",
    "    print(\"es_stopped_ep  :\", getattr(es_cb, \"stopped_epoch\", None))\n",
    "    print(\"es_wait_count  :\", getattr(es_cb, \"wait_count\", None))\n",
    "\n",
    "    best_ckpt_paths.append(ckpt_cb.best_model_path)\n",
    "    fold_metrics.append(float(ckpt_cb.best_model_score))\n",
    "    cv_rmse = np.mean(fold_metrics)\n",
    "    print(f\"[CV] mean val_RMSE = {cv_rmse:.6f}\")\n",
    "\n",
    "print(\"[done] best_ckpts =\", best_ckpt_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
