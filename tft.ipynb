{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c087134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 20:06:27][tft] ===== start =====\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pytorch_lightning' has no attribute 'scan_parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mglob(glob_pat\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maz://\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo parquet shards under: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglob_pat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m lf \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_parquet\u001b[49m(glob_pat, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m    115\u001b[0m grid_path \u001b[38;5;241m=\u001b[39m P(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtft/panel/grid_timeidx.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(grid_path)\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pytorch_lightning' has no attribute 'scan_parquet'"
     ]
    }
   ],
   "source": [
    "# ====== 强化版主流程（可直接替换） ======\n",
    "from __future__ import annotations\n",
    "import os, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import pytorch_lightning as ptl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import SMAPE\n",
    "\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "\n",
    "def _now(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# -------- 工具函数 --------\n",
    "def add_missing_flags_and_fill(df: pd.DataFrame, group_col: str, cont_cols: list[str]) -> tuple[list[str], pd.DataFrame]:\n",
    "    \"\"\"对连续特征：加 __isna 标记、组内 ffill、0 填充；返回新增标记列名列表。\"\"\"\n",
    "    df[cont_cols] = df[cont_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    flags = []\n",
    "    for c in cont_cols:\n",
    "        flag = f\"{c}__isna\"\n",
    "        flags.append(flag)\n",
    "        df[flag] = df[c].isna().astype(\"int8\")\n",
    "        df[c] = df.groupby(group_col, observed=False)[c].ffill()\n",
    "        df[c] = df[c].fillna(0.0)\n",
    "    return flags, df\n",
    "\n",
    "def standardize_by_symbol(train_df: pd.DataFrame,\n",
    "                          val_df: pd.DataFrame,\n",
    "                          group_col: str,\n",
    "                          cont_cols: list[str],\n",
    "                          eps: float = 1e-6):\n",
    "    \"\"\"按 symbol 标准化连续特征；val 用 train 的统计量，新 symbol 回退到全局。\"\"\"\n",
    "    mu_df  = train_df.groupby(group_col, observed=False)[cont_cols].mean()\n",
    "    std_df = train_df.groupby(group_col, observed=False)[cont_cols].std(ddof=1).clip(lower=eps)\n",
    "\n",
    "    # train 映射\n",
    "    train_mu  = mu_df.loc[train_df[group_col]].to_numpy()\n",
    "    train_std = std_df.loc[train_df[group_col]].to_numpy()\n",
    "    X_train   = train_df[cont_cols].to_numpy(dtype=np.float32)\n",
    "    train_df[cont_cols] = ((X_train - train_mu) / train_std).astype(np.float32)\n",
    "\n",
    "    # val 映射（未知 symbol 回退全局统计）\n",
    "    val_syms = val_df[group_col]\n",
    "    if (~val_syms.isin(mu_df.index)).any():\n",
    "        g_mu  = train_df[cont_cols].mean()\n",
    "        g_std = train_df[cont_cols].std(ddof=1).clip(lower=eps)\n",
    "        val_mu  = mu_df.reindex(val_syms).fillna(g_mu).to_numpy()\n",
    "        val_std = std_df.reindex(val_syms).fillna(g_std).to_numpy()\n",
    "    else:\n",
    "        val_mu  = mu_df.loc[val_syms].to_numpy()\n",
    "        val_std = std_df.loc[val_syms].to_numpy()\n",
    "\n",
    "    X_val = val_df[cont_cols].to_numpy(dtype=np.float32)\n",
    "    val_df[cont_cols] = ((X_val - val_mu) / val_std).astype(np.float32)\n",
    "    return train_df, val_df, mu_df, std_df\n",
    "\n",
    "def build_trainer():\n",
    "    # AMP 精度选择\n",
    "    precision = \"bf16-mixed\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 32\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=P(\"local\", \"tft/logs\"),\n",
    "        name=\"tft\",\n",
    "        default_hp_metric=False,\n",
    "    )\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1,\n",
    "                        filename=\"tft-best-{epoch:02d}-{val_loss:.5f}\"),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ]\n",
    "\n",
    "    trainer = ptl.Trainer(\n",
    "        max_epochs=int(cfg.get(\"tft\", {}).get(\"max_epochs\", 30)),\n",
    "        accelerator=\"auto\",\n",
    "        precision=precision,\n",
    "        gradient_clip_val=float(cfg.get(\"tft\", {}).get(\"grad_clip\", 1.0)),\n",
    "        log_every_n_steps=50,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        default_root_dir=P(\"local\", \"tft/ckpts\"),\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "print(f\"[{_now()}][tft] ===== start =====\")\n",
    "target_col = cfg[\"target\"]                 # e.g. responder_6\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (symbol_id, date_id, time_id)\n",
    "TIME_SORT = cfg[\"sorts\"].get(\"time_major\", [g_date, g_time, g_sym])\n",
    "\n",
    "# 1) 选择特征列（示例；把你要用的列填进来）\n",
    "base_features   = [\"feature_01\"]                 # TODO: 换成你的原始 79 列子集\n",
    "resp_his_feats  = [\"responder_6_prevday_close\"]  # 示例\n",
    "feat_his_feats  = [\"feature_00__lag1\"]           # 示例\n",
    "feature_cols = list(dict.fromkeys(base_features + resp_his_feats + feat_his_feats))\n",
    "\n",
    "# TimeSeriesDataSet 的 unknown_reals = 模型在解码时“未知”的连续变量（不含 target）\n",
    "need_cols = list(dict.fromkeys(cfg[\"keys\"] + [target_col] + feature_cols))\n",
    "\n",
    "# 2) 读 panel（Lazy） & 构 grid\n",
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "glob_pat  = f\"{panel_dir}/*.parquet\"\n",
    "if not fs.glob(glob_pat.replace(\"az://\", \"\")):\n",
    "    raise FileNotFoundError(f\"No parquet shards under: {glob_pat}\")\n",
    "lf = pl.scan_parquet(glob_pat, storage_options=storage_options)\n",
    "\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\")\n",
    "if not Path(grid_path).exists():\n",
    "    lf_grid = (\n",
    "        lf.select([g_date, g_time]).unique()\n",
    "            .sort([g_date, g_time])\n",
    "            .with_row_index(\"time_idx\")\n",
    "            .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "    )\n",
    "    ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "    lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")\n",
    "    print(f\"[{_now()}][tft] grid saved -> {grid_path}\")\n",
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# 全局 time_idx 连续性检查\n",
    "grid_df = grid_lazy.select([g_date, g_time, \"time_idx\"]).collect()\n",
    "ti = grid_df[\"time_idx\"]\n",
    "assert grid_df.select(pl.col(\"time_idx\").is_duplicated().any()).item() is False\n",
    "assert ti.max() - ti.min() + 1 == len(ti), \"全局 time_idx 不连续\"\n",
    "\n",
    "# 3) 时间窗 + join time_idx + 挑列\n",
    "lo = cfg[\"dates\"][\"tft_dates\"][\"date_lo\"]; hi = cfg[\"dates\"][\"tft_dates\"][\"date_hi\"]\n",
    "lw = lf.filter(pl.col(g_date).is_between(lo, hi, closed=\"both\"))\n",
    "lw_with_idx = (\n",
    "    lw.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "        .select(need_cols + [\"time_idx\"])\n",
    "        .sort(TIME_SORT)\n",
    ")\n",
    "print(f\"[{_now()}][tft] schema -> {lw_with_idx.collect_schema().names()}\")\n",
    "\n",
    "# 4) 小窗 demo → Pandas\n",
    "demo_lo, demo_hi = 1600, 1630\n",
    "df = (\n",
    "    lw_with_idx\n",
    "    .filter(pl.col(g_date).is_between(demo_lo, demo_hi, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ").sort_values([g_sym, \"time_idx\"])\n",
    "df[g_sym] = df[g_sym].astype(\"string\")\n",
    "df[g_sym] = df[g_sym].astype(\"category\")\n",
    "# 类型\n",
    "df[\"time_idx\"] = df[\"time_idx\"].astype(\"int64\")\n",
    "\n",
    "# 缺失处理：连续特征加标记 + ffill + 0 填充\n",
    "miss_flags, df = add_missing_flags_and_fill(df, g_sym, feature_cols)\n",
    "unknown_reals = list(dict.fromkeys(feature_cols + miss_flags))   # 仅特征 + 标记（不含 target）\n",
    "\n",
    "# 降精度\n",
    "for c in unknown_reals:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "df[target_col] = pd.to_numeric(df[target_col], downcast=\"float\")\n",
    "\n",
    "# 因果时间切分\n",
    "cutoff = int(df[\"time_idx\"].quantile(0.9))\n",
    "train_df = df[df[\"time_idx\"] <= cutoff].copy()\n",
    "val_df   = df[df[\"time_idx\"] >  cutoff].copy()\n",
    "\n",
    "# 5) 按 symbol 标准化（仅连续特征；不含标记、不含 target）\n",
    "cont_cols = [c for c in feature_cols if c in train_df.columns]\n",
    "train_df, val_df, mu_df, std_df = standardize_by_symbol(train_df, val_df, g_sym, cont_cols)\n",
    "train_df[cont_cols] = train_df[cont_cols].astype(\"float32\")\n",
    "val_df[cont_cols]   = val_df[cont_cols].astype(\"float32\")\n",
    "for f in miss_flags:\n",
    "    if f in train_df: train_df[f] = train_df[f].astype(\"int8\")\n",
    "    if f in val_df:   val_df[f]   = val_df[f].astype(\"int8\")\n",
    "\n",
    "print(f\"[{_now()}][tft] standardize done, cont_cols={len(cont_cols)}\")\n",
    "\n",
    "# 6) TimeSeriesDataSet\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df.sort_values([g_sym, \"time_idx\"]),\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target_col,\n",
    "    group_ids=[g_sym],\n",
    "    static_categoricals=[g_sym],    # 等价于 [\"symbol_id\"]\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],   # 后续可加交易日历等“未来可知”变量\n",
    "    time_varying_known_reals=[],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=unknown_reals,  # 只放特征 + 标记\n",
    "    max_encoder_length=int(cfg.get(\"tft\",{}).get(\"enc_len\", 30)),\n",
    "    max_prediction_length=1,\n",
    "    target_normalizer=None,\n",
    "    categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "validation = TimeSeriesDataSet.from_dataset(training, val_df, stop_randomization=True)\n",
    "\n",
    "train_loader = training.to_dataloader(\n",
    "    train=True, batch_size=int(cfg.get(\"tft\",{}).get(\"batch_size\", 1024)), num_workers=4\n",
    ")\n",
    "val_loader = validation.to_dataloader(\n",
    "    train=False, batch_size=int(cfg.get(\"tft\",{}).get(\"batch_size\", 1024)), num_workers=4\n",
    ")\n",
    "\n",
    "# 7) Trainer + Model\n",
    "trainer = build_trainer()\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=float(cfg.get(\"tft\",{}).get(\"lr\", 1e-3)),\n",
    "    hidden_size=int(cfg.get(\"tft\",{}).get(\"hidden_size\",128)),\n",
    "    attention_head_size=int(cfg.get(\"tft\",{}).get(\"heads\",4)),\n",
    "    dropout=float(cfg.get(\"tft\",{}).get(\"dropout\",0.2)),\n",
    "    loss=SMAPE(),\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_loader, val_loader)\n",
    "print(f\"[{_now()}][tft] ===== finished =====\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72dafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86380b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff35f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92155428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6466d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
