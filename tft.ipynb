{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34503ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin_ml/Jackson/projects/js/JS/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-03 07:47:42] imports ok\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import time, numpy as np, polars as pl, pandas as pd, torch, lightning as L\n",
    "from pathlib import Path\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder, GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "\n",
    "def _now(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ab8741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ready\n"
     ]
    }
   ],
   "source": [
    "# ---- 关键配置（按需改）----\n",
    "target_col = cfg[\"target\"]                 # e.g. \"responder_6\"\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "weight_col = cfg[\"weight\"]\n",
    "\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "base_features   = [\"feature_36\"]\n",
    "resp_his_feats  = [\"responder_3_prevday_std\"]\n",
    "feat_his_feats  = [\"feature_08__ewm5\"]\n",
    "feature_cols = list(dict.fromkeys(base_features + resp_his_feats + feat_his_feats))\n",
    "\n",
    "need_cols = list(dict.fromkeys([g_sym, g_date, g_time, weight_col, target_col] + time_features + feature_cols))\n",
    "\n",
    "# CV & 训练\n",
    "N_SPLITS   = 5\n",
    "GAP_DAYS   = 7\n",
    "TRAIN_TO_VAL = 4               # 训练:验证 = 4:1\n",
    "ENC_LEN    = 10\n",
    "PRED_LEN   = 1\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3\n",
    "HIDDEN     = 64\n",
    "HEADS      = 2\n",
    "DROPOUT    = 0.1\n",
    "MAX_EPOCHS_PER_SHARD = 1\n",
    "CHUNK_DAYS = 20               # 训练分片：每片多少天\n",
    "\n",
    "print(\"config ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d88ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "data_path  = fs.glob(f\"{panel_dir}/*.parquet\")\n",
    "az_path = [f\"az://{p}\" for p in data_path]\n",
    "lf_data = pl.scan_parquet(az_path, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局 time_idx 仅一次\n",
    "lf_grid = (\n",
    "    lf_data.select([g_date, g_time]).unique()\n",
    "        .sort([g_date, g_time])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\"); ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "\n",
    "lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86cfd570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-03 08:11:18] lazyframe ready\n"
     ]
    }
   ],
   "source": [
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# 接入 time_idx + 只保留所需列（仍是 Lazy）\n",
    "lf0 = (\n",
    "    lf_data.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "          .select(need_cols + [\"time_idx\"])\n",
    "          .sort([g_date, g_time, g_sym])\n",
    ")\n",
    "\n",
    "print(f\"[{_now()}] lazyframe ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02b157ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全量天列表\n",
    "all_days = (\n",
    "    lf0.select(pl.col(g_date)).unique().sort(by=g_date)\n",
    "       .collect(streaming=True).get_column(g_date).to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f408ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造滑动时间窗 CV\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0: return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0: return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days): break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a831bc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615,\n",
       "         1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626,\n",
       "         1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637,\n",
       "         1638, 1639, 1640], dtype=int32),\n",
       "  array([1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657],\n",
       "        dtype=int32)),\n",
       " (array([1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625,\n",
       "         1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636,\n",
       "         1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647,\n",
       "         1648, 1649, 1650], dtype=int32),\n",
       "  array([1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667],\n",
       "        dtype=int32)),\n",
       " (array([1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635,\n",
       "         1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
       "         1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657,\n",
       "         1658, 1659, 1660], dtype=int32),\n",
       "  array([1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677],\n",
       "        dtype=int32)),\n",
       " (array([1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645,\n",
       "         1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656,\n",
       "         1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667,\n",
       "         1668, 1669, 1670], dtype=int32),\n",
       "  array([1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687],\n",
       "        dtype=int32)),\n",
       " (array([1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655,\n",
       "         1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666,\n",
       "         1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677,\n",
       "         1678, 1679, 1680], dtype=int32),\n",
       "  array([1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697],\n",
       "        dtype=int32))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8c5edd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats_hi (for global z-score) = 1640; first-fold train days end at this day.\n"
     ]
    }
   ],
   "source": [
    "# 选“标准化统计区间”上界：第1折训练天的最大值（不含验证）\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"stats_hi (for global z-score) = {stats_hi}; first-fold train days end at this day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_dir = P(\"local\", \"tft/clean_by_day\")\n",
    "ensure_dir_local(clean_dir)\n",
    "\n",
    "# ========== 1) 连续特征：一次性处理 inf->null、打缺失标记、组内 ffill、兜底 0 ==========\n",
    "# 先做 flag（要基于原始缺失），再做填充；合并成两次 with_columns，避免在 for 循环里多次改列\n",
    "inf2null_exprs = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
    "                for c in feature_cols] # inf -> null 不产生新列\n",
    "flags_exprs    = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\")\n",
    "                for c in feature_cols] # 产生新列\n",
    "fill_exprs     = [pl.col(c).forward_fill().over(g_sym).fill_null(0.0).alias(c)\n",
    "                for c in feature_cols] # 填充，覆盖原列\n",
    "\n",
    "lf_clean = (\n",
    "    lf0.with_columns(inf2null_exprs)         # inf -> null\n",
    "    .with_columns(flags_exprs)            # 缺失标记（基于原始缺失）\n",
    "    .with_columns(fill_exprs)             # 组内 ffill + 兜底 0\n",
    ")\n",
    "\n",
    "# ========== 2) 训练区间（<= stats_hi）按组计算 mu/std（一次） ==========\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .group_by(g_sym)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in feature_cols] +\n",
    "        [pl.col(c).std().alias(f\"std_{c}\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 2) 训练期全局统计（作为回退）\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in feature_cols] +\n",
    "            [pl.col(c).std().alias(f\"std_{c}_glb\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 3) 把全局统计作为常量列加到每行（cross join 方式）\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\")\n",
    "\n",
    "# 4) join per-symbol 统计，并对每个特征做回退 & z-score\n",
    "\n",
    "lf_z = lf_z.join(lf_stats_sym, on=g_sym, how=\"left\")\n",
    "\n",
    "# 对每个特征做回退 & z-score\n",
    "eps = 1e-6\n",
    "z_cols = []\n",
    "for c in feature_cols:\n",
    "    mu_c_sym, std_c_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_c_glb, std_c_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    c_z = f\"{c}_z\"\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_c_sym).is_null()).then(pl.col(mu_c_glb)).otherwise(pl.col(mu_c_sym)).alias(f\"{c}_mu_use\"),\n",
    "        pl.when(pl.col(std_c_sym).is_null() | (pl.col(std_c_sym) == 0)).then(pl.col(std_c_glb)).otherwise(pl.col(std_c_sym)).alias(f\"{c}_std_use\")\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(f\"{c}_mu_use\")) / (pl.col(f\"{c}_std_use\") + eps)).alias(c_z)\n",
    "    ).drop([mu_c_glb, std_c_glb, mu_c_sym, std_c_sym, f\"{c}_mu_use\", f\"{c}_std_use\"])\n",
    "    z_cols.append(c_z)\n",
    "\n",
    "# 5) 输出列（z_特征 + isna 标记 + 时间/分类/目标/权重）\n",
    "namark_cols = [f\"{c}__isna\" for c in feature_cols]\n",
    "out_cols = [g_sym, g_date, g_time, \"time_idx\", weight_col, target_col] + time_features + z_cols + namark_cols\n",
    "\n",
    "lf_out = lf_z.select(out_cols).sort([g_date, g_time, g_sym])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== 4) 分块收集 + 分区写盘 ==========\n",
    "# 关键：不要“逐天 collect”，而是每次收集一批天，然后一次性按 day 分区写入，显著减少 IO 次数\n",
    "CHUNK_DAYS = 20  # 可根据机器内存/速度调整；比如 10~30 天一块\n",
    "day_list = list(map(int, all_days))\n",
    "day_chunks = [day_list[i:i+CHUNK_DAYS] for i in range(0, len(day_list), CHUNK_DAYS)]\n",
    "\n",
    "for ci, chunk in enumerate(day_chunks, 1):\n",
    "    df_chunk = (\n",
    "        lf_out.filter(pl.col(g_date).is_in(chunk))\n",
    "              .collect(streaming=True)              # 单次 collect 一大块\n",
    "    )\n",
    "    # 将symbol_id 转为 category\n",
    "    \n",
    "    # 利用 polars 的分区写出（需要较新的 polars；若你的版本不支持 partition_by，退化为循环写）\n",
    "    try:\n",
    "        df_chunk.write_parquet(\n",
    "            Path(clean_dir).as_posix(),\n",
    "            compression=\"zstd\",\n",
    "            # ★ 这行很关键：一次性把该 chunk 里所有 day 写成多个子文件\n",
    "            partition_by=[g_date],                 # polars>=0.20 支持；不支持就改成下面的 fallback\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fallback：分天写（比原先好，因为前面的 join/with_columns 已经批量完成，只剩 IO）\n",
    "        for d in chunk:\n",
    "            out_path = Path(clean_dir) / f\"day={d}.parquet\"\n",
    "            df_chunk.filter(pl.col(g_date) == d).write_parquet(out_path.as_posix(), compression=\"zstd\")\n",
    "    print(f\"[{_now()}] chunk {ci}/{len(day_chunks)} -> days {chunk[0]}..{chunk[-1]} written\")\n",
    "\n",
    "print(f\"[{_now()}] cleaned & standardized shards written to {clean_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Template & Validation ===\n",
    "import glob, numpy as np, polars as pl, pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "\n",
    "# 假设以下变量已由你前置代码给出：\n",
    "# cfg, P, clean_dir, folds_by_day, g_sym, g_date, g_time, target_col, weight_col\n",
    "# time_features, z_cols, namark_cols, ENC_LEN, PRED_LEN, BATCH_SIZE\n",
    "\n",
    "fold_idx = 1\n",
    "train_days, val_days = folds_by_day[fold_idx-1]\n",
    "days_sorted = np.sort(train_days)\n",
    "\n",
    "# ---- 验证集 ----\n",
    "val_paths = []\n",
    "for d in val_days:\n",
    "    val_paths.extend(glob.glob(f\"{clean_dir}/date_id={d}/*.parquet\"))\n",
    "\n",
    "pdf_val = pl.scan_parquet(val_paths).collect(streaming=True).to_pandas()\n",
    "pdf_val[g_sym] = pdf_val[g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "# ---- Template（用第一天分片建立，固化 encoders/scalers）----\n",
    "first_train_day = int(days_sorted[0])\n",
    "tmpl_paths = glob.glob(f\"{clean_dir}/date_id={first_train_day}/*.parquet\")\n",
    "pdf_tmpl = pl.scan_parquet(tmpl_paths).collect(streaming=True).to_pandas()\n",
    "pdf_tmpl[g_sym] = pdf_tmpl[g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "unknown_reals = time_features + z_cols + namark_cols\n",
    "# sklearn 的 FunctionTransformer 在 func=None 时就是恒等变换，并且是可 pickling 的\n",
    "identity_scalers = {c: FunctionTransformer(validate=False) for c in unknown_reals}\n",
    "\n",
    "\n",
    "\n",
    "template = TimeSeriesDataSet(\n",
    "    pdf_tmpl.sort_values([g_sym, \"time_idx\"]),\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target_col,\n",
    "    group_ids=[g_sym],\n",
    "    weight=weight_col,\n",
    "    max_encoder_length=ENC_LEN,\n",
    "    max_prediction_length=PRED_LEN,\n",
    "    static_categoricals=[g_sym],\n",
    "    time_varying_unknown_reals=unknown_reals,\n",
    "    target_normalizer=None,\n",
    "    categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=True, add_target_scales=True, add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    scalers=identity_scalers,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    template, data=pdf_val.sort_values([g_sym, \"time_idx\"]), stop_randomization=True\n",
    ")\n",
    "val_loader = validation.to_dataloader(\n",
    "    train=False, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "len(val_loader), pdf_tmpl.shape, pdf_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b67c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: IterableDataset 真·流式（按分片产出 batch） + 加权 WR² ===\n",
    "import random, torch, polars as pl, glob\n",
    "from torch.utils.data import IterableDataset\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "class ShardedBatchStream(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        template_tsd,\n",
    "        shard_days,\n",
    "        clean_dir: str,\n",
    "        g_sym: str,\n",
    "        batch_size: int = 1024,\n",
    "        num_workers: int = 4,\n",
    "        shuffle_within_shard: bool = True,\n",
    "        buffer_batches: int = 0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.template = template_tsd\n",
    "        self.days = list(map(int, shard_days))\n",
    "        self.clean_dir = clean_dir\n",
    "        self.g_sym = g_sym\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle_within_shard = shuffle_within_shard\n",
    "        self.buffer_batches = buffer_batches\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = random.Random(self.seed)\n",
    "        days = self.days[:]\n",
    "        rng.shuffle(days)\n",
    "\n",
    "        from collections import deque\n",
    "        buf = deque()\n",
    "\n",
    "        for d in days:\n",
    "            paths = glob.glob(f\"{self.clean_dir}/date_id={d}/*.parquet\")\n",
    "            if not paths:\n",
    "                continue\n",
    "\n",
    "            pdf = pl.scan_parquet(paths).collect(streaming=True).to_pandas()\n",
    "            if pdf.empty:\n",
    "                continue\n",
    "            pdf[self.g_sym] = pdf[self.g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "            tsds = TimeSeriesDataSet.from_dataset(\n",
    "                self.template,\n",
    "                data=pdf.sort_values([self.g_sym, \"time_idx\"]),\n",
    "                stop_randomization=True,\n",
    "            )\n",
    "\n",
    "            dl = tsds.to_dataloader(\n",
    "                train=True,\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "                shuffle=self.shuffle_within_shard,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=self.num_workers > 0,\n",
    "            )\n",
    "\n",
    "            for batch in dl:\n",
    "                if self.buffer_batches > 0:\n",
    "                    buf.append(batch)\n",
    "                    if len(buf) >= self.buffer_batches:\n",
    "                        k = rng.randrange(len(buf))\n",
    "                        if k:\n",
    "                            buf.rotate(-k)\n",
    "                        yield buf.popleft()\n",
    "                else:\n",
    "                    yield batch\n",
    "\n",
    "        while buf:\n",
    "            yield buf.popleft()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166851e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_wr2_tensor(yhat: torch.Tensor, y: torch.Tensor, w: torch.Tensor, eps: float = 1e-9):\n",
    "    x  = yhat.reshape(-1)\n",
    "    yt = y.reshape(-1)\n",
    "    wt = w.reshape(-1)\n",
    "    m = torch.isfinite(x) & torch.isfinite(yt) & torch.isfinite(wt)\n",
    "    x, yt, wt = x[m], yt[m], torch.clamp(wt[m], min=0.0)\n",
    "    ws = torch.clamp(wt.sum(), min=eps)\n",
    "    mx = (wt * x ).sum()/ws\n",
    "    my = (wt * yt).sum()/ws\n",
    "    cov = (wt * (x - mx) * (yt - my)).sum()/ws\n",
    "    vx  = (wt * (x - mx )**2).sum()/ws\n",
    "    vy  = (wt * (yt - my)**2).sum()/ws\n",
    "    return (cov * cov) / (torch.clamp(vx, min=eps) * torch.clamp(vy, min=eps) + eps)\n",
    "\n",
    "class LogWeightedWR2Callback(Callback):\n",
    "    \"\"\"验证阶段逐批前向；y_true 与 w 直接来自 batch 第二项 (target, weight)。\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._buf_yhat, self._buf_y, self._buf_w = [], [], []\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        self._buf_yhat.clear(); self._buf_y.clear(); self._buf_w.clear()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "        # ✅ 官方接口：batch 是 (x, (target, weight))\n",
    "        x, (y, w) = batch\n",
    "        # 当前设备前向（Lightning 已放好设备）\n",
    "        out   = pl_module(x)\n",
    "        yhat  = out[\"prediction\"]      # [B, pred_len, 1] 或 [B, pred_len]\n",
    "        # 对齐维度：若 yhat 是 [...,1] 则挤掉最后一维\n",
    "        if yhat.dim() == 3 and yhat.size(-1) == 1:\n",
    "            yhat = yhat.squeeze(-1)\n",
    "        # 累计\n",
    "        self._buf_yhat.append(yhat.reshape(-1).detach())\n",
    "        self._buf_y.append(y.reshape(-1).detach())\n",
    "        # 有些数据没有提供逐时间步 weight，会给 shape=[B, pred_len]；正好与 y 对齐\n",
    "        self._buf_w.append(w.reshape(-1).detach())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if not self._buf_yhat:\n",
    "            return\n",
    "        yhat = torch.cat(self._buf_yhat, dim=0)\n",
    "        y    = torch.cat(self._buf_y,    dim=0)\n",
    "        w    = torch.cat(self._buf_w,    dim=0)\n",
    "        wr2_w = weighted_wr2_tensor(yhat, y, w).float().cpu()\n",
    "        trainer.callback_metrics[\"val_WR2_weighted\"] = wr2_w\n",
    "        if trainer.logger is not None:\n",
    "            trainer.logger.log_metrics({\"val_WR2_weighted\": wr2_w.item()}, step=trainer.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: 修正后的带权 WR² 回调 + 模型/Trainer ===\n",
    "import torch, pandas as pd, lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor, Callback\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import MAE\n",
    "\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "\n",
    "# 设置随机种子\n",
    "L.seed_everything(int(cfg.get(\"seed\", 42)), workers=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    LogWeightedWR2Callback(),\n",
    "    EarlyStopping(monitor=\"val_WR2_weighted\", mode=\"max\", patience=5),\n",
    "    ModelCheckpoint(monitor=\"val_WR2_weighted\", mode=\"max\", save_top_k=1,\n",
    "                    filename=\"tft-best-{epoch:02d}-{val_WR2_weighted:.5f}\"),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "]\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=P(\"local\", \"tft/logs\"), name=\"tft\", default_hp_metric=False)\n",
    "\n",
    "VAL_EVERY_STEPS = 10\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\", devices=1, precision=32,\n",
    "    max_epochs=1,\n",
    "    val_check_interval=VAL_EVERY_STEPS,\n",
    "    num_sanity_val_steps=0,\n",
    "    gradient_clip_val=0.5,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=P(\"local\", \"tft/ckpts\"),\n",
    ")\n",
    "\n",
    "\n",
    "# 创建模型\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    template,\n",
    "    loss=MAE(),\n",
    "    logging_metrics=[val_loss],\n",
    "    learning_rate=float(cfg.get(\"tft\", {}).get(\"lr\", 1e-3)),\n",
    "    hidden_size=int(cfg.get(\"tft\", {}).get(\"hidden_size\", 128)),\n",
    "    attention_head_size=int(cfg.get(\"tft\", {}).get(\"heads\", 4)),\n",
    "    dropout=float(cfg.get(\"tft\", {}).get(\"dropout\", 0.2)),\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(\"model device (before fit):\", next(tft.parameters()).device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c159797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: 构建流式训练集 & 单次 fit ===\n",
    "train_stream = ShardedBatchStream(\n",
    "    template_tsd=template,\n",
    "    shard_days=days_sorted,\n",
    "    clean_dir=clean_dir,\n",
    "    g_sym=g_sym,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=4,\n",
    "    shuffle_within_shard=True,\n",
    "    buffer_batches=16,   # 0 代表关闭跨分片缓冲打乱；8~64 可微调\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_stream, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23d4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cff97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5e406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dfb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import MAE  # 用 MAE 训练\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "\n",
    "# 项目内工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local\n",
    "\n",
    "def _now(): \n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72595a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"[{_now()}][tft] ===== start =====\")\n",
    "target_col = cfg[\"target\"]                 # 如 \"responder_6\"\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # 如 (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "weight_col = cfg[\"weight\"]\n",
    "TIME_SORT = cfg[\"sorts\"].get(\"time_major\", [g_date, g_time, g_sym])\n",
    "\n",
    "# 1) 选择特征列（示例：你后续替换为真实列表）\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "base_features   = [\"feature_36\", \"feature_06\"]                 # TODO: 放入你的真实特征集合\n",
    "resp_his_feats  = [\"responder_5_prevday_std\", \"responder_3_prevday_std\", \"responder_4_prev_tail_d1\"]  # 示例\n",
    "feat_his_feats  = [\"feature_08__ewm5\", \"feature_53__rstd3\"]           # 示例\n",
    "feature_cols = list(dict.fromkeys(base_features + resp_his_feats + feat_his_feats))\n",
    "\n",
    "need_cols = list(dict.fromkeys(cfg[\"keys\"] + [weight_col] + [target_col] + time_features + feature_cols))\n",
    "\n",
    "# 2) 读 panel（Lazy） & 构 grid\n",
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "glob_pat  = f\"{panel_dir}/*.parquet\"\n",
    "if not fs.glob(glob_pat.replace(\"az://\", \"\")):\n",
    "    raise FileNotFoundError(f\"No parquet shards under: {glob_pat}\")\n",
    "lf = pl.scan_parquet(glob_pat, storage_options=storage_options)\n",
    "\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\")\n",
    "if not Path(grid_path).exists():\n",
    "    lf_grid = (\n",
    "        lf.select([g_date, g_time]).unique()\n",
    "        .sort([g_date, g_time])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    "    )\n",
    "    ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "    lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")\n",
    "    print(f\"[{_now()}][tft] grid saved -> {grid_path}\")\n",
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# 全局 time_idx 连续性（安全检查）\n",
    "grid_df = grid_lazy.select([g_date, g_time, \"time_idx\"]).collect()\n",
    "ti = grid_df[\"time_idx\"]\n",
    "assert grid_df.select(pl.col(\"time_idx\").is_duplicated().any()).item() is False\n",
    "assert ti.max() - ti.min() + 1 == len(ti), \"全局 time_idx 不连续\"\n",
    "\n",
    "# 3) 时间窗 + join time_idx + 选列\n",
    "lo = cfg[\"dates\"][\"tft_dates\"][\"date_lo\"]; hi = cfg[\"dates\"][\"tft_dates\"][\"date_hi\"]\n",
    "lw = lf.filter(pl.col(g_date).is_between(lo, hi, closed=\"both\"))\n",
    "lf0 = (\n",
    "    lw.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "        .select(need_cols + [\"time_idx\"])\n",
    "        .sort(TIME_SORT)\n",
    ")\n",
    "print(f\"[{_now()}][tft] schema -> {lf0.collect_schema().names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed48bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 转换 symbol_id 为 category\n",
    "lw_with_idx = lw_with_idx.with_columns(pl.col(g_sym).cast(str).cast(pl.Categorical))\n",
    "\n",
    "# 1) 把 ±inf 变成 null（只对 feature_cols）\n",
    "lw_with_idx = lw_with_idx.with_columns([\n",
    "    pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
    "    for c in feature_cols\n",
    "])\n",
    "\n",
    "# 2) 缺失标记（基于上一步的 null）\n",
    "miss_flags = [f\"{c}__isna\" for c in feature_cols]\n",
    "lw_with_idx = lw_with_idx.with_columns([\n",
    "    pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\")\n",
    "    for c in feature_cols\n",
    "])\n",
    "\n",
    "# 3) 组内前向填充 + 兜底 0（仍然只作用于 feature_cols）\n",
    "#    注意：over(g_sym) 需要 g_sym 在表中；确保你的数据按时间排序后再用\n",
    "lw_with_idx = lw_with_idx.with_columns([\n",
    "    pl.col(c).fill_null(strategy=\"forward\").over(g_sym).fill_null(0.0).alias(c)\n",
    "    for c in feature_cols\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473d271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sliding_cv_by_days(days: np.ndarray, *, n_splits: int, gap_days: int = 5, train_to_val: int = 9):\n",
    "    days = np.asarray(days).ravel()  # 唯一天且已排序的天列表\n",
    "    N, R, K, G = len(days), int(train_to_val), int(n_splits), int(gap_days)\n",
    "    usable = N - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0:\n",
    "        return []\n",
    "\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0:\n",
    "        return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "\n",
    "    v_lo = T + G\n",
    "    folds = []\n",
    "    for V_i in v_lens:\n",
    "        v_hi  = v_lo + V_i\n",
    "        tr_hi = v_lo - G\n",
    "        tr_lo = tr_hi - T\n",
    "        if tr_lo < 0 or v_hi > N:\n",
    "            break\n",
    "        train_days = days[tr_lo:tr_hi]   # 连续天段\n",
    "        val_days   = days[v_lo:v_hi]\n",
    "        folds.append((train_days, val_days))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "# 生成“全量天列表”\n",
    "all_days = lw_with_idx.select(pl.col(g_date)).unique().sort(by=g_date).collect().get_column(g_date).to_numpy()\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=5, gap_days=7, train_to_val=4)\n",
    "\n",
    "for k, (tr_days, va_days) in enumerate(folds_by_day, 1):\n",
    "    lf_tr = lw_with_idx.filter(pl.col(g_date).is_in(tr_days))\n",
    "    lf_va = lw_with_idx.filter(pl.col(g_date).is_in(va_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df51f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4) 取一个窗口 demo → pandas（你可以换成全量）\n",
    "demo_lo, demo_hi = 1610, 1698\n",
    "df = (\n",
    "    lw_with_idx\n",
    "    .filter(pl.col(g_date).is_between(demo_lo, demo_hi, closed=\"both\"))\n",
    "    .collect(streaming=True)\n",
    "    .to_pandas()\n",
    ").sort_values([g_sym, \"time_idx\"])\n",
    "\n",
    "# 打印df实际日期范围\n",
    "print(f\"实际日期范围: {df[g_date].min()} 到 {df[g_date].max()}\")\n",
    "\n",
    "# 类型\n",
    "df[g_sym] = df[g_sym].astype(\"string\").astype(\"category\")\n",
    "df[\"time_idx\"] = df[\"time_idx\"].astype(\"int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder, GroupNormalizer\n",
    "\n",
    "# unknown_reals：特征 + 标记（不包含 weight）\n",
    "unknown_reals = list(dict.fromkeys(time_features + feature_cols + miss_flags ))\n",
    "# TimeSeriesDataSet —— 把权重列作为 weight 传入；不要放进 unknown_reals； 按照group对自变量做标准化\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df.sort_values([g_sym, \"time_idx\"]),\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target_col,\n",
    "    group_ids=[g_sym],\n",
    "    weight=weight_col,       # 关键：启用“样本加权”\n",
    "    \n",
    "    max_encoder_length=36,\n",
    "    max_prediction_length=1,\n",
    "    static_categoricals=[g_sym],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=unknown_reals,   # 不含 weight\n",
    "    \n",
    "    target_normalizer=None,\n",
    "    scalers={c: GroupNormalizer(groups=[g_sym]) for c in feature_cols},\n",
    "    \n",
    "    categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "    add_relative_time_idx=True, add_target_scales=True, add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, val_df, stop_randomization=True)\n",
    "\n",
    "train_loader = training.to_dataloader(\n",
    "    train=True, batch_size=int(cfg.get(\"tft\",{}).get(\"batch_size\", 1024)), num_workers=4\n",
    ")\n",
    "val_loader = validation.to_dataloader(\n",
    "    train=False, batch_size=int(cfg.get(\"tft\",{}).get(\"batch_size\", 1024)), num_workers=4\n",
    ")\n",
    "\n",
    "from pytorch_forecasting.metrics import MAE  # 你这版有 MSE 就换 MSE()\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    loss=MAE(),\n",
    "    learning_rate=float(cfg.get(\"tft\", {}).get(\"lr\", 1e-3)),\n",
    "    hidden_size=int(cfg.get(\"tft\", {}).get(\"hidden_size\", 128)),\n",
    "    attention_head_size=int(cfg.get(\"tft\", {}).get(\"heads\", 4)),\n",
    "    dropout=float(cfg.get(\"tft\", {}).get(\"dropout\", 0.2)),\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning as L\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=P(\"local\", \"tft/logs\"), name=\"tft\", default_hp_metric=False)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
    "    ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"tft-best-{epoch:02d}-{val_loss:.5f}\"),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "]\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=int(cfg.get(\"tft\", {}).get(\"max_epochs\", 2)),\n",
    "    accelerator=\"auto\",\n",
    "    precision=32,          # 稳定优先，跑顺了再试 bf16\n",
    "    gradient_clip_val=0.5,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=P(\"local\", \"tft/ckpts\"),\n",
    ")\n",
    "\n",
    "L.seed_everything(int(cfg.get(\"seed\", 42)), workers=True)\n",
    "trainer.fit(tft, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f984a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 覆盖整个 validation dataloader 的预测\n",
    "pred = tft.predict(val_loader, mode=\"prediction\")\n",
    "# pred 通常是 [N, max_prediction_length]；你现在 max_prediction_length=1\n",
    "import torch, numpy as np\n",
    "if isinstance(pred, torch.Tensor):\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "pred = np.asarray(pred)\n",
    "y_pred = pred.squeeze()  # 变成 [N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tft.predict(val_loader, mode=\"prediction\", return_index=True)\n",
    "y_pred = res.output\n",
    "if isinstance(y_pred, torch.Tensor):\n",
    "    y_pred = y_pred.detach().cpu()\n",
    "# 压成一维（max_prediction_length=1 的情况）\n",
    "\n",
    "y_pred = y_pred[:, -1]\n",
    "y_pred = np.asarray(y_pred)\n",
    "y_pred = pd.DataFrame(y_pred, columns=[\"y_pred\"])\n",
    "\n",
    "y_pred_index = res.index\n",
    "# join 真实值 val_df.target\n",
    "\n",
    "y_actual = y_pred_index.merge(\n",
    "    val_df[[g_sym, \"time_idx\", target_col]],\n",
    "    on=[g_sym, \"time_idx\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "# 拼接真实值与预测值\n",
    "y_pred_actual = pd.concat([y_actual, y_pred], axis=1)\n",
    "\n",
    "# 画图展示\n",
    "y_pred_actual.plot(x=\"time_idx\", y=[target_col, \"y_pred\"], kind=\"line\", alpha=0.7)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
