{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a5d780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 13:37:57] imports ok\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库（stdlib） ─────────────────────────────────────────────────────────────\n",
    "import os\n",
    "\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "# ── 第三方（third-party） ───────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import torch\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, IterableDataset, get_worker_info\n",
    "\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import (\n",
    "    TimeSeriesDataSet,\n",
    "    TemporalFusionTransformer,\n",
    ")\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import (\n",
    "    NaNLabelEncoder,\n",
    ")\n",
    "\n",
    "os.environ.setdefault(\"POLARS_MAX_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "pl.enable_string_cache()  # 字符串编码缓存，提速 join/分类\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "\n",
    "# 全局一次性设置（不要放到训练/epoch/折叠循环里）\n",
    "import torch.backends.cudnn as cudnn \n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\") \n",
    "\n",
    "\n",
    "def _now(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入文件 /mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\n",
    "input_file = \"/mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\"\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    data = file.read()\n",
    "lines = data.split('\\n')\n",
    "features = [line.split()[0] for line in lines if line.strip() and not line.startswith('#')]\n",
    "\n",
    "print(f\"[{_now()}] loaded {len(features)} features from {input_file}\")  \n",
    "\n",
    "topK_features = features[:50]\n",
    "print(f\"[{_now()}] top 10 features: {topK_features}\") \n",
    "\n",
    "\n",
    "# 严格匹配基础特征：feature_后跟两位数字、无后缀\n",
    "base_features = [f for f in features if re.fullmatch(r\"feature_\\d{2}\", f)]\n",
    "# 响应类历史特征：以 responder_ 开头\n",
    "resp_his_feats = [f for f in features if f.startswith(\"responder_\")]\n",
    "# 其他派生的 feature_*（有后缀），但排除基础特征\n",
    "feat_his_feats = [f for f in features if f.startswith(\"feature_\") and f not in base_features]\n",
    "\n",
    "# 只保留 topK 特征\n",
    "base_features = [f for f in base_features if f in topK_features]\n",
    "resp_his_feats = [f for f in resp_his_feats if f in topK_features]\n",
    "feat_his_feats = [f for f in feat_his_feats if f in topK_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4611fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- 关键配置（按需改）----\n",
    "target_col = cfg[\"target\"]                 # e.g. \"responder_6\"\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "weight_col = cfg[\"weight\"]\n",
    "\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "\n",
    "feature_cols = list([\"feature_36\"])\n",
    "need_cols = list(dict.fromkeys([g_sym, g_date, g_time, weight_col, target_col] + time_features + feature_cols))\n",
    "\n",
    "# CV & 训练\n",
    "N_SPLITS   = 2\n",
    "GAP_DAYS   = 7\n",
    "TRAIN_TO_VAL = 4               # 训练:验证 = 4:1\n",
    "ENC_LEN    = 10\n",
    "PRED_LEN   = 1\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "HIDDEN     = 64\n",
    "HEADS      = 2\n",
    "DROPOUT    = 0.1\n",
    "MAX_EPOCHS_PER_SHARD = 1\n",
    "CHUNK_DAYS = 20               # 训练分片：每片多少天\n",
    "\n",
    "print(\"config ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e55c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "data_path  = fs.glob(f\"{panel_dir}/*.parquet\")\n",
    "az_path = [f\"az://{p}\" for p in data_path]\n",
    "lf_data = pl.scan_parquet(az_path, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b71745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 792)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>time_bucket</th><th>responder_6</th><th>weight</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>&hellip;</th><th>feature_66__rz3</th><th>feature_67__rmean3</th><th>feature_67__rstd3</th><th>feature_67__rz3</th><th>feature_68__rmean3</th><th>feature_68__rstd3</th><th>feature_68__rz3</th><th>feature_69__rmean3</th><th>feature_69__rstd3</th><th>feature_69__rz3</th><th>feature_70__rmean3</th><th>feature_70__rstd3</th><th>feature_70__rz3</th><th>feature_71__rmean3</th><th>feature_71__rstd3</th><th>feature_71__rz3</th><th>feature_72__rmean3</th><th>feature_72__rstd3</th><th>feature_72__rz3</th><th>feature_73__rmean3</th><th>feature_73__rstd3</th><th>feature_73__rz3</th><th>feature_74__rmean3</th><th>feature_74__rstd3</th><th>feature_74__rz3</th><th>feature_75__rmean3</th><th>feature_75__rstd3</th><th>feature_75__rz3</th><th>feature_76__rmean3</th><th>feature_76__rstd3</th><th>feature_76__rz3</th><th>feature_77__rmean3</th><th>feature_77__rstd3</th><th>feature_77__rz3</th><th>feature_78__rmean3</th><th>feature_78__rstd3</th><th>feature_78__rz3</th></tr><tr><td>i32</td><td>i32</td><td>i32</td><td>u8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>1605</td><td>0</td><td>0</td><td>-1.338004</td><td>3.771476</td><td>3.249785</td><td>1.644559</td><td>2.951776</td><td>3.072733</td><td>3.042695</td><td>-1.182882</td><td>-0.097529</td><td>-0.417959</td><td>0.358264</td><td>11.0</td><td>7.0</td><td>76.0</td><td>-0.815249</td><td>1.583994</td><td>-0.189974</td><td>-0.677662</td><td>-0.494887</td><td>-0.662308</td><td>-1.601268</td><td>-1.917711</td><td>0.896151</td><td>-0.038347</td><td>1.120629</td><td>0.518504</td><td>1.476256</td><td>0.785076</td><td>1.017946</td><td>0.941139</td><td>0.361583</td><td>-0.63828</td><td>-0.815442</td><td>&hellip;</td><td>-1.15466</td><td>-0.071019</td><td>0.005233</td><td>-1.098445</td><td>-0.207571</td><td>0.093457</td><td>0.952124</td><td>-0.091556</td><td>0.026293</td><td>0.366903</td><td>-0.300884</td><td>0.050481</td><td>-0.177875</td><td>-0.176997</td><td>0.187346</td><td>1.119846</td><td>-0.173731</td><td>0.040365</td><td>1.040592</td><td>-0.256137</td><td>0.034156</td><td>1.097758</td><td>-0.320625</td><td>0.115785</td><td>-0.962262</td><td>-0.107168</td><td>0.113762</td><td>-1.130666</td><td>-0.208219</td><td>0.124273</td><td>-0.990429</td><td>-0.189998</td><td>0.01129</td><td>-0.528344</td><td>-0.234332</td><td>0.015547</td><td>-0.780572</td></tr><tr><td>1</td><td>1605</td><td>0</td><td>0</td><td>-1.572615</td><td>3.858156</td><td>2.95633</td><td>1.300913</td><td>2.780389</td><td>3.074109</td><td>3.066704</td><td>-0.744921</td><td>-0.076711</td><td>-0.488637</td><td>0.391012</td><td>11.0</td><td>7.0</td><td>76.0</td><td>-1.138502</td><td>1.157193</td><td>-0.209437</td><td>-0.806402</td><td>-0.339077</td><td>-0.592733</td><td>-1.848096</td><td>-1.581626</td><td>0.612074</td><td>0.191684</td><td>0.837864</td><td>0.397742</td><td>2.657752</td><td>1.168224</td><td>-1.7849</td><td>-0.085103</td><td>0.907257</td><td>-0.541719</td><td>-0.956384</td><td>&hellip;</td><td>0.731997</td><td>-0.083682</td><td>0.021103</td><td>-1.000173</td><td>-0.151283</td><td>0.119696</td><td>-0.735771</td><td>0.096701</td><td>0.054554</td><td>-0.734044</td><td>-0.371035</td><td>0.101811</td><td>0.345227</td><td>-0.079026</td><td>0.216957</td><td>1.114831</td><td>-0.168288</td><td>0.036253</td><td>1.041875</td><td>0.433767</td><td>0.088799</td><td>-0.848983</td><td>0.302826</td><td>0.125203</td><td>-0.598476</td><td>0.401475</td><td>0.168661</td><td>0.730511</td><td>0.391299</td><td>0.083686</td><td>-0.803683</td><td>0.450136</td><td>0.019665</td><td>-0.626704</td><td>0.366767</td><td>0.044257</td><td>1.154521</td></tr><tr><td>2</td><td>1605</td><td>0</td><td>0</td><td>0.085172</td><td>4.029465</td><td>3.216702</td><td>1.9241</td><td>2.972498</td><td>3.181072</td><td>2.676115</td><td>-0.88276</td><td>-0.098824</td><td>-0.782345</td><td>0.471035</td><td>81.0</td><td>2.0</td><td>59.0</td><td>-1.232034</td><td>1.410958</td><td>-0.425981</td><td>-1.075019</td><td>-0.15005</td><td>-0.752805</td><td>-1.431982</td><td>-1.390779</td><td>-0.092707</td><td>-0.093736</td><td>0.566463</td><td>0.213935</td><td>1.357846</td><td>-0.06643</td><td>0.772687</td><td>0.473541</td><td>-0.005689</td><td>-0.39475</td><td>-0.57903</td><td>&hellip;</td><td>1.091695</td><td>-0.40541</td><td>0.220284</td><td>-1.140669</td><td>-0.129095</td><td>0.120208</td><td>-0.018042</td><td>-0.344662</td><td>0.088683</td><td>0.421717</td><td>-0.470735</td><td>0.082466</td><td>0.22053</td><td>0.158388</td><td>0.230158</td><td>1.153774</td><td>-0.106315</td><td>0.084159</td><td>1.153407</td><td>-0.363056</td><td>0.066604</td><td>0.935868</td><td>-0.409656</td><td>0.047955</td><td>0.591057</td><td>0.094667</td><td>0.340072</td><td>0.623284</td><td>0.144877</td><td>0.459288</td><td>0.603794</td><td>-0.157004</td><td>0.090291</td><td>0.747794</td><td>-0.203354</td><td>0.072811</td><td>0.847102</td></tr><tr><td>3</td><td>1605</td><td>0</td><td>0</td><td>-2.768082</td><td>2.377184</td><td>2.948765</td><td>1.438616</td><td>2.949908</td><td>3.298159</td><td>3.010296</td><td>-1.109763</td><td>-0.115104</td><td>-0.513872</td><td>0.451119</td><td>4.0</td><td>3.0</td><td>11.0</td><td>-1.054552</td><td>1.535755</td><td>-0.179677</td><td>-0.487148</td><td>-0.487206</td><td>-0.704949</td><td>-1.559941</td><td>-1.154272</td><td>-0.168588</td><td>-0.023594</td><td>0.106641</td><td>-0.863546</td><td>2.006068</td><td>0.233557</td><td>-0.011306</td><td>0.072626</td><td>-0.502054</td><td>-0.889185</td><td>-0.516596</td><td>&hellip;</td><td>-1.072401</td><td>0.106596</td><td>0.03842</td><td>0.653794</td><td>0.19051</td><td>0.344181</td><td>0.238918</td><td>-0.123472</td><td>0.065196</td><td>1.008725</td><td>-0.001479</td><td>0.015971</td><td>1.107286</td><td>0.20157</td><td>0.144305</td><td>0.988813</td><td>-0.054001</td><td>0.050241</td><td>1.15088</td><td>0.72496</td><td>0.18746</td><td>0.809649</td><td>0.697034</td><td>0.089986</td><td>-0.271157</td><td>2.011532</td><td>1.252324</td><td>0.60383</td><td>2.291722</td><td>1.300281</td><td>0.435139</td><td>0.967457</td><td>0.231634</td><td>1.126723</td><td>0.808233</td><td>0.150006</td><td>0.518416</td></tr><tr><td>4</td><td>1605</td><td>0</td><td>0</td><td>2.090984</td><td>2.563155</td><td>3.026436</td><td>1.184749</td><td>3.041878</td><td>3.604426</td><td>2.657762</td><td>-0.970559</td><td>-0.139817</td><td>-0.532435</td><td>0.612716</td><td>15.0</td><td>1.0</td><td>9.0</td><td>-0.667211</td><td>3.580876</td><td>0.087301</td><td>-0.706757</td><td>-0.242437</td><td>-0.567562</td><td>-1.178818</td><td>-1.936775</td><td>-0.751342</td><td>0.169354</td><td>-0.066423</td><td>-0.4728</td><td>0.777907</td><td>-0.851054</td><td>-1.80886</td><td>-1.056851</td><td>-0.143766</td><td>-0.846094</td><td>-0.693096</td><td>&hellip;</td><td>-1.058791</td><td>1.214794</td><td>0.327573</td><td>0.919609</td><td>1.683343</td><td>0.643852</td><td>0.594364</td><td>1.945899</td><td>0.376164</td><td>0.128812</td><td>1.440363</td><td>0.586327</td><td>0.149032</td><td>1.035848</td><td>0.712859</td><td>1.13505</td><td>2.101918</td><td>0.296111</td><td>-0.786615</td><td>-0.504521</td><td>0.068624</td><td>-0.771453</td><td>-0.372165</td><td>0.028806</td><td>1.084732</td><td>0.246521</td><td>1.099255</td><td>1.127171</td><td>0.377151</td><td>1.200843</td><td>1.124583</td><td>-0.104984</td><td>0.145686</td><td>1.136897</td><td>0.08527</td><td>0.210976</td><td>1.154264</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 792)\n",
       "┌───────────┬─────────┬─────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ time_bucke ┆ … ┆ feature_77 ┆ feature_78 ┆ feature_7 ┆ feature_7 │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ t          ┆   ┆ __rz3      ┆ __rmean3   ┆ 8__rstd3  ┆ 8__rz3    │\n",
       "│ i32       ┆ i32     ┆ i32     ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
       "│           ┆         ┆         ┆ u8         ┆   ┆ f32        ┆ f32        ┆ f32       ┆ f32       │\n",
       "╞═══════════╪═════════╪═════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 0         ┆ 1605    ┆ 0       ┆ 0          ┆ … ┆ -0.528344  ┆ -0.234332  ┆ 0.015547  ┆ -0.780572 │\n",
       "│ 1         ┆ 1605    ┆ 0       ┆ 0          ┆ … ┆ -0.626704  ┆ 0.366767   ┆ 0.044257  ┆ 1.154521  │\n",
       "│ 2         ┆ 1605    ┆ 0       ┆ 0          ┆ … ┆ 0.747794   ┆ -0.203354  ┆ 0.072811  ┆ 0.847102  │\n",
       "│ 3         ┆ 1605    ┆ 0       ┆ 0          ┆ … ┆ 1.126723   ┆ 0.808233   ┆ 0.150006  ┆ 0.518416  │\n",
       "│ 4         ┆ 1605    ┆ 0       ┆ 0          ┆ … ┆ 1.136897   ┆ 0.08527    ┆ 0.210976  ┆ 1.154264  │\n",
       "└───────────┴─────────┴─────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_data.limit(5).collect()  # 测试连通性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fba1f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>min</th><th>max</th><th>mean</th><th>std</th><th>count</th></tr><tr><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>u32</td></tr></thead><tbody><tr><td>30</td><td>-4.125087</td><td>5.0</td><td>-0.081503</td><td>0.659414</td><td>20328</td></tr><tr><td>25</td><td>-5.0</td><td>4.943763</td><td>-0.0727</td><td>0.808597</td><td>20328</td></tr><tr><td>27</td><td>-5.0</td><td>5.0</td><td>-0.064174</td><td>0.682261</td><td>19360</td></tr><tr><td>12</td><td>-5.0</td><td>5.0</td><td>-0.048375</td><td>0.80594</td><td>20328</td></tr><tr><td>2</td><td>-5.0</td><td>5.0</td><td>-0.048374</td><td>0.771517</td><td>20328</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>26</td><td>-5.0</td><td>5.0</td><td>0.019668</td><td>0.786453</td><td>20328</td></tr><tr><td>4</td><td>-5.0</td><td>5.0</td><td>0.025079</td><td>0.952476</td><td>18392</td></tr><tr><td>17</td><td>-5.0</td><td>5.0</td><td>0.031603</td><td>0.657436</td><td>20328</td></tr><tr><td>23</td><td>-5.0</td><td>5.0</td><td>0.04033</td><td>0.877483</td><td>20328</td></tr><tr><td>8</td><td>-5.0</td><td>5.0</td><td>0.046574</td><td>1.077924</td><td>20328</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 6)\n",
       "┌───────────┬───────────┬──────────┬───────────┬──────────┬───────┐\n",
       "│ symbol_id ┆ min       ┆ max      ┆ mean      ┆ std      ┆ count │\n",
       "│ ---       ┆ ---       ┆ ---      ┆ ---       ┆ ---      ┆ ---   │\n",
       "│ i32       ┆ f32       ┆ f32      ┆ f32       ┆ f32      ┆ u32   │\n",
       "╞═══════════╪═══════════╪══════════╪═══════════╪══════════╪═══════╡\n",
       "│ 30        ┆ -4.125087 ┆ 5.0      ┆ -0.081503 ┆ 0.659414 ┆ 20328 │\n",
       "│ 25        ┆ -5.0      ┆ 4.943763 ┆ -0.0727   ┆ 0.808597 ┆ 20328 │\n",
       "│ 27        ┆ -5.0      ┆ 5.0      ┆ -0.064174 ┆ 0.682261 ┆ 19360 │\n",
       "│ 12        ┆ -5.0      ┆ 5.0      ┆ -0.048375 ┆ 0.80594  ┆ 20328 │\n",
       "│ 2         ┆ -5.0      ┆ 5.0      ┆ -0.048374 ┆ 0.771517 ┆ 20328 │\n",
       "│ …         ┆ …         ┆ …        ┆ …         ┆ …        ┆ …     │\n",
       "│ 26        ┆ -5.0      ┆ 5.0      ┆ 0.019668  ┆ 0.786453 ┆ 20328 │\n",
       "│ 4         ┆ -5.0      ┆ 5.0      ┆ 0.025079  ┆ 0.952476 ┆ 18392 │\n",
       "│ 17        ┆ -5.0      ┆ 5.0      ┆ 0.031603  ┆ 0.657436 ┆ 20328 │\n",
       "│ 23        ┆ -5.0      ┆ 5.0      ┆ 0.04033   ┆ 0.877483 ┆ 20328 │\n",
       "│ 8         ┆ -5.0      ┆ 5.0      ┆ 0.046574  ┆ 1.077924 ┆ 20328 │\n",
       "└───────────┴───────────┴──────────┴───────────┴──────────┴───────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_data.filter(pl.col(g_date).is_between(1610, 1630)).group_by(g_sym).agg(\n",
    "    pl.col(target_col).min().alias(\"min\"),\n",
    "    pl.col(target_col).max().alias(\"max\"),\n",
    "    pl.col(target_col).mean().alias(\"mean\"),\n",
    "    pl.col(target_col).std().alias(\"std\"),\n",
    "    pl.col(target_col).count().alias(\"count\")\n",
    ").sort([\"mean\", \"std\", \"count\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815beaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f09ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194a5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a6174",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "lf_data.select([g_sym, target_col])\n",
    "    .group_by(g_sym)\n",
    "    .agg([\n",
    "        pl.col(target_col).mean().alias(\"mean\"),\n",
    "        pl.col(target_col).std().alias(\"std\"),\n",
    "        pl.col(target_col).count().alias(\"count\")\n",
    "    ])\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c70f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 全局 time_idx 仅一次\n",
    "lf_grid = (\n",
    "    lf_data.select([g_date, g_time]).unique()\n",
    "        .sort([g_date, g_time])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\"); ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "\n",
    "lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")\n",
    "\n",
    "\n",
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# 接入 time_idx + 只保留所需列（仍是 Lazy）\n",
    "lf0 = (\n",
    "    lf_data.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "        .select(need_cols + [\"time_idx\"])\n",
    "        .sort([g_date, g_time, g_sym])\n",
    ")\n",
    "\n",
    "print(f\"[{_now()}] lazyframe ready\")\n",
    "\n",
    "\n",
    "# 全量天列表\n",
    "all_days = (\n",
    "    lf0.select(pl.col(g_date)).unique().sort(by=g_date)\n",
    "       .collect(streaming=True).get_column(g_date).to_numpy()\n",
    ")\n",
    "\n",
    "\n",
    "# 构造滑动时间窗 CV\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0: return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0: return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days): break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\"\n",
    "\n",
    "\n",
    "# 选“标准化统计区间”上界：第1折训练天的最大值（不含验证）\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"stats_hi (for global z-score) = {stats_hi}; first-fold train days end at this day.\")\n",
    "\n",
    "\n",
    "# ========== 1) 连续特征：一次性处理 inf->null、打缺失标记、组内 ffill、兜底 0 ==========\n",
    "# 先做 flag（要基于原始缺失），再做填充；合并成两次 with_columns，避免在 for 循环里多次改列\n",
    "inf2null_exprs = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
    "                for c in feature_cols] # inf -> null 不产生新列\n",
    "flags_exprs    = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\")\n",
    "                for c in feature_cols] # 产生新列\n",
    "fill_exprs     = [pl.col(c).forward_fill().over(g_sym).fill_null(0.0).alias(c)\n",
    "                for c in feature_cols] # 填充，覆盖原列\n",
    "\n",
    "lf_clean = (\n",
    "    lf0.with_columns(inf2null_exprs)         # inf -> null\n",
    "    .with_columns(flags_exprs)            # 缺失标记（基于原始缺失）\n",
    "    .with_columns(fill_exprs)             # 组内 ffill + 兜底 0\n",
    ")\n",
    "\n",
    "# 为数据标准化做准备\n",
    "\n",
    "# ========== 2) 训练区间（<= stats_hi）按组计算 mu/std（一次） ==========\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .group_by(g_sym)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in feature_cols] +\n",
    "        [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 训练期全局统计（作为回退）\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in feature_cols] +\n",
    "            [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 3) 把全局统计作为常量列加到每行（cross join 方式）\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\")\n",
    "\n",
    "# 4) join per-symbol 统计，并对每个特征做回退 & z-score\n",
    "\n",
    "lf_z = lf_z.join(lf_stats_sym, on=g_sym, how=\"left\")\n",
    "\n",
    "# 对每个特征做回退 & z-score\n",
    "eps = 1e-6\n",
    "z_cols = []\n",
    "for c in feature_cols:\n",
    "    mu_c_sym, std_c_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_c_glb, std_c_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    c_z = f\"{c}_z\"\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_c_sym).is_null()).then(pl.col(mu_c_glb)).otherwise(pl.col(mu_c_sym)).alias(f\"{c}_mu_use\"),\n",
    "        pl.when(pl.col(std_c_sym).is_null() | (pl.col(std_c_sym) == 0)).then(pl.col(std_c_glb)).otherwise(pl.col(std_c_sym)).alias(f\"{c}_std_use\")\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(f\"{c}_mu_use\")) / (pl.col(f\"{c}_std_use\") + eps)).alias(c_z)\n",
    "    ).drop([mu_c_glb, std_c_glb, mu_c_sym, std_c_sym, f\"{c}_mu_use\", f\"{c}_std_use\"])\n",
    "    z_cols.append(c_z)\n",
    "    \n",
    "    \n",
    "# 5) 输出列（z_特征 + isna 标记 + 时间/分类/目标/权重）\n",
    "namark_cols = [f\"{c}__isna\" for c in feature_cols]\n",
    "out_cols = [g_sym, g_date, g_time, \"time_idx\", weight_col, target_col] + time_features + z_cols + namark_cols\n",
    "\n",
    "lf_out = lf_z.select(out_cols).sort([g_date, g_time, g_sym])\n",
    "\n",
    "\n",
    "# \n",
    "# 关键：不要“逐天 collect”，而是每次收集一批天，然后一次性按 day 分区写入，显著减少 IO 次数\n",
    "\n",
    "\n",
    "tft_root = P(\"az\", \"tft\"); ensure_dir_az(tft_root)\n",
    "clean_dir = f\"{tft_root}/clean\"; ensure_dir_az(clean_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_DAYS = 30\n",
    "day_list   = list(map(int, all_days))\n",
    "day_chunks = [day_list[i:i+CHUNK_DAYS] for i in range(0, len(day_list), CHUNK_DAYS)]\n",
    "\n",
    "for ci, chunk in enumerate(day_chunks, 1):\n",
    "    df_chunk = lf_out.filter(pl.col(g_date).is_in(chunk)).collect()\n",
    "    table = df_chunk.to_arrow()\n",
    "\n",
    "    chunk_dir = f\"{clean_dir}/chunk_{chunk[0]:04d}_{chunk[-1]:04d}\"  # e.g. chunk_20240101_20240130\n",
    "    ds.write_dataset(\n",
    "        data=table,\n",
    "        base_dir=chunk_dir,\n",
    "        filesystem=fs,\n",
    "        format=\"parquet\",\n",
    "        partitioning=None,                         # ← 不再按天分区\n",
    "        existing_data_behavior=\"overwrite_or_ignore\",\n",
    "        basename_template=\"data-{i}.parquet\",      # 少量大文件\n",
    "        max_rows_per_file=50_000_000,              # 按行数切文件，防止超大\n",
    "    )\n",
    "    print(f\"[{_now()}] chunk {ci}/{len(day_chunks)} -> days {chunk[0]}..{chunk[-1]} written\")\n",
    "\n",
    "print(fs.ls(clean_dir)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# ========= 预分组：按 chunk 归并所有 parquet =========\n",
    "# 1) 列出 chunk 目录（加 az:// 前缀并排序）\n",
    "entries = fs.ls(clean_dir)\n",
    "chunk_dirs = []\n",
    "for e in entries:\n",
    "    if isinstance(e, str):\n",
    "        path = e\n",
    "    else:\n",
    "        path = e.get(\"name\") or e.get(\"path\") or e.get(\"Key\") or str(e)\n",
    "    if path.rstrip(\"/\").split(\"/\")[-1].startswith(\"chunk_\"):\n",
    "        chunk_dirs.append(path if path.startswith(\"az://\") else f\"az://{path}\")\n",
    "chunk_dirs = sorted(chunk_dirs)\n",
    "\n",
    "\n",
    "# 2) 构建 {chunk_dir: [该目录下所有 parquet 文件]}\n",
    "chunk2paths = defaultdict(list)\n",
    "for cdir in chunk_dirs:\n",
    "    paths = fs.glob(f\"{cdir}/*.parquet\")\n",
    "    paths = [p if p.startswith(\"az://\") else f\"az://{p}\" for p in paths]\n",
    "    if not paths:\n",
    "        print(f\"[WARN] empty chunk: {cdir}\")\n",
    "    chunk2paths[cdir] = paths\n",
    "\n",
    "\n",
    "# 3) 展平为 all_paths（模板/验证时 lazy 扫描用）\n",
    "all_paths = [p for plist in chunk2paths.values() for p in plist]\n",
    "print(f\"[prep] {len(chunk_dirs)} chunks; {len(all_paths)} files total\")\n",
    "\n",
    "\n",
    "unknown_reals = z_cols + namark_cols + time_features  # 为了快速验证，暂用这个\n",
    "cols = [g_sym, \"time_idx\", weight_col, target_col, *unknown_reals]\n",
    "\n",
    "from pipeline.stream_input import ShardedBatchStream\n",
    "\n",
    "# ========= 训练 =========\n",
    "best_ckpt_paths, fold_metrics = [], []\n",
    "\n",
    "tft_root = P(\"local\", \"tft\"); ensure_dir_local(tft_root)\n",
    "ckpts_root = Path(tft_root) / \"ckpts\"; ensure_dir_local(ckpts_root.as_posix())\n",
    "logs_root  = Path(tft_root) / \"logs\";  ensure_dir_local(logs_root.as_posix())\n",
    "\n",
    "\n",
    "\n",
    "for fold_id, (train_days, val_days) in enumerate(folds_by_day, start=1):\n",
    "    print(f\"[fold {fold_id}] train {train_days[0]}..{train_days[-1]} ({len(train_days)} days), \"\n",
    "        f\"val {val_days[0]}..{val_days[-1]} ({len(val_days)} days)\")\n",
    "\n",
    "    # ---- Template（用前 N 天构建编码器/缩放器）----\n",
    "    days_sorted = np.sort(train_days)\n",
    "    TEMPLATE_DAYS = min(5, len(days_sorted))\n",
    "    tmpl_days = list(map(int, days_sorted[:TEMPLATE_DAYS]))\n",
    "\n",
    "    pdf_tmpl = (\n",
    "        pl.scan_parquet(all_paths, storage_options=storage_options)\n",
    "        .filter(pl.col(g_date).is_in(tmpl_days))\n",
    "        .select(cols)\n",
    "        .collect(streaming=True)\n",
    "        .to_pandas()\n",
    "    )\n",
    "    pdf_tmpl[g_sym] = pdf_tmpl[g_sym].astype(\"str\")\n",
    "    pdf_tmpl.sort_values([g_sym, \"time_idx\"], inplace=True)\n",
    "    print(f\"[fold {fold_id}] template days={tmpl_days}, template shape={pdf_tmpl.shape}\")\n",
    "\n",
    "    # ---- 验证集 ----\n",
    "    pdf_val = (\n",
    "        pl.scan_parquet(all_paths, storage_options=storage_options)\n",
    "        .filter(pl.col(g_date).is_in(list(map(int, val_days))))\n",
    "        .select(cols)\n",
    "        .collect(streaming=True)\n",
    "        .to_pandas()\n",
    "    )\n",
    "    pdf_val[g_sym] = pdf_val[g_sym].astype(\"str\")\n",
    "    pdf_val.sort_values([g_sym, \"time_idx\"], inplace=True)\n",
    "    print(f\"template {pdf_tmpl.shape}, val {pdf_val.shape}\")\n",
    "\n",
    "    # ---- TimeSeries template ----\n",
    "    identity_scalers = {name: None for name in unknown_reals}  # 连续特征直接透传（你已做 z/标准化）\n",
    "    template = TimeSeriesDataSet(\n",
    "        pdf_tmpl,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_col,\n",
    "        group_ids=[g_sym],\n",
    "        weight=weight_col,                        # None 时 TSD 会忽略\n",
    "        max_encoder_length=ENC_LEN,\n",
    "        max_prediction_length=PRED_LEN,\n",
    "        static_categoricals=[g_sym],\n",
    "        time_varying_unknown_reals=unknown_reals,\n",
    "        lags=None,\n",
    "        categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "        add_relative_time_idx=False,\n",
    "        add_target_scales=False,\n",
    "        add_encoder_length=False,\n",
    "        allow_missing_timesteps=True,\n",
    "        target_normalizer=None,\n",
    "        scalers=identity_scalers,\n",
    "    )\n",
    "\n",
    "    # ---- 验证 Loader ----\n",
    "    validation = TimeSeriesDataSet.from_dataset(template, data=pdf_val, stop_randomization=True)\n",
    "    val_loader = validation.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=BATCH_SIZE * 2,\n",
    "        num_workers=min(8, max(1, os.cpu_count() - 2)),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "    )\n",
    "\n",
    "    # ---- 训练流（按 chunk 一次读取）----\n",
    "    train_stream = ShardedBatchStream(\n",
    "        template_tsd=template,\n",
    "        chunk_dirs=chunk_dirs,\n",
    "        chunk2paths=chunk2paths,\n",
    "        g_sym=g_sym,\n",
    "        batch_size=1024,\n",
    "        buffer_batches=0,\n",
    "        seed=42,\n",
    "        cols=cols,\n",
    "        print_every_chunks=1,   # 每处理一个 chunk 打印一次\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_stream,\n",
    "        batch_size=None,\n",
    "        num_workers=min(8, max(1, os.cpu_count() - 2)),\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "        multiprocessing_context=\"spawn\",   # 保险：远端 FS + 多进程\n",
    "    )\n",
    "\n",
    "    # ---- callbacks/logger/trainer ----\n",
    "    ckpt_dir_fold = Path(ckpts_root) / f\"fold_{fold_id}\"\n",
    "    ensure_dir_local(ckpt_dir_fold.as_posix())\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_RMSE\", mode=\"min\", patience=3),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_RMSE\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            dirpath=ckpt_dir_fold.as_posix(),\n",
    "            filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_RMSE:.5f}}\",\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ]\n",
    "    run_tag = (\n",
    "        f\"seed{cfg['seed']}\"\n",
    "        f\"_enc{ENC_LEN}_pred{PRED_LEN}\"\n",
    "        f\"_lr{LR}_hid{HIDDEN}_hd{HEADS}_do{DROPOUT}\"\n",
    "        f\"_fold{fold_id}\"\n",
    "        f\"_{time.strftime('%m%d-%H%M')}\"   # 或用 {short_git()}\n",
    "    )\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=logs_root, \n",
    "        name=f\"tft_f{fold_id}\", \n",
    "        default_hp_metric=False)\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"gpu\", devices=1, precision=\"bf16-mixed\",\n",
    "        max_epochs=30,\n",
    "        num_sanity_val_steps=0,\n",
    "        gradient_clip_val=0.5,\n",
    "        log_every_n_steps=50,          # 更频繁\n",
    "        enable_progress_bar=True,      # 显示进度条\n",
    "        enable_model_summary=False,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        default_root_dir=ckpts_root,\n",
    "    )\n",
    "\n",
    "    # ---- 模型并训练 ----\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        template,\n",
    "        loss=MAE(),\n",
    "        logging_metrics=[RMSE()],\n",
    "        learning_rate=float(cfg.get(\"tft\", {}).get(\"lr\", 1e-3)),\n",
    "        hidden_size=int(cfg.get(\"tft\", {}).get(\"hidden_size\", 64)),\n",
    "        attention_head_size=int(cfg.get(\"tft\", {}).get(\"heads\", 2)),\n",
    "        dropout=float(cfg.get(\"tft\", {}).get(\"dropout\", 0.1)),\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    # ---- 记录结果 ----\n",
    "    es_cb, ckpt_cb = callbacks[0], callbacks[1]\n",
    "    print(\"epoch_end_at   :\", trainer.current_epoch)\n",
    "    print(\"global_step    :\", trainer.global_step)\n",
    "    print(\"val_best_score :\", float(ckpt_cb.best_model_score))\n",
    "    print(\"es_stopped_ep  :\", getattr(es_cb, \"stopped_epoch\", None))\n",
    "    print(\"es_wait_count  :\", getattr(es_cb, \"wait_count\", None))\n",
    "\n",
    "    best_ckpt_paths.append(ckpt_cb.best_model_path)\n",
    "    fold_metrics.append(float(ckpt_cb.best_model_score))\n",
    "    cv_rmse = np.mean(fold_metrics)\n",
    "    print(f\"[CV] mean val_RMSE = {cv_rmse:.6f}\")\n",
    "\n",
    "print(\"[done] best_ckpts =\", best_ckpt_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cc7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5111b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
