{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34503ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ── 标准库（stdlib） ─────────────────────────────────────────────────────────────\n",
    "import os\n",
    "\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "# ── 第三方（third-party） ───────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import torch\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import (\n",
    "    TimeSeriesDataSet,\n",
    "    TemporalFusionTransformer,\n",
    ")\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from pytorch_forecasting.data.encoders import (\n",
    "    NaNLabelEncoder,\n",
    ")\n",
    "\n",
    "# 你的工程工具\n",
    "from pipeline.io import cfg, P, fs, storage_options, ensure_dir_local, ensure_dir_az\n",
    "\n",
    "def _now(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{_now()}] imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入文件 /mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\n",
    "input_file = \"/mnt/data/js/exp/v1/reports/fi/features__fs__1600-1690__cv2-g2-r4__seed42__top1000__1758551023.txt\"\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    data = file.read()\n",
    "lines = data.split('\\n')\n",
    "features = [line.split()[0] for line in lines if line.strip() and not line.startswith('#')]\n",
    "print(f\"[{_now()}] loaded {len(features)} features from {input_file}\")  \n",
    "\n",
    "# ── 本地（local） ────────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad855cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ---- 关键配置（按需改）----\n",
    "target_col = cfg[\"target\"]                 # e.g. \"responder_6\"\n",
    "g_sym, g_date, g_time = cfg[\"keys\"]        # e.g. (\"symbol_id\",\"date_id\",\"time_id\")\n",
    "weight_col = cfg[\"weight\"]\n",
    "\n",
    "time_features = [\"time_pos\", \"time_sin\", \"time_cos\", \"time_bucket\"]\n",
    "\n",
    "# 严格匹配基础特征：feature_后跟两位数字、无后缀\n",
    "base_features = [f for f in features if re.fullmatch(r\"feature_\\d{2}\", f)]\n",
    "\n",
    "# 响应类历史特征：以 responder_ 开头\n",
    "resp_his_feats = [f for f in features if f.startswith(\"responder_\")]\n",
    "\n",
    "# 其他派生的 feature_*（有后缀），但排除基础特征\n",
    "feat_his_feats = [f for f in features if f.startswith(\"feature_\") and f not in base_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_cols = list(dict.fromkeys(base_features + resp_his_feats + feat_his_feats))\n",
    "\n",
    "need_cols = list(dict.fromkeys([g_sym, g_date, g_time, weight_col, target_col] + time_features + feature_cols))\n",
    "\n",
    "# CV & 训练\n",
    "N_SPLITS   = 2\n",
    "GAP_DAYS   = 7\n",
    "TRAIN_TO_VAL = 4               # 训练:验证 = 4:1\n",
    "ENC_LEN    = 10\n",
    "PRED_LEN   = 1\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3\n",
    "HIDDEN     = 64\n",
    "HEADS      = 2\n",
    "DROPOUT    = 0.1\n",
    "MAX_EPOCHS_PER_SHARD = 1\n",
    "CHUNK_DAYS = 20               # 训练分片：每片多少天\n",
    "\n",
    "print(\"config ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_dir = P(\"az\", cfg[\"paths\"].get(\"panel_shards\", \"panel_shards\"))\n",
    "data_path  = fs.glob(f\"{panel_dir}/*.parquet\")\n",
    "az_path = [f\"az://{p}\" for p in data_path]\n",
    "lf_data = pl.scan_parquet(az_path, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局 time_idx 仅一次\n",
    "lf_grid = (\n",
    "    lf_data.select([g_date, g_time]).unique()\n",
    "        .sort([g_date, g_time])\n",
    "        .with_row_index(\"time_idx\")\n",
    "        .with_columns(pl.col(\"time_idx\").cast(pl.Int64))\n",
    ")\n",
    "\n",
    "grid_path = P(\"local\", \"tft/panel/grid_timeidx.parquet\"); ensure_dir_local(Path(grid_path).parent.as_posix())\n",
    "\n",
    "lf_grid.collect(streaming=True).write_parquet(grid_path, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lazy = pl.scan_parquet(grid_path)\n",
    "\n",
    "# 接入 time_idx + 只保留所需列（仍是 Lazy）\n",
    "lf0 = (\n",
    "    lf_data.join(grid_lazy, on=[g_date, g_time], how=\"left\")\n",
    "        .select(need_cols + [\"time_idx\"])\n",
    "        .sort([g_date, g_time, g_sym])\n",
    ")\n",
    "\n",
    "print(f\"[{_now()}] lazyframe ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b157ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全量天列表\n",
    "all_days = (\n",
    "    lf0.select(pl.col(g_date)).unique().sort(by=g_date)\n",
    "       .collect(streaming=True).get_column(g_date).to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造滑动时间窗 CV\n",
    "def make_sliding_cv_by_days(all_days: np.ndarray, *, n_splits: int, gap_days: int, train_to_val: int):\n",
    "    all_days = np.asarray(all_days).ravel()\n",
    "    K, R, G = n_splits, train_to_val, gap_days\n",
    "    usable = len(all_days) - G\n",
    "    if usable <= 0 or K <= 0 or R <= 0: return []\n",
    "    V_base, rem = divmod(usable, R + K)\n",
    "    if V_base <= 0: return []\n",
    "    T = R * V_base\n",
    "    v_lens = [V_base + 1 if i < rem else V_base for i in range(K)]\n",
    "    folds, v_lo = [], T + G\n",
    "    for V_i in v_lens:\n",
    "        v_hi, tr_hi, tr_lo = v_lo + V_i, v_lo - G, v_lo - G - T\n",
    "        if tr_lo < 0 or v_hi > len(all_days): break\n",
    "        folds.append((all_days[tr_lo:tr_hi], all_days[v_lo:v_hi]))\n",
    "        v_lo = v_hi\n",
    "    return folds\n",
    "\n",
    "folds_by_day = make_sliding_cv_by_days(all_days, n_splits=N_SPLITS, gap_days=GAP_DAYS, train_to_val=TRAIN_TO_VAL)\n",
    "assert len(folds_by_day) > 0, \"no CV folds constructed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选“标准化统计区间”上界：第1折训练天的最大值（不含验证）\n",
    "stats_hi = int(folds_by_day[0][0][-1])\n",
    "print(f\"stats_hi (for global z-score) = {stats_hi}; first-fold train days end at this day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8918944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1) 连续特征：一次性处理 inf->null、打缺失标记、组内 ffill、兜底 0 ==========\n",
    "# 先做 flag（要基于原始缺失），再做填充；合并成两次 with_columns，避免在 for 循环里多次改列\n",
    "inf2null_exprs = [pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
    "                for c in feature_cols] # inf -> null 不产生新列\n",
    "flags_exprs    = [pl.col(c).is_null().cast(pl.Int8).alias(f\"{c}__isna\")\n",
    "                for c in feature_cols] # 产生新列\n",
    "fill_exprs     = [pl.col(c).forward_fill().over(g_sym).fill_null(0.0).alias(c)\n",
    "                for c in feature_cols] # 填充，覆盖原列\n",
    "\n",
    "lf_clean = (\n",
    "    lf0.with_columns(inf2null_exprs)         # inf -> null\n",
    "    .with_columns(flags_exprs)            # 缺失标记（基于原始缺失）\n",
    "    .with_columns(fill_exprs)             # 组内 ffill + 兜底 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为数据标准化做准备\n",
    "\n",
    "# ========== 2) 训练区间（<= stats_hi）按组计算 mu/std（一次） ==========\n",
    "lf_stats_sym = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .group_by(g_sym)\n",
    "    .agg([pl.col(c).mean().alias(f\"mu_{c}\") for c in feature_cols] +\n",
    "        [pl.col(c).std(ddof=0).alias(f\"std_{c}\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 训练期全局统计（作为回退）\n",
    "lf_stats_glb = (\n",
    "    lf_clean.filter(pl.col(g_date) <= stats_hi)\n",
    "    .select([pl.col(c).mean().alias(f\"mu_{c}_glb\") for c in feature_cols] +\n",
    "            [pl.col(c).std(ddof=0).alias(f\"std_{c}_glb\") for c in feature_cols])\n",
    ")\n",
    "\n",
    "# 3) 把全局统计作为常量列加到每行（cross join 方式）\n",
    "lf_z = lf_clean.join(lf_stats_glb, how=\"cross\")\n",
    "\n",
    "# 4) join per-symbol 统计，并对每个特征做回退 & z-score\n",
    "\n",
    "lf_z = lf_z.join(lf_stats_sym, on=g_sym, how=\"left\")\n",
    "\n",
    "# 对每个特征做回退 & z-score\n",
    "eps = 1e-6\n",
    "z_cols = []\n",
    "for c in feature_cols:\n",
    "    mu_c_sym, std_c_sym = f\"mu_{c}\", f\"std_{c}\"\n",
    "    mu_c_glb, std_c_glb = f\"mu_{c}_glb\", f\"std_{c}_glb\"\n",
    "    c_z = f\"{c}_z\"\n",
    "    lf_z = lf_z.with_columns(\n",
    "        pl.when(pl.col(mu_c_sym).is_null()).then(pl.col(mu_c_glb)).otherwise(pl.col(mu_c_sym)).alias(f\"{c}_mu_use\"),\n",
    "        pl.when(pl.col(std_c_sym).is_null() | (pl.col(std_c_sym) == 0)).then(pl.col(std_c_glb)).otherwise(pl.col(std_c_sym)).alias(f\"{c}_std_use\")\n",
    "    ).with_columns(\n",
    "        ((pl.col(c) - pl.col(f\"{c}_mu_use\")) / (pl.col(f\"{c}_std_use\") + eps)).alias(c_z)\n",
    "    ).drop([mu_c_glb, std_c_glb, mu_c_sym, std_c_sym, f\"{c}_mu_use\", f\"{c}_std_use\"])\n",
    "    z_cols.append(c_z)\n",
    "    \n",
    "    \n",
    "# 5) 输出列（z_特征 + isna 标记 + 时间/分类/目标/权重）\n",
    "namark_cols = [f\"{c}__isna\" for c in feature_cols]\n",
    "out_cols = [g_sym, g_date, g_time, \"time_idx\", weight_col, target_col] + time_features + z_cols + namark_cols\n",
    "\n",
    "lf_out = lf_z.select(out_cols).sort([g_date, g_time, g_sym])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== 4) ==========\n",
    "# 关键：不要“逐天 collect”，而是每次收集一批天，然后一次性按 day 分区写入，显著减少 IO 次数\n",
    "\n",
    "\n",
    "tft_root = P(\"az\", \"tft\"); ensure_dir_az(tft_root)\n",
    "clean_dir = f\"{tft_root}/clean\"; ensure_dir_az(clean_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_DAYS = 30  # 可根据机器内存/速度调整；比如 10~30 天一块\n",
    "day_list = list(map(int, all_days))\n",
    "day_chunks = [day_list[i:i+CHUNK_DAYS] for i in range(0, len(day_list), CHUNK_DAYS)]\n",
    "\n",
    "\n",
    "for ci, chunk in enumerate(day_chunks, 1):\n",
    "    df_chunk = lf_out.filter(pl.col(g_date).is_in(chunk)).collect()\n",
    "    table = df_chunk.to_arrow()\n",
    "\n",
    "    ds.write_dataset(\n",
    "        data=table,\n",
    "        base_dir=clean_dir,\n",
    "        filesystem=fs,\n",
    "        format=\"parquet\",\n",
    "        partitioning=ds.partitioning(pa.schema([(g_date, pa.int32())])),  # or pa.int64()\n",
    "        existing_data_behavior=\"overwrite_or_ignore\",  # 按需改： \"delete_matching\" / \"overwrite_or_ignore\"\n",
    "    )\n",
    "    print(f\"[{_now()}] chunk {ci}/{len(day_chunks)} -> days {chunk[0]}..{chunk[-1]} written\")\n",
    "\n",
    "print(fs.ls(clean_dir)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ===\n",
    "\n",
    "# 设置随机种子\n",
    "L.seed_everything(int(cfg.get(\"seed\", 42)), workers=True)\n",
    "\n",
    "logs_root = P(\"local\", \"tft/logs\"); ensure_dir_local(Path(logs_root).as_posix())\n",
    "ckpts_root = P(\"local\", \"tft/ckpts\"); ensure_dir_local(Path(ckpts_root).as_posix())\n",
    "\n",
    "\n",
    "class ShardedBatchStream(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        template_tsd,\n",
    "        shard_days,\n",
    "        clean_dir: str,\n",
    "        g_sym: str,\n",
    "        batch_size: int = 1024,\n",
    "        num_workers: int = 8,\n",
    "        shuffle_within_shard: bool = True,\n",
    "        buffer_batches: int = 0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.template = template_tsd\n",
    "        self.days = list(map(int, shard_days))\n",
    "        self.clean_dir = clean_dir\n",
    "        self.g_sym = g_sym\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle_within_shard = shuffle_within_shard\n",
    "        self.buffer_batches = buffer_batches\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = random.Random(self.seed)\n",
    "        days = self.days[:]\n",
    "        rng.shuffle(days)\n",
    "\n",
    "        from collections import deque\n",
    "        buf = deque()\n",
    "\n",
    "        for d in days:\n",
    "            paths = fs.glob(f\"{self.clean_dir}/{d}/*.parquet\")\n",
    "            if not paths:\n",
    "                raise RuntimeError(f\"no data files found for day {d} in {self.clean_dir}\")\n",
    "            paths = [f\"az://{p}\" for p in paths]\n",
    "            pdf = pl.scan_parquet(paths, storage_options=storage_options).collect().to_pandas()\n",
    "            if pdf.empty:\n",
    "                raise RuntimeError(f\"empty data for day {d} in {self.clean_dir}\")\n",
    "            pdf[self.g_sym] = pdf[self.g_sym].astype(\"str\").astype(\"category\")\n",
    "\n",
    "            tsds = TimeSeriesDataSet.from_dataset(\n",
    "                self.template,\n",
    "                data=pdf.sort_values([self.g_sym, \"time_idx\"]),\n",
    "                stop_randomization=False,\n",
    "            )\n",
    "\n",
    "            dl = tsds.to_dataloader(\n",
    "                train=True,\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "                shuffle=self.shuffle_within_shard,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=False,\n",
    "            )\n",
    "\n",
    "            for batch in dl:\n",
    "                if self.buffer_batches > 0:\n",
    "                    buf.append(batch)\n",
    "                    if len(buf) >= self.buffer_batches:\n",
    "                        k = rng.randrange(len(buf))\n",
    "                        if k:\n",
    "                            buf.rotate(-k)\n",
    "                        yield buf.popleft()\n",
    "                else:\n",
    "                    yield batch\n",
    "\n",
    "        while buf:\n",
    "            yield buf.popleft()\n",
    "\n",
    "\n",
    "best_ckpt_paths = []\n",
    "fold_metrics = []\n",
    "\n",
    "# ---- 训练集 & 验证集 folds----\n",
    "for fold_id, (train, val) in enumerate(folds_by_day, start=1):\n",
    "    print(f\"[fold {fold_id}] train {train[0]}..{train[-1]} ({len(train)} days), \"\n",
    "          f\"val {val[0]}..{val[-1]} ({len(val)} days)\")\n",
    "    \n",
    "    days_sorted = np.sort(train)\n",
    "    \n",
    "    # ---- Template（用第一天分片建立，固化 encoders/scalers）----\n",
    "    TEMPLATE_DAYS = min(10, len(days_sorted))   # 你可按需调大/调小，比如 5/7/全部\n",
    "\n",
    "    tmpl_paths = []\n",
    "    for d in days_sorted[:TEMPLATE_DAYS]:\n",
    "        tmpl_paths.extend(fs.glob(f\"{clean_dir}/{d}/*.parquet\"))\n",
    "    tmpl_paths = [f\"az://{p}\" for p in tmpl_paths]\n",
    "    \n",
    "    pdf_tmpl = pl.scan_parquet(tmpl_paths, storage_options=storage_options).collect().to_pandas()\n",
    "    pdf_tmpl[g_sym] = pdf_tmpl[g_sym].astype(\"str\").astype(\"category\")\n",
    "    \n",
    "    print(f\"[fold {fold_id}] template days={list(map(int, days_sorted[:TEMPLATE_DAYS]))}, \"\n",
    "    f\"template shape={pdf_tmpl.shape}\")\n",
    "    \n",
    "    # 验证集\n",
    "    val_paths = []\n",
    "    for d in val:\n",
    "        val_paths.extend(fs.glob(f\"{clean_dir}/{d}/*.parquet\"))\n",
    "    val_paths = [f\"az://{p}\" for p in val_paths]\n",
    "    pdf_val = pl.scan_parquet(val_paths, storage_options=storage_options).collect().to_pandas()\n",
    "    pdf_val[g_sym] = pdf_val[g_sym].astype(\"str\").astype(\"category\")\n",
    "    \n",
    "    print(f\"template {pdf_tmpl.shape}, val {pdf_val.shape}\")\n",
    "    \n",
    "    unknown_reals = time_features + z_cols + namark_cols\n",
    "    \n",
    "    identity_scalers = {name: None for name in unknown_reals} # 我们的自变量连续特征只有unknown_reals      \n",
    "    template = TimeSeriesDataSet(\n",
    "        pdf_tmpl.sort_values([g_sym, \"time_idx\"]),\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_col,\n",
    "        group_ids=[g_sym],\n",
    "        weight=weight_col,\n",
    "        max_encoder_length=ENC_LEN,\n",
    "        max_prediction_length=PRED_LEN,\n",
    "        \n",
    "        static_categoricals=[g_sym],\n",
    "        time_varying_unknown_reals=unknown_reals,\n",
    "        \n",
    "        lags=None,  # 不用自动滞后\n",
    "        \n",
    "        categorical_encoders={g_sym: NaNLabelEncoder(add_nan=True)},\n",
    "        add_relative_time_idx=False, \n",
    "        add_target_scales=False, \n",
    "        add_encoder_length=False,\n",
    "        \n",
    "        allow_missing_timesteps=True,\n",
    "        \n",
    "        target_normalizer=None,\n",
    "        scalers=identity_scalers,\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        template, data=pdf_val.sort_values([g_sym, \"time_idx\"]), stop_randomization=True\n",
    "    )\n",
    "    val_loader = validation.to_dataloader(\n",
    "        train=False, batch_size=BATCH_SIZE, num_workers=min(8, max(1, os.cpu_count() - 2))\n",
    "    )\n",
    "\n",
    "    len(val_loader), pdf_tmpl.shape, pdf_val.shape\n",
    "    \n",
    "    train_stream = ShardedBatchStream(\n",
    "        template_tsd=template,\n",
    "        shard_days=days_sorted,\n",
    "        clean_dir=clean_dir,\n",
    "        g_sym=g_sym,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=min(8, max(1, os.cpu_count() - 2)),\n",
    "        shuffle_within_shard=True,\n",
    "        buffer_batches=16,   # 0 代表关闭跨分片缓冲打乱；8~64 可微调\n",
    "        seed=42,\n",
    "    )\n",
    "    # 外层 DataLoader 不再做 batch/多进程\n",
    "    train_loader = DataLoader(train_stream, batch_size=None, num_workers=0)\n",
    "    \n",
    "    # 在 callbacks 定义前，先为本折建独立目录\n",
    "    ckpt_dir_fold = Path(ckpts_root) / f\"fold_{fold_id}\"\n",
    "    ensure_dir_local(ckpt_dir_fold.as_posix())\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_RMSE\", mode=\"min\", patience=5),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_RMSE\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            dirpath=ckpt_dir_fold.as_posix(),            # 每折独立目录\n",
    "            filename=f\"fold{fold_id}-tft-best-{{epoch:02d}}-{{val_RMSE:.5f}}\",  # 文件名含 fold\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ]\n",
    "\n",
    "    # （可选）logger 名字也带上 fold，便于区分\n",
    "    logger = TensorBoardLogger(save_dir=logs_root, name=f\"tft_f{fold_id}\", default_hp_metric=False)\n",
    "    \n",
    "    VAL_EVERY_STEPS = 50\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"gpu\", devices=1, precision=\"bf16-mixed\",\n",
    "        max_epochs=5,\n",
    "        val_check_interval=VAL_EVERY_STEPS,\n",
    "        num_sanity_val_steps=0,\n",
    "        gradient_clip_val=0.5,\n",
    "        log_every_n_steps=50,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        default_root_dir=ckpts_root,\n",
    "    )\n",
    "\n",
    "    # 创建模型\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        template,\n",
    "        loss=MAE(),\n",
    "        logging_metrics=[RMSE()],\n",
    "        learning_rate=float(cfg.get(\"tft\", {}).get(\"lr\", 1e-3)),\n",
    "        hidden_size=int(cfg.get(\"tft\", {}).get(\"hidden_size\", 128)),\n",
    "        attention_head_size=int(cfg.get(\"tft\", {}).get(\"heads\", 4)),\n",
    "        dropout=float(cfg.get(\"tft\", {}).get(\"dropout\", 0.2)),\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    # 每折训练后：\n",
    "    es_cb = callbacks[0]  # EarlyStopping\n",
    "    ckpt_cb = callbacks[1]\n",
    "\n",
    "    print(\"epoch_end_at   :\", trainer.current_epoch)              # 停下时的 epoch 索引（0 基）\n",
    "    print(\"global_step    :\", trainer.global_step)                # 训练过的 step 数\n",
    "    print(\"val_best_score :\", float(ckpt_cb.best_model_score))    # 最优 val_RMSE\n",
    "    print(\"es_stopped_ep  :\", getattr(es_cb, \"stopped_epoch\", None))  # 触发早停的 epoch\n",
    "    print(\"es_wait_count  :\", getattr(es_cb, \"wait_count\", None))     # 连续未提升的验证次数\n",
    "\n",
    "    best_ckpt_paths.append(ckpt_cb.best_model_path)\n",
    "    fold_metrics.append(float(ckpt_cb.best_model_score))  # 这是监控的 val_RMSE\n",
    "\n",
    "    # CV 聚合（简单平均或按验证样本数加权平均）\n",
    "    cv_rmse = np.mean(fold_metrics)  # 或按样本数加权\n",
    "    print(f\"[CV] mean val_RMSE = {cv_rmse:.6f}\")\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
